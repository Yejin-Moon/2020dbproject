{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPHn7fmzMJa6zSTxNKvbt0+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yejin-Moon/2020dbproject/blob/main/Untitled1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8gnTM061UdW",
        "outputId": "ac8df59f-a440-46d8-fc8e-9dc95d9e3a98"
      },
      "source": [
        "!git clone https://github.com/DeepLabCut/DeepLabCut.git\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'DeepLabCut'...\n",
            "remote: Enumerating objects: 7024, done.\u001b[K\n",
            "remote: Counting objects: 100% (102/102), done.\u001b[K\n",
            "remote: Compressing objects: 100% (77/77), done.\u001b[K\n",
            "remote: Total 7024 (delta 52), reused 47 (delta 25), pack-reused 6922\u001b[K\n",
            "Receiving objects: 100% (7024/7024), 151.12 MiB | 37.57 MiB/s, done.\n",
            "Resolving deltas: 100% (4865/4865), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RFIPTp4vLAT6",
        "outputId": "54ab6090-c3ea-4265-9932-245d88ab1a03"
      },
      "source": [
        "!git clone -l -s git://github.com/AlexEMG/DeepLabCut.git cloned-DLC-repo\n",
        "%cd cloned-DLC-repo\n",
        "!ls"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'cloned-DLC-repo'...\n",
            "warning: --local is ignored\n",
            "remote: Enumerating objects: 7024, done.\u001b[K\n",
            "remote: Counting objects: 100% (102/102), done.\u001b[K\n",
            "remote: Compressing objects: 100% (77/77), done.\u001b[K\n",
            "remote: Total 7024 (delta 52), reused 47 (delta 25), pack-reused 6922\u001b[K\n",
            "Receiving objects: 100% (7024/7024), 151.11 MiB | 32.49 MiB/s, done.\n",
            "Resolving deltas: 100% (4865/4865), done.\n",
            "/content/cloned-DLC-repo\n",
            "AUTHORS\t\t    CONTRIBUTING.md  LICENSE\t       tests\n",
            "CODE_OF_CONDUCT.md  deeplabcut\t     README.md\t       testscript_cli.py\n",
            "compile.sh\t    dlc.py\t     reinstall.sh\n",
            "conda-environments  docs\t     requirements.txt\n",
            "_config.yml\t    examples\t     setup.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kR4WOvJELFDv",
        "outputId": "8742bb52-6a12-41b2-c298-e8578a6782fd"
      },
      "source": [
        "%cd /content/cloned-DLC-repo/examples/openfield-Pranav-2018-10-30\n",
        "!ls"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/cloned-DLC-repo/examples/openfield-Pranav-2018-10-30\n",
            "config.yaml  labeled-data  videos\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "A0yXWSCLLH1B",
        "outputId": "34cf0037-e5e9-4afa-a05d-79effdb0cc13"
      },
      "source": [
        "!pip install deeplabcut"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting deeplabcut\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/04/fc/b5774e22a3eeaac1e1ca5670aa2281a5c408c9abc25f51e53e3f2525aebd/deeplabcut-2.1.10.4-py3-none-any.whl (695kB)\n",
            "\u001b[K     |████████████████████████████████| 696kB 7.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (0.2.0)\n",
            "Requirement already satisfied: patsy in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (0.5.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (2.5.1)\n",
            "Requirement already satisfied: tables in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (3.4.4)\n",
            "Collecting opencv-python-headless~=3.4.9.33\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/67/1c/5544e626593158c6a23599f40193464121526e45aa470001a8113e45d9b8/opencv_python_headless-3.4.9.33-cp37-cp37m-manylinux1_x86_64.whl (21.6MB)\n",
            "\u001b[K     |████████████████████████████████| 21.6MB 39.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (1.15.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (3.13)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (2021.5.30)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (0.22.2.post1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (2.8.1)\n",
            "Collecting ruamel.yaml>=0.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0e/57/19361b93542a1bb071fe8b7dd5ae792de6e8ab86c707aa2c44db08c60b99/ruamel.yaml-0.17.10-py3-none-any.whl (108kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 52.7MB/s \n",
            "\u001b[?25hCollecting filterpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/1d/ac8914360460fafa1990890259b7fa5ef7ba4cd59014e782e4ab3ab144d8/filterpy-1.4.5.zip (177kB)\n",
            "\u001b[K     |████████████████████████████████| 184kB 56.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (0.29.23)\n",
            "Collecting tensorpack==0.9.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/cb/62dc9115722a0b4fbeca6275ffbe47118149171ffafa7d1db6e295453aae/tensorpack-0.9.8-py2.py3-none-any.whl (288kB)\n",
            "\u001b[K     |████████████████████████████████| 296kB 46.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (7.1.2)\n",
            "Collecting numpy~=1.17.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b1/51/20098150b6108061cb7542af3de7bfcfe0182bca21613697153e49dc4adc/numpy-1.17.5-cp37-cp37m-manylinux1_x86_64.whl (20.0MB)\n",
            "\u001b[K     |████████████████████████████████| 20.0MB 35.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (0.36.2)\n",
            "Requirement already satisfied: imgaug in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (0.2.9)\n",
            "Requirement already satisfied: scipy>=1.4 in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (1.4.1)\n",
            "Collecting statsmodels>=0.11\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/da/69/8eef30a6237c54f3c0b524140e2975f4b1eea3489b45eb3339574fc8acee/statsmodels-0.12.2-cp37-cp37m-manylinux1_x86_64.whl (9.5MB)\n",
            "\u001b[K     |████████████████████████████████| 9.5MB 156kB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (4.41.1)\n",
            "Requirement already satisfied: intel-openmp in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (2021.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (57.0.0)\n",
            "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (1.1.5)\n",
            "Collecting bayesian-optimization\n",
            "  Downloading https://files.pythonhosted.org/packages/bb/7a/fd8059a3881d3ab37ac8f72f56b73937a14e8bb14a9733e68cc8b17dbe3c/bayesian-optimization-1.2.0.tar.gz\n",
            "Collecting scikit-image>=0.17\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fe/01/3a830f3df578ea3ed94ee7fd9f91e85c3dec2431d8548ab1c91869e51450/scikit_image-0.18.1-cp37-cp37m-manylinux1_x86_64.whl (29.2MB)\n",
            "\u001b[K     |████████████████████████████████| 29.2MB 104kB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (3.1.0)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (5.5.0)\n",
            "Collecting matplotlib==3.1.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/9b/35ab3469fd1509f7636a344940569ebfd33239673fd2318e80b4700a257c/matplotlib-3.1.3-cp37-cp37m-manylinux1_x86_64.whl (13.1MB)\n",
            "\u001b[K     |████████████████████████████████| 13.1MB 239kB/s \n",
            "\u001b[?25hRequirement already satisfied: moviepy<=1.0.1 in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (0.2.3.5)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (3.0.4)\n",
            "Collecting numba==0.51.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e4/f3/a32d3bc3d23923228e49276bbc1bdce3763dd19a299c4b4164d83bb5989f/numba-0.51.1-cp37-cp37m-manylinux2014_x86_64.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1MB 42.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx->deeplabcut) (4.4.2)\n",
            "Requirement already satisfied: numexpr>=2.5.2 in /usr/local/lib/python3.7/dist-packages (from tables->deeplabcut) (2.7.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->deeplabcut) (1.0.1)\n",
            "Collecting ruamel.yaml.clib>=0.1.2; platform_python_implementation == \"CPython\" and python_version < \"3.10\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/8e/4d77d58d398bcf2d608d8e953fa739f975fbf2c882f5150ee41f544d638b/ruamel.yaml.clib-0.2.4-cp37-cp37m-manylinux1_x86_64.whl (546kB)\n",
            "\u001b[K     |████████████████████████████████| 552kB 45.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: msgpack>=0.5.2 in /usr/local/lib/python3.7/dist-packages (from tensorpack==0.9.8->deeplabcut) (1.0.2)\n",
            "Collecting msgpack-numpy>=0.4.4.2\n",
            "  Downloading https://files.pythonhosted.org/packages/19/05/05b8d7c69c6abb36a34325cc3150089bdafc359f0a81fb998d93c5d5c737/msgpack_numpy-0.4.7.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: pyzmq>=16 in /usr/local/lib/python3.7/dist-packages (from tensorpack==0.9.8->deeplabcut) (22.1.0)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.7/dist-packages (from tensorpack==0.9.8->deeplabcut) (1.1.0)\n",
            "Requirement already satisfied: psutil>=5 in /usr/local/lib/python3.7/dist-packages (from tensorpack==0.9.8->deeplabcut) (5.4.8)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.7/dist-packages (from tensorpack==0.9.8->deeplabcut) (0.8.9)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.7/dist-packages (from imgaug->deeplabcut) (2.4.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from imgaug->deeplabcut) (4.1.2.30)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.7/dist-packages (from imgaug->deeplabcut) (1.7.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from imgaug->deeplabcut) (7.1.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.1->deeplabcut) (2018.9)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.17->deeplabcut) (2021.6.14)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.17->deeplabcut) (1.1.1)\n",
            "Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py->deeplabcut) (1.5.2)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->deeplabcut) (1.0.18)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->deeplabcut) (2.6.1)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.7/dist-packages (from ipython->deeplabcut) (4.8.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->deeplabcut) (0.8.1)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->deeplabcut) (5.0.5)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->deeplabcut) (0.7.5)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.3->deeplabcut) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.3->deeplabcut) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.3->deeplabcut) (2.4.7)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba==0.51.1->deeplabcut) (0.34.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->deeplabcut) (0.2.5)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect; sys_platform != \"win32\"->ipython->deeplabcut) (0.7.0)\n",
            "Building wheels for collected packages: filterpy, bayesian-optimization\n",
            "  Building wheel for filterpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for filterpy: filename=filterpy-1.4.5-cp37-none-any.whl size=110476 sha256=6fc74612882e036a45cbc4248d299f631fb66801c7e8129b40c9aa03ea25e448\n",
            "  Stored in directory: /root/.cache/pip/wheels/c3/0c/dd/e92392c3f38a41371602d99fc77d6c1d42aadbf0c6afccdd02\n",
            "  Building wheel for bayesian-optimization (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bayesian-optimization: filename=bayesian_optimization-1.2.0-cp37-none-any.whl size=11686 sha256=7e4a8cb2c29b21317af7407e9a62c1c8e20d2a07a83e99816c9bd5e7e6094ce1\n",
            "  Stored in directory: /root/.cache/pip/wheels/5a/56/ae/e0e3c1fc1954dc3ec712e2df547235ed072b448094d8f94aec\n",
            "Successfully built filterpy bayesian-optimization\n",
            "\u001b[31mERROR: tensorflow 2.5.0 has requirement numpy~=1.19.2, but you'll have numpy 1.17.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: kapre 0.3.5 has requirement numpy>=1.18.5, but you'll have numpy 1.17.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: numpy, opencv-python-headless, ruamel.yaml.clib, ruamel.yaml, matplotlib, filterpy, msgpack-numpy, tensorpack, statsmodels, bayesian-optimization, scikit-image, numba, deeplabcut\n",
            "  Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "  Found existing installation: matplotlib 3.2.2\n",
            "    Uninstalling matplotlib-3.2.2:\n",
            "      Successfully uninstalled matplotlib-3.2.2\n",
            "  Found existing installation: statsmodels 0.10.2\n",
            "    Uninstalling statsmodels-0.10.2:\n",
            "      Successfully uninstalled statsmodels-0.10.2\n",
            "  Found existing installation: scikit-image 0.16.2\n",
            "    Uninstalling scikit-image-0.16.2:\n",
            "      Successfully uninstalled scikit-image-0.16.2\n",
            "  Found existing installation: numba 0.51.2\n",
            "    Uninstalling numba-0.51.2:\n",
            "      Successfully uninstalled numba-0.51.2\n",
            "Successfully installed bayesian-optimization-1.2.0 deeplabcut-2.1.10.4 filterpy-1.4.5 matplotlib-3.1.3 msgpack-numpy-0.4.7.1 numba-0.51.1 numpy-1.17.5 opencv-python-headless-3.4.9.33 ruamel.yaml-0.17.10 ruamel.yaml.clib-0.2.4 scikit-image-0.18.1 statsmodels-0.12.2 tensorpack-0.9.8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "matplotlib",
                  "mpl_toolkits",
                  "numpy"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTfJUj32LalA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcc5145c-cc2c-4981-f98d-035c5f28122d"
      },
      "source": [
        "# Use TensorFlow 1.x:\n",
        "%tensorflow_version 1.x"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMXDBL7NLdI9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bbc516e-d977-4882-d4df-30243438b6e6"
      },
      "source": [
        "#GUIs don't work on the cloud, so we will supress wxPython: \n",
        "import os\n",
        "os.environ[\"DLClight\"]=\"True\"\n",
        "\n",
        "import deeplabcut"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DLC loaded in light mode; you cannot use any GUI (labeling, relabeling and standalone GUI)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "id": "VSPXutUhDr50",
        "outputId": "c8f0108a-e0c8-4ef0-ca2a-623e4f83c7c7"
      },
      "source": [
        "%cd /content/cloned-DLC-repo/examples\n",
        "\n",
        "from google.colab import files\n",
        "file_uploaded = files.upload()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/cloned-DLC-repo/examples\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-7928786f-7e53-4e5b-893c-9efca2eec24d\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-7928786f-7e53-4e5b-893c-9efca2eec24d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving walkdataset.zip to walkdataset.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 76
        },
        "id": "Njh6qbZwIKq1",
        "outputId": "bfd374c3-642b-4fb9-f6da-37abcf191fa9"
      },
      "source": [
        "%cd /content/cloned-DLC-repo/examples/dataset0624_5/videos\n",
        "\n",
        "from google.colab import files\n",
        "file_uploaded = files.upload()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: '/content/cloned-DLC-repo/examples/dataset0624_5/videos'\n",
            "/content/cloned-DLC-repo/examples\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-448d5db0-5256-4b07-9534-e001ba8f46a7\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-448d5db0-5256-4b07-9534-e001ba8f46a7\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "reaF6uhtEPzG",
        "outputId": "3c992b20-91d4-4244-804c-3e745d9fb088"
      },
      "source": [
        "%cd /content/cloned-DLC-repo/examples \n",
        "!unzip -uq \"walkdataset.zip\" -d \"/content/cloned-DLC-repo/examples/walkdataset\""
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/cloned-DLC-repo/examples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wd-VblbaIN40",
        "outputId": "fc20441f-335c-40e7-c349-49ffe515797f"
      },
      "source": [
        "ProjectFolderName = 'walkdataset'\n",
        "VideoType = 'mp4' \n",
        "\n",
        "#don't edit these:\n",
        "videofile_path = ['/content/cloned-DLC-repo/examples/walkdataset/'] #Enter the list of videos or folder to analyze.\n",
        "videofile_path"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/cloned-DLC-repo/examples/walkdataset/']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "fEG7CcpVIWJe",
        "outputId": "c03856a0-0b63-40c3-e14e-47a41ebfef07"
      },
      "source": [
        "path_config_file = '/content/cloned-DLC-repo/examples/walkdataset/config.yaml'\n",
        "path_config_file"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/cloned-DLC-repo/examples/walkdataset/config.yaml'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "DfZKXnW9Icyt",
        "outputId": "36c7f4fb-2df2-4d69-9636-1acc653ce51f"
      },
      "source": [
        "deeplabcut.__version__"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.1.10.4'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9LQ9sQLIexQ",
        "outputId": "8c3a725b-9962-4a35-8c38-4fe296b36ca2"
      },
      "source": [
        "deeplabcut.create_training_dataset('/content/cloned-DLC-repo/examples/walkdataset/config.yaml',num_shuffles=1)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/cloned-DLC-repo/examples/walkdataset/training-datasets/iteration-0/UnaugmentedDataSet_walkdatasetJune26  already exists!\n",
            "It appears that the images were labeled on a Windows system, but you are currently trying to create a training set on a Unix system. \n",
            " In this case the paths should be converted. Do you want to proceed with the conversion?\n",
            "yes/noyes\n",
            "Annotation data converted to unix format...\n",
            "Downloading a ImageNet-pretrained model from http://download.tensorflow.org/models/resnet_v1_50_2016_08_28.tar.gz....\n",
            "/content/cloned-DLC-repo/examples/walkdataset/dlc-models/iteration-0/walkdatasetJune26-trainset95shuffle1  already exists!\n",
            "/content/cloned-DLC-repo/examples/walkdataset/dlc-models/iteration-0/walkdatasetJune26-trainset95shuffle1/train  already exists!\n",
            "/content/cloned-DLC-repo/examples/walkdataset/dlc-models/iteration-0/walkdatasetJune26-trainset95shuffle1/test  already exists!\n",
            "The training dataset is successfully created. Use the function 'train_network' to start training. Happy training!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0.95,\n",
              "  1,\n",
              "  (array([ 2, 73, 53, 49, 26, 64, 55, 68,  6, 75, 40, 54, 27, 44, 28, 14, 59,\n",
              "          72, 20,  0, 51, 69, 71, 13, 25, 47,  5, 38, 15, 24, 12, 61, 66, 60,\n",
              "          56, 34, 31, 23, 30, 16, 46, 11, 74, 43, 32, 33, 62, 35, 19, 67, 21,\n",
              "          10,  1, 48, 63, 50, 65, 70, 18, 22,  8,  3, 41, 52, 45, 58, 57, 76,\n",
              "           4, 36, 42, 17,  7]), array([ 9, 39, 29, 37])))]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FD2W2aYkInFe",
        "outputId": "6cc9dce6-b737-4a3c-b0d2-a64a0468a9a4"
      },
      "source": [
        "deeplabcut.train_network(path_config_file, shuffle=1, displayiters=10,saveiters=500)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Config:\n",
            "{'all_joints': [[0],\n",
            "                [1],\n",
            "                [2],\n",
            "                [3],\n",
            "                [4],\n",
            "                [5],\n",
            "                [6],\n",
            "                [7],\n",
            "                [8],\n",
            "                [9],\n",
            "                [10],\n",
            "                [11],\n",
            "                [12],\n",
            "                [13],\n",
            "                [14]],\n",
            " 'all_joints_names': ['nose',\n",
            "                      'front_left_t',\n",
            "                      'front_left_m',\n",
            "                      'front_left_b',\n",
            "                      'front_right_t',\n",
            "                      'front_right_m',\n",
            "                      'front_right_b',\n",
            "                      'hind_left_t',\n",
            "                      'hind_left_m',\n",
            "                      'hind_left_b',\n",
            "                      'hind_right_t',\n",
            "                      'hind _right_m',\n",
            "                      'hind_right_b',\n",
            "                      'tail_start',\n",
            "                      'tail_end'],\n",
            " 'alpha_r': 0.02,\n",
            " 'batch_size': 1,\n",
            " 'clahe': True,\n",
            " 'claheratio': 0.1,\n",
            " 'crop_pad': 0,\n",
            " 'cropratio': 0.4,\n",
            " 'dataset': 'training-datasets/iteration-0/UnaugmentedDataSet_walkdatasetJune26/walkdataset_moon95shuffle1.mat',\n",
            " 'dataset_type': 'default',\n",
            " 'decay_steps': 30000,\n",
            " 'deterministic': False,\n",
            " 'display_iters': 1000,\n",
            " 'edge': False,\n",
            " 'emboss': {'alpha': [0.0, 1.0], 'embossratio': 0.1, 'strength': [0.5, 1.5]},\n",
            " 'fg_fraction': 0.25,\n",
            " 'global_scale': 0.8,\n",
            " 'histeq': True,\n",
            " 'histeqratio': 0.1,\n",
            " 'init_weights': '/usr/local/lib/python3.7/dist-packages/deeplabcut/pose_estimation_tensorflow/models/pretrained/resnet_v1_50.ckpt',\n",
            " 'intermediate_supervision': False,\n",
            " 'intermediate_supervision_layer': 12,\n",
            " 'location_refinement': True,\n",
            " 'locref_huber_loss': True,\n",
            " 'locref_loss_weight': 0.05,\n",
            " 'locref_stdev': 7.2801,\n",
            " 'log_dir': 'log',\n",
            " 'lr_init': 0.0005,\n",
            " 'max_input_size': 1500,\n",
            " 'mean_pixel': [123.68, 116.779, 103.939],\n",
            " 'metadataset': 'training-datasets/iteration-0/UnaugmentedDataSet_walkdatasetJune26/Documentation_data-walkdataset_95shuffle1.pickle',\n",
            " 'min_input_size': 64,\n",
            " 'mirror': False,\n",
            " 'multi_step': [[0.005, 10000],\n",
            "                [0.02, 430000],\n",
            "                [0.002, 730000],\n",
            "                [0.001, 1030000]],\n",
            " 'net_type': 'resnet_50',\n",
            " 'num_joints': 15,\n",
            " 'optimizer': 'sgd',\n",
            " 'pairwise_huber_loss': False,\n",
            " 'pairwise_predict': False,\n",
            " 'partaffinityfield_predict': False,\n",
            " 'pos_dist_thresh': 17,\n",
            " 'project_path': '/content/cloned-DLC-repo/examples/walkdataset',\n",
            " 'regularize': False,\n",
            " 'rotation': 25,\n",
            " 'rotratio': 0.4,\n",
            " 'save_iters': 50000,\n",
            " 'scale_jitter_lo': 0.5,\n",
            " 'scale_jitter_up': 1.25,\n",
            " 'scoremap_dir': 'test',\n",
            " 'sharpen': False,\n",
            " 'sharpenratio': 0.3,\n",
            " 'shuffle': True,\n",
            " 'snapshot_prefix': '/content/cloned-DLC-repo/examples/walkdataset/dlc-models/iteration-0/walkdatasetJune26-trainset95shuffle1/train/snapshot',\n",
            " 'stride': 8.0,\n",
            " 'weigh_negatives': False,\n",
            " 'weigh_only_present_joints': False,\n",
            " 'weigh_part_predictions': False,\n",
            " 'weight_decay': 0.0001}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Selecting single-animal trainer\n",
            "Starting with imgaug pose-dataset loader (=default).\n",
            "Batch Size is 1\n",
            "Initializing ResNet\n",
            "Loading ImageNet-pretrained resnet_50\n",
            "Display_iters overwritten as 10\n",
            "Save_iters overwritten as 500\n",
            "Training parameter:\n",
            "{'stride': 8.0, 'weigh_part_predictions': False, 'weigh_negatives': False, 'fg_fraction': 0.25, 'mean_pixel': [123.68, 116.779, 103.939], 'shuffle': True, 'snapshot_prefix': '/content/cloned-DLC-repo/examples/walkdataset/dlc-models/iteration-0/walkdatasetJune26-trainset95shuffle1/train/snapshot', 'log_dir': 'log', 'global_scale': 0.8, 'location_refinement': True, 'locref_stdev': 7.2801, 'locref_loss_weight': 0.05, 'locref_huber_loss': True, 'optimizer': 'sgd', 'intermediate_supervision': False, 'intermediate_supervision_layer': 12, 'regularize': False, 'weight_decay': 0.0001, 'crop_pad': 0, 'scoremap_dir': 'test', 'batch_size': 1, 'dataset_type': 'default', 'deterministic': False, 'mirror': False, 'pairwise_huber_loss': False, 'weigh_only_present_joints': False, 'partaffinityfield_predict': False, 'pairwise_predict': False, 'all_joints': [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14]], 'all_joints_names': ['nose', 'front_left_t', 'front_left_m', 'front_left_b', 'front_right_t', 'front_right_m', 'front_right_b', 'hind_left_t', 'hind_left_m', 'hind_left_b', 'hind_right_t', 'hind _right_m', 'hind_right_b', 'tail_start', 'tail_end'], 'alpha_r': 0.02, 'clahe': True, 'claheratio': 0.1, 'cropratio': 0.4, 'dataset': 'training-datasets/iteration-0/UnaugmentedDataSet_walkdatasetJune26/walkdataset_moon95shuffle1.mat', 'decay_steps': 30000, 'display_iters': 1000, 'edge': False, 'emboss': {'alpha': [0.0, 1.0], 'embossratio': 0.1, 'strength': [0.5, 1.5]}, 'histeq': True, 'histeqratio': 0.1, 'init_weights': '/usr/local/lib/python3.7/dist-packages/deeplabcut/pose_estimation_tensorflow/models/pretrained/resnet_v1_50.ckpt', 'lr_init': 0.0005, 'max_input_size': 1500, 'metadataset': 'training-datasets/iteration-0/UnaugmentedDataSet_walkdatasetJune26/Documentation_data-walkdataset_95shuffle1.pickle', 'min_input_size': 64, 'multi_step': [[0.005, 10000], [0.02, 430000], [0.002, 730000], [0.001, 1030000]], 'net_type': 'resnet_50', 'num_joints': 15, 'pos_dist_thresh': 17, 'project_path': '/content/cloned-DLC-repo/examples/walkdataset', 'rotation': 25, 'rotratio': 0.4, 'save_iters': 50000, 'scale_jitter_lo': 0.5, 'scale_jitter_up': 1.25, 'sharpen': False, 'sharpenratio': 0.3, 'covering': True, 'elastic_transform': True, 'motion_blur': True, 'motion_blur_params': {'k': 7, 'angle': (-90, 90)}}\n",
            "Starting training....\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "iteration: 10 loss: 0.5612 lr: 0.005\n",
            "iteration: 20 loss: 0.0750 lr: 0.005\n",
            "iteration: 30 loss: 0.0451 lr: 0.005\n",
            "iteration: 40 loss: 0.0353 lr: 0.005\n",
            "iteration: 50 loss: 0.0277 lr: 0.005\n",
            "iteration: 60 loss: 0.0282 lr: 0.005\n",
            "iteration: 70 loss: 0.0244 lr: 0.005\n",
            "iteration: 80 loss: 0.0257 lr: 0.005\n",
            "iteration: 90 loss: 0.0266 lr: 0.005\n",
            "iteration: 100 loss: 0.0267 lr: 0.005\n",
            "iteration: 110 loss: 0.0269 lr: 0.005\n",
            "iteration: 120 loss: 0.0260 lr: 0.005\n",
            "iteration: 130 loss: 0.0250 lr: 0.005\n",
            "iteration: 140 loss: 0.0231 lr: 0.005\n",
            "iteration: 150 loss: 0.0285 lr: 0.005\n",
            "iteration: 160 loss: 0.0245 lr: 0.005\n",
            "iteration: 170 loss: 0.0251 lr: 0.005\n",
            "iteration: 180 loss: 0.0234 lr: 0.005\n",
            "iteration: 190 loss: 0.0240 lr: 0.005\n",
            "iteration: 200 loss: 0.0255 lr: 0.005\n",
            "iteration: 210 loss: 0.0233 lr: 0.005\n",
            "iteration: 220 loss: 0.0256 lr: 0.005\n",
            "iteration: 230 loss: 0.0243 lr: 0.005\n",
            "iteration: 240 loss: 0.0213 lr: 0.005\n",
            "iteration: 250 loss: 0.0247 lr: 0.005\n",
            "iteration: 260 loss: 0.0222 lr: 0.005\n",
            "iteration: 270 loss: 0.0245 lr: 0.005\n",
            "iteration: 280 loss: 0.0203 lr: 0.005\n",
            "iteration: 290 loss: 0.0228 lr: 0.005\n",
            "iteration: 300 loss: 0.0251 lr: 0.005\n",
            "iteration: 310 loss: 0.0221 lr: 0.005\n",
            "iteration: 320 loss: 0.0225 lr: 0.005\n",
            "iteration: 330 loss: 0.0275 lr: 0.005\n",
            "iteration: 340 loss: 0.0228 lr: 0.005\n",
            "iteration: 350 loss: 0.0210 lr: 0.005\n",
            "iteration: 360 loss: 0.0224 lr: 0.005\n",
            "iteration: 370 loss: 0.0224 lr: 0.005\n",
            "iteration: 380 loss: 0.0219 lr: 0.005\n",
            "iteration: 390 loss: 0.0228 lr: 0.005\n",
            "iteration: 400 loss: 0.0227 lr: 0.005\n",
            "iteration: 410 loss: 0.0221 lr: 0.005\n",
            "iteration: 420 loss: 0.0242 lr: 0.005\n",
            "iteration: 430 loss: 0.0254 lr: 0.005\n",
            "iteration: 440 loss: 0.0244 lr: 0.005\n",
            "iteration: 450 loss: 0.0221 lr: 0.005\n",
            "iteration: 460 loss: 0.0224 lr: 0.005\n",
            "iteration: 470 loss: 0.0248 lr: 0.005\n",
            "iteration: 480 loss: 0.0227 lr: 0.005\n",
            "iteration: 490 loss: 0.0226 lr: 0.005\n",
            "iteration: 500 loss: 0.0208 lr: 0.005\n",
            "iteration: 510 loss: 0.0211 lr: 0.005\n",
            "iteration: 520 loss: 0.0242 lr: 0.005\n",
            "iteration: 530 loss: 0.0224 lr: 0.005\n",
            "iteration: 540 loss: 0.0222 lr: 0.005\n",
            "iteration: 550 loss: 0.0246 lr: 0.005\n",
            "iteration: 560 loss: 0.0230 lr: 0.005\n",
            "iteration: 570 loss: 0.0216 lr: 0.005\n",
            "iteration: 580 loss: 0.0243 lr: 0.005\n",
            "iteration: 590 loss: 0.0248 lr: 0.005\n",
            "iteration: 600 loss: 0.0232 lr: 0.005\n",
            "iteration: 610 loss: 0.0216 lr: 0.005\n",
            "iteration: 620 loss: 0.0224 lr: 0.005\n",
            "iteration: 630 loss: 0.0207 lr: 0.005\n",
            "iteration: 640 loss: 0.0199 lr: 0.005\n",
            "iteration: 650 loss: 0.0235 lr: 0.005\n",
            "iteration: 660 loss: 0.0186 lr: 0.005\n",
            "iteration: 670 loss: 0.0186 lr: 0.005\n",
            "iteration: 680 loss: 0.0256 lr: 0.005\n",
            "iteration: 690 loss: 0.0231 lr: 0.005\n",
            "iteration: 700 loss: 0.0255 lr: 0.005\n",
            "iteration: 710 loss: 0.0206 lr: 0.005\n",
            "iteration: 720 loss: 0.0235 lr: 0.005\n",
            "iteration: 730 loss: 0.0201 lr: 0.005\n",
            "iteration: 740 loss: 0.0203 lr: 0.005\n",
            "iteration: 750 loss: 0.0207 lr: 0.005\n",
            "iteration: 760 loss: 0.0218 lr: 0.005\n",
            "iteration: 770 loss: 0.0194 lr: 0.005\n",
            "iteration: 780 loss: 0.0230 lr: 0.005\n",
            "iteration: 790 loss: 0.0243 lr: 0.005\n",
            "iteration: 800 loss: 0.0209 lr: 0.005\n",
            "iteration: 810 loss: 0.0239 lr: 0.005\n",
            "iteration: 820 loss: 0.0212 lr: 0.005\n",
            "iteration: 830 loss: 0.0227 lr: 0.005\n",
            "iteration: 840 loss: 0.0202 lr: 0.005\n",
            "iteration: 850 loss: 0.0220 lr: 0.005\n",
            "iteration: 860 loss: 0.0226 lr: 0.005\n",
            "iteration: 870 loss: 0.0209 lr: 0.005\n",
            "iteration: 880 loss: 0.0235 lr: 0.005\n",
            "iteration: 890 loss: 0.0216 lr: 0.005\n",
            "iteration: 900 loss: 0.0202 lr: 0.005\n",
            "iteration: 910 loss: 0.0238 lr: 0.005\n",
            "iteration: 920 loss: 0.0232 lr: 0.005\n",
            "iteration: 930 loss: 0.0218 lr: 0.005\n",
            "iteration: 940 loss: 0.0209 lr: 0.005\n",
            "iteration: 950 loss: 0.0202 lr: 0.005\n",
            "iteration: 960 loss: 0.0222 lr: 0.005\n",
            "iteration: 970 loss: 0.0200 lr: 0.005\n",
            "iteration: 980 loss: 0.0220 lr: 0.005\n",
            "iteration: 990 loss: 0.0214 lr: 0.005\n",
            "iteration: 1000 loss: 0.0234 lr: 0.005\n",
            "iteration: 1010 loss: 0.0191 lr: 0.005\n",
            "iteration: 1020 loss: 0.0230 lr: 0.005\n",
            "iteration: 1030 loss: 0.0221 lr: 0.005\n",
            "iteration: 1040 loss: 0.0241 lr: 0.005\n",
            "iteration: 1050 loss: 0.0226 lr: 0.005\n",
            "iteration: 1060 loss: 0.0184 lr: 0.005\n",
            "iteration: 1070 loss: 0.0201 lr: 0.005\n",
            "iteration: 1080 loss: 0.0223 lr: 0.005\n",
            "iteration: 1090 loss: 0.0222 lr: 0.005\n",
            "iteration: 1100 loss: 0.0240 lr: 0.005\n",
            "iteration: 1110 loss: 0.0210 lr: 0.005\n",
            "iteration: 1120 loss: 0.0213 lr: 0.005\n",
            "iteration: 1130 loss: 0.0205 lr: 0.005\n",
            "iteration: 1140 loss: 0.0248 lr: 0.005\n",
            "iteration: 1150 loss: 0.0228 lr: 0.005\n",
            "iteration: 1160 loss: 0.0217 lr: 0.005\n",
            "iteration: 1170 loss: 0.0208 lr: 0.005\n",
            "iteration: 1180 loss: 0.0194 lr: 0.005\n",
            "iteration: 1190 loss: 0.0209 lr: 0.005\n",
            "iteration: 1200 loss: 0.0224 lr: 0.005\n",
            "iteration: 1210 loss: 0.0215 lr: 0.005\n",
            "iteration: 1220 loss: 0.0247 lr: 0.005\n",
            "iteration: 1230 loss: 0.0212 lr: 0.005\n",
            "iteration: 1240 loss: 0.0202 lr: 0.005\n",
            "iteration: 1250 loss: 0.0222 lr: 0.005\n",
            "iteration: 1260 loss: 0.0202 lr: 0.005\n",
            "iteration: 1270 loss: 0.0231 lr: 0.005\n",
            "iteration: 1280 loss: 0.0243 lr: 0.005\n",
            "iteration: 1290 loss: 0.0211 lr: 0.005\n",
            "iteration: 1300 loss: 0.0216 lr: 0.005\n",
            "iteration: 1310 loss: 0.0212 lr: 0.005\n",
            "iteration: 1320 loss: 0.0233 lr: 0.005\n",
            "iteration: 1330 loss: 0.0190 lr: 0.005\n",
            "iteration: 1340 loss: 0.0238 lr: 0.005\n",
            "iteration: 1350 loss: 0.0227 lr: 0.005\n",
            "iteration: 1360 loss: 0.0190 lr: 0.005\n",
            "iteration: 1370 loss: 0.0208 lr: 0.005\n",
            "iteration: 1380 loss: 0.0195 lr: 0.005\n",
            "iteration: 1390 loss: 0.0237 lr: 0.005\n",
            "iteration: 1400 loss: 0.0198 lr: 0.005\n",
            "iteration: 1410 loss: 0.0226 lr: 0.005\n",
            "iteration: 1420 loss: 0.0198 lr: 0.005\n",
            "iteration: 1430 loss: 0.0195 lr: 0.005\n",
            "iteration: 1440 loss: 0.0215 lr: 0.005\n",
            "iteration: 1450 loss: 0.0217 lr: 0.005\n",
            "iteration: 1460 loss: 0.0188 lr: 0.005\n",
            "iteration: 1470 loss: 0.0221 lr: 0.005\n",
            "iteration: 1480 loss: 0.0188 lr: 0.005\n",
            "iteration: 1490 loss: 0.0216 lr: 0.005\n",
            "iteration: 1500 loss: 0.0204 lr: 0.005\n",
            "iteration: 1510 loss: 0.0207 lr: 0.005\n",
            "iteration: 1520 loss: 0.0230 lr: 0.005\n",
            "iteration: 1530 loss: 0.0185 lr: 0.005\n",
            "iteration: 1540 loss: 0.0225 lr: 0.005\n",
            "iteration: 1550 loss: 0.0241 lr: 0.005\n",
            "iteration: 1560 loss: 0.0249 lr: 0.005\n",
            "iteration: 1570 loss: 0.0198 lr: 0.005\n",
            "iteration: 1580 loss: 0.0205 lr: 0.005\n",
            "iteration: 1590 loss: 0.0194 lr: 0.005\n",
            "iteration: 1600 loss: 0.0224 lr: 0.005\n",
            "iteration: 1610 loss: 0.0228 lr: 0.005\n",
            "iteration: 1620 loss: 0.0197 lr: 0.005\n",
            "iteration: 1630 loss: 0.0210 lr: 0.005\n",
            "iteration: 1640 loss: 0.0233 lr: 0.005\n",
            "iteration: 1650 loss: 0.0207 lr: 0.005\n",
            "iteration: 1660 loss: 0.0169 lr: 0.005\n",
            "iteration: 1670 loss: 0.0187 lr: 0.005\n",
            "iteration: 1680 loss: 0.0232 lr: 0.005\n",
            "iteration: 1690 loss: 0.0223 lr: 0.005\n",
            "iteration: 1700 loss: 0.0232 lr: 0.005\n",
            "iteration: 1710 loss: 0.0207 lr: 0.005\n",
            "iteration: 1720 loss: 0.0192 lr: 0.005\n",
            "iteration: 1730 loss: 0.0179 lr: 0.005\n",
            "iteration: 1740 loss: 0.0185 lr: 0.005\n",
            "iteration: 1750 loss: 0.0195 lr: 0.005\n",
            "iteration: 1760 loss: 0.0215 lr: 0.005\n",
            "iteration: 1770 loss: 0.0198 lr: 0.005\n",
            "iteration: 1780 loss: 0.0224 lr: 0.005\n",
            "iteration: 1790 loss: 0.0227 lr: 0.005\n",
            "iteration: 1800 loss: 0.0213 lr: 0.005\n",
            "iteration: 1810 loss: 0.0235 lr: 0.005\n",
            "iteration: 1820 loss: 0.0217 lr: 0.005\n",
            "iteration: 1830 loss: 0.0216 lr: 0.005\n",
            "iteration: 1840 loss: 0.0171 lr: 0.005\n",
            "iteration: 1850 loss: 0.0220 lr: 0.005\n",
            "iteration: 1860 loss: 0.0224 lr: 0.005\n",
            "iteration: 1870 loss: 0.0192 lr: 0.005\n",
            "iteration: 1880 loss: 0.0200 lr: 0.005\n",
            "iteration: 1890 loss: 0.0229 lr: 0.005\n",
            "iteration: 1900 loss: 0.0205 lr: 0.005\n",
            "iteration: 1910 loss: 0.0213 lr: 0.005\n",
            "iteration: 1920 loss: 0.0219 lr: 0.005\n",
            "iteration: 1930 loss: 0.0206 lr: 0.005\n",
            "iteration: 1940 loss: 0.0204 lr: 0.005\n",
            "iteration: 1950 loss: 0.0186 lr: 0.005\n",
            "iteration: 1960 loss: 0.0179 lr: 0.005\n",
            "iteration: 1970 loss: 0.0196 lr: 0.005\n",
            "iteration: 1980 loss: 0.0183 lr: 0.005\n",
            "iteration: 1990 loss: 0.0216 lr: 0.005\n",
            "iteration: 2000 loss: 0.0220 lr: 0.005\n",
            "iteration: 2010 loss: 0.0197 lr: 0.005\n",
            "iteration: 2020 loss: 0.0215 lr: 0.005\n",
            "iteration: 2030 loss: 0.0213 lr: 0.005\n",
            "iteration: 2040 loss: 0.0204 lr: 0.005\n",
            "iteration: 2050 loss: 0.0201 lr: 0.005\n",
            "iteration: 2060 loss: 0.0178 lr: 0.005\n",
            "iteration: 2070 loss: 0.0200 lr: 0.005\n",
            "iteration: 2080 loss: 0.0197 lr: 0.005\n",
            "iteration: 2090 loss: 0.0168 lr: 0.005\n",
            "iteration: 2100 loss: 0.0218 lr: 0.005\n",
            "iteration: 2110 loss: 0.0184 lr: 0.005\n",
            "iteration: 2120 loss: 0.0208 lr: 0.005\n",
            "iteration: 2130 loss: 0.0196 lr: 0.005\n",
            "iteration: 2140 loss: 0.0174 lr: 0.005\n",
            "iteration: 2150 loss: 0.0201 lr: 0.005\n",
            "iteration: 2160 loss: 0.0205 lr: 0.005\n",
            "iteration: 2170 loss: 0.0206 lr: 0.005\n",
            "iteration: 2180 loss: 0.0190 lr: 0.005\n",
            "iteration: 2190 loss: 0.0171 lr: 0.005\n",
            "iteration: 2200 loss: 0.0181 lr: 0.005\n",
            "iteration: 2210 loss: 0.0193 lr: 0.005\n",
            "iteration: 2220 loss: 0.0199 lr: 0.005\n",
            "iteration: 2230 loss: 0.0212 lr: 0.005\n",
            "iteration: 2240 loss: 0.0220 lr: 0.005\n",
            "iteration: 2250 loss: 0.0218 lr: 0.005\n",
            "iteration: 2260 loss: 0.0202 lr: 0.005\n",
            "iteration: 2270 loss: 0.0202 lr: 0.005\n",
            "iteration: 2280 loss: 0.0163 lr: 0.005\n",
            "iteration: 2290 loss: 0.0215 lr: 0.005\n",
            "iteration: 2300 loss: 0.0209 lr: 0.005\n",
            "iteration: 2310 loss: 0.0187 lr: 0.005\n",
            "iteration: 2320 loss: 0.0181 lr: 0.005\n",
            "iteration: 2330 loss: 0.0200 lr: 0.005\n",
            "iteration: 2340 loss: 0.0219 lr: 0.005\n",
            "iteration: 2350 loss: 0.0190 lr: 0.005\n",
            "iteration: 2360 loss: 0.0193 lr: 0.005\n",
            "iteration: 2370 loss: 0.0208 lr: 0.005\n",
            "iteration: 2380 loss: 0.0209 lr: 0.005\n",
            "iteration: 2390 loss: 0.0214 lr: 0.005\n",
            "iteration: 2400 loss: 0.0197 lr: 0.005\n",
            "iteration: 2410 loss: 0.0189 lr: 0.005\n",
            "iteration: 2420 loss: 0.0182 lr: 0.005\n",
            "iteration: 2430 loss: 0.0197 lr: 0.005\n",
            "iteration: 2440 loss: 0.0176 lr: 0.005\n",
            "iteration: 2450 loss: 0.0204 lr: 0.005\n",
            "iteration: 2460 loss: 0.0194 lr: 0.005\n",
            "iteration: 2470 loss: 0.0196 lr: 0.005\n",
            "iteration: 2480 loss: 0.0180 lr: 0.005\n",
            "iteration: 2490 loss: 0.0198 lr: 0.005\n",
            "iteration: 2500 loss: 0.0222 lr: 0.005\n",
            "iteration: 2510 loss: 0.0203 lr: 0.005\n",
            "iteration: 2520 loss: 0.0179 lr: 0.005\n",
            "iteration: 2530 loss: 0.0200 lr: 0.005\n",
            "iteration: 2540 loss: 0.0188 lr: 0.005\n",
            "iteration: 2550 loss: 0.0197 lr: 0.005\n",
            "iteration: 2560 loss: 0.0189 lr: 0.005\n",
            "iteration: 2570 loss: 0.0180 lr: 0.005\n",
            "iteration: 2580 loss: 0.0154 lr: 0.005\n",
            "iteration: 2590 loss: 0.0202 lr: 0.005\n",
            "iteration: 2600 loss: 0.0180 lr: 0.005\n",
            "iteration: 2610 loss: 0.0197 lr: 0.005\n",
            "iteration: 2620 loss: 0.0200 lr: 0.005\n",
            "iteration: 2630 loss: 0.0210 lr: 0.005\n",
            "iteration: 2640 loss: 0.0176 lr: 0.005\n",
            "iteration: 2650 loss: 0.0192 lr: 0.005\n",
            "iteration: 2660 loss: 0.0189 lr: 0.005\n",
            "iteration: 2670 loss: 0.0213 lr: 0.005\n",
            "iteration: 2680 loss: 0.0194 lr: 0.005\n",
            "iteration: 2690 loss: 0.0201 lr: 0.005\n",
            "iteration: 2700 loss: 0.0194 lr: 0.005\n",
            "iteration: 2710 loss: 0.0192 lr: 0.005\n",
            "iteration: 2720 loss: 0.0190 lr: 0.005\n",
            "iteration: 2730 loss: 0.0177 lr: 0.005\n",
            "iteration: 2740 loss: 0.0211 lr: 0.005\n",
            "iteration: 2750 loss: 0.0213 lr: 0.005\n",
            "iteration: 2760 loss: 0.0184 lr: 0.005\n",
            "iteration: 2770 loss: 0.0215 lr: 0.005\n",
            "iteration: 2780 loss: 0.0214 lr: 0.005\n",
            "iteration: 2790 loss: 0.0212 lr: 0.005\n",
            "iteration: 2800 loss: 0.0175 lr: 0.005\n",
            "iteration: 2810 loss: 0.0212 lr: 0.005\n",
            "iteration: 2820 loss: 0.0175 lr: 0.005\n",
            "iteration: 2830 loss: 0.0180 lr: 0.005\n",
            "iteration: 2840 loss: 0.0145 lr: 0.005\n",
            "iteration: 2850 loss: 0.0176 lr: 0.005\n",
            "iteration: 2860 loss: 0.0201 lr: 0.005\n",
            "iteration: 2870 loss: 0.0183 lr: 0.005\n",
            "iteration: 2880 loss: 0.0186 lr: 0.005\n",
            "iteration: 2890 loss: 0.0184 lr: 0.005\n",
            "iteration: 2900 loss: 0.0189 lr: 0.005\n",
            "iteration: 2910 loss: 0.0186 lr: 0.005\n",
            "iteration: 2920 loss: 0.0187 lr: 0.005\n",
            "iteration: 2930 loss: 0.0197 lr: 0.005\n",
            "iteration: 2940 loss: 0.0173 lr: 0.005\n",
            "iteration: 2950 loss: 0.0185 lr: 0.005\n",
            "iteration: 2960 loss: 0.0181 lr: 0.005\n",
            "iteration: 2970 loss: 0.0193 lr: 0.005\n",
            "iteration: 2980 loss: 0.0202 lr: 0.005\n",
            "iteration: 2990 loss: 0.0185 lr: 0.005\n",
            "iteration: 3000 loss: 0.0196 lr: 0.005\n",
            "iteration: 3010 loss: 0.0177 lr: 0.005\n",
            "iteration: 3020 loss: 0.0182 lr: 0.005\n",
            "iteration: 3030 loss: 0.0190 lr: 0.005\n",
            "iteration: 3040 loss: 0.0195 lr: 0.005\n",
            "iteration: 3050 loss: 0.0177 lr: 0.005\n",
            "iteration: 3060 loss: 0.0166 lr: 0.005\n",
            "iteration: 3070 loss: 0.0187 lr: 0.005\n",
            "iteration: 3080 loss: 0.0190 lr: 0.005\n",
            "iteration: 3090 loss: 0.0186 lr: 0.005\n",
            "iteration: 3100 loss: 0.0171 lr: 0.005\n",
            "iteration: 3110 loss: 0.0159 lr: 0.005\n",
            "iteration: 3120 loss: 0.0148 lr: 0.005\n",
            "iteration: 3130 loss: 0.0155 lr: 0.005\n",
            "iteration: 3140 loss: 0.0192 lr: 0.005\n",
            "iteration: 3150 loss: 0.0174 lr: 0.005\n",
            "iteration: 3160 loss: 0.0199 lr: 0.005\n",
            "iteration: 3170 loss: 0.0180 lr: 0.005\n",
            "iteration: 3180 loss: 0.0202 lr: 0.005\n",
            "iteration: 3190 loss: 0.0178 lr: 0.005\n",
            "iteration: 3200 loss: 0.0169 lr: 0.005\n",
            "iteration: 3210 loss: 0.0195 lr: 0.005\n",
            "iteration: 3220 loss: 0.0203 lr: 0.005\n",
            "iteration: 3230 loss: 0.0200 lr: 0.005\n",
            "iteration: 3240 loss: 0.0174 lr: 0.005\n",
            "iteration: 3250 loss: 0.0188 lr: 0.005\n",
            "iteration: 3260 loss: 0.0175 lr: 0.005\n",
            "iteration: 3270 loss: 0.0160 lr: 0.005\n",
            "iteration: 3280 loss: 0.0191 lr: 0.005\n",
            "iteration: 3290 loss: 0.0165 lr: 0.005\n",
            "iteration: 3300 loss: 0.0179 lr: 0.005\n",
            "iteration: 3310 loss: 0.0145 lr: 0.005\n",
            "iteration: 3320 loss: 0.0202 lr: 0.005\n",
            "iteration: 3330 loss: 0.0174 lr: 0.005\n",
            "iteration: 3340 loss: 0.0176 lr: 0.005\n",
            "iteration: 3350 loss: 0.0166 lr: 0.005\n",
            "iteration: 3360 loss: 0.0190 lr: 0.005\n",
            "iteration: 3370 loss: 0.0179 lr: 0.005\n",
            "iteration: 3380 loss: 0.0176 lr: 0.005\n",
            "iteration: 3390 loss: 0.0162 lr: 0.005\n",
            "iteration: 3400 loss: 0.0159 lr: 0.005\n",
            "iteration: 3410 loss: 0.0163 lr: 0.005\n",
            "iteration: 3420 loss: 0.0178 lr: 0.005\n",
            "iteration: 3430 loss: 0.0183 lr: 0.005\n",
            "iteration: 3440 loss: 0.0157 lr: 0.005\n",
            "iteration: 3450 loss: 0.0158 lr: 0.005\n",
            "iteration: 3460 loss: 0.0162 lr: 0.005\n",
            "iteration: 3470 loss: 0.0203 lr: 0.005\n",
            "iteration: 3480 loss: 0.0182 lr: 0.005\n",
            "iteration: 3490 loss: 0.0182 lr: 0.005\n",
            "iteration: 3500 loss: 0.0187 lr: 0.005\n",
            "iteration: 3510 loss: 0.0199 lr: 0.005\n",
            "iteration: 3520 loss: 0.0179 lr: 0.005\n",
            "iteration: 3530 loss: 0.0175 lr: 0.005\n",
            "iteration: 3540 loss: 0.0186 lr: 0.005\n",
            "iteration: 3550 loss: 0.0164 lr: 0.005\n",
            "iteration: 3560 loss: 0.0176 lr: 0.005\n",
            "iteration: 3570 loss: 0.0178 lr: 0.005\n",
            "iteration: 3580 loss: 0.0181 lr: 0.005\n",
            "iteration: 3590 loss: 0.0179 lr: 0.005\n",
            "iteration: 3600 loss: 0.0144 lr: 0.005\n",
            "iteration: 3610 loss: 0.0158 lr: 0.005\n",
            "iteration: 3620 loss: 0.0166 lr: 0.005\n",
            "iteration: 3630 loss: 0.0173 lr: 0.005\n",
            "iteration: 3640 loss: 0.0177 lr: 0.005\n",
            "iteration: 3650 loss: 0.0155 lr: 0.005\n",
            "iteration: 3660 loss: 0.0183 lr: 0.005\n",
            "iteration: 3670 loss: 0.0165 lr: 0.005\n",
            "iteration: 3680 loss: 0.0178 lr: 0.005\n",
            "iteration: 3690 loss: 0.0180 lr: 0.005\n",
            "iteration: 3700 loss: 0.0189 lr: 0.005\n",
            "iteration: 3710 loss: 0.0171 lr: 0.005\n",
            "iteration: 3720 loss: 0.0190 lr: 0.005\n",
            "iteration: 3730 loss: 0.0193 lr: 0.005\n",
            "iteration: 3740 loss: 0.0182 lr: 0.005\n",
            "iteration: 3750 loss: 0.0167 lr: 0.005\n",
            "iteration: 3760 loss: 0.0173 lr: 0.005\n",
            "iteration: 3770 loss: 0.0161 lr: 0.005\n",
            "iteration: 3780 loss: 0.0184 lr: 0.005\n",
            "iteration: 3790 loss: 0.0136 lr: 0.005\n",
            "iteration: 3800 loss: 0.0163 lr: 0.005\n",
            "iteration: 3810 loss: 0.0182 lr: 0.005\n",
            "iteration: 3820 loss: 0.0177 lr: 0.005\n",
            "iteration: 3830 loss: 0.0179 lr: 0.005\n",
            "iteration: 3840 loss: 0.0181 lr: 0.005\n",
            "iteration: 3850 loss: 0.0164 lr: 0.005\n",
            "iteration: 3860 loss: 0.0174 lr: 0.005\n",
            "iteration: 3870 loss: 0.0168 lr: 0.005\n",
            "iteration: 3880 loss: 0.0172 lr: 0.005\n",
            "iteration: 3890 loss: 0.0157 lr: 0.005\n",
            "iteration: 3900 loss: 0.0169 lr: 0.005\n",
            "iteration: 3910 loss: 0.0171 lr: 0.005\n",
            "iteration: 3920 loss: 0.0176 lr: 0.005\n",
            "iteration: 3930 loss: 0.0174 lr: 0.005\n",
            "iteration: 3940 loss: 0.0176 lr: 0.005\n",
            "iteration: 3950 loss: 0.0160 lr: 0.005\n",
            "iteration: 3960 loss: 0.0178 lr: 0.005\n",
            "iteration: 3970 loss: 0.0196 lr: 0.005\n",
            "iteration: 3980 loss: 0.0166 lr: 0.005\n",
            "iteration: 3990 loss: 0.0178 lr: 0.005\n",
            "iteration: 4000 loss: 0.0168 lr: 0.005\n",
            "iteration: 4010 loss: 0.0161 lr: 0.005\n",
            "iteration: 4020 loss: 0.0166 lr: 0.005\n",
            "iteration: 4030 loss: 0.0185 lr: 0.005\n",
            "iteration: 4040 loss: 0.0143 lr: 0.005\n",
            "iteration: 4050 loss: 0.0173 lr: 0.005\n",
            "iteration: 4060 loss: 0.0170 lr: 0.005\n",
            "iteration: 4070 loss: 0.0182 lr: 0.005\n",
            "iteration: 4080 loss: 0.0150 lr: 0.005\n",
            "iteration: 4090 loss: 0.0160 lr: 0.005\n",
            "iteration: 4100 loss: 0.0162 lr: 0.005\n",
            "iteration: 4110 loss: 0.0160 lr: 0.005\n",
            "iteration: 4120 loss: 0.0176 lr: 0.005\n",
            "iteration: 4130 loss: 0.0169 lr: 0.005\n",
            "iteration: 4140 loss: 0.0162 lr: 0.005\n",
            "iteration: 4150 loss: 0.0169 lr: 0.005\n",
            "iteration: 4160 loss: 0.0167 lr: 0.005\n",
            "iteration: 4170 loss: 0.0166 lr: 0.005\n",
            "iteration: 4180 loss: 0.0182 lr: 0.005\n",
            "iteration: 4190 loss: 0.0180 lr: 0.005\n",
            "iteration: 4200 loss: 0.0152 lr: 0.005\n",
            "iteration: 4210 loss: 0.0171 lr: 0.005\n",
            "iteration: 4220 loss: 0.0142 lr: 0.005\n",
            "iteration: 4230 loss: 0.0151 lr: 0.005\n",
            "iteration: 4240 loss: 0.0160 lr: 0.005\n",
            "iteration: 4250 loss: 0.0155 lr: 0.005\n",
            "iteration: 4260 loss: 0.0173 lr: 0.005\n",
            "iteration: 4270 loss: 0.0156 lr: 0.005\n",
            "iteration: 4280 loss: 0.0156 lr: 0.005\n",
            "iteration: 4290 loss: 0.0161 lr: 0.005\n",
            "iteration: 4300 loss: 0.0164 lr: 0.005\n",
            "iteration: 4310 loss: 0.0161 lr: 0.005\n",
            "iteration: 4320 loss: 0.0161 lr: 0.005\n",
            "iteration: 4330 loss: 0.0155 lr: 0.005\n",
            "iteration: 4340 loss: 0.0148 lr: 0.005\n",
            "iteration: 4350 loss: 0.0164 lr: 0.005\n",
            "iteration: 4360 loss: 0.0181 lr: 0.005\n",
            "iteration: 4370 loss: 0.0164 lr: 0.005\n",
            "iteration: 4380 loss: 0.0168 lr: 0.005\n",
            "iteration: 4390 loss: 0.0169 lr: 0.005\n",
            "iteration: 4400 loss: 0.0186 lr: 0.005\n",
            "iteration: 4410 loss: 0.0164 lr: 0.005\n",
            "iteration: 4420 loss: 0.0158 lr: 0.005\n",
            "iteration: 4430 loss: 0.0152 lr: 0.005\n",
            "iteration: 4440 loss: 0.0151 lr: 0.005\n",
            "iteration: 4450 loss: 0.0130 lr: 0.005\n",
            "iteration: 4460 loss: 0.0181 lr: 0.005\n",
            "iteration: 4470 loss: 0.0169 lr: 0.005\n",
            "iteration: 4480 loss: 0.0144 lr: 0.005\n",
            "iteration: 4490 loss: 0.0172 lr: 0.005\n",
            "iteration: 4500 loss: 0.0141 lr: 0.005\n",
            "iteration: 4510 loss: 0.0153 lr: 0.005\n",
            "iteration: 4520 loss: 0.0163 lr: 0.005\n",
            "iteration: 4530 loss: 0.0178 lr: 0.005\n",
            "iteration: 4540 loss: 0.0161 lr: 0.005\n",
            "iteration: 4550 loss: 0.0141 lr: 0.005\n",
            "iteration: 4560 loss: 0.0187 lr: 0.005\n",
            "iteration: 4570 loss: 0.0158 lr: 0.005\n",
            "iteration: 4580 loss: 0.0152 lr: 0.005\n",
            "iteration: 4590 loss: 0.0155 lr: 0.005\n",
            "iteration: 4600 loss: 0.0143 lr: 0.005\n",
            "iteration: 4610 loss: 0.0173 lr: 0.005\n",
            "iteration: 4620 loss: 0.0147 lr: 0.005\n",
            "iteration: 4630 loss: 0.0164 lr: 0.005\n",
            "iteration: 4640 loss: 0.0152 lr: 0.005\n",
            "iteration: 4650 loss: 0.0137 lr: 0.005\n",
            "iteration: 4660 loss: 0.0163 lr: 0.005\n",
            "iteration: 4670 loss: 0.0144 lr: 0.005\n",
            "iteration: 4680 loss: 0.0155 lr: 0.005\n",
            "iteration: 4690 loss: 0.0140 lr: 0.005\n",
            "iteration: 4700 loss: 0.0162 lr: 0.005\n",
            "iteration: 4710 loss: 0.0164 lr: 0.005\n",
            "iteration: 4720 loss: 0.0143 lr: 0.005\n",
            "iteration: 4730 loss: 0.0166 lr: 0.005\n",
            "iteration: 4740 loss: 0.0150 lr: 0.005\n",
            "iteration: 4750 loss: 0.0149 lr: 0.005\n",
            "iteration: 4760 loss: 0.0130 lr: 0.005\n",
            "iteration: 4770 loss: 0.0173 lr: 0.005\n",
            "iteration: 4780 loss: 0.0196 lr: 0.005\n",
            "iteration: 4790 loss: 0.0173 lr: 0.005\n",
            "iteration: 4800 loss: 0.0161 lr: 0.005\n",
            "iteration: 4810 loss: 0.0170 lr: 0.005\n",
            "iteration: 4820 loss: 0.0159 lr: 0.005\n",
            "iteration: 4830 loss: 0.0170 lr: 0.005\n",
            "iteration: 4840 loss: 0.0179 lr: 0.005\n",
            "iteration: 4850 loss: 0.0155 lr: 0.005\n",
            "iteration: 4860 loss: 0.0169 lr: 0.005\n",
            "iteration: 4870 loss: 0.0166 lr: 0.005\n",
            "iteration: 4880 loss: 0.0170 lr: 0.005\n",
            "iteration: 4890 loss: 0.0177 lr: 0.005\n",
            "iteration: 4900 loss: 0.0129 lr: 0.005\n",
            "iteration: 4910 loss: 0.0172 lr: 0.005\n",
            "iteration: 4920 loss: 0.0149 lr: 0.005\n",
            "iteration: 4930 loss: 0.0138 lr: 0.005\n",
            "iteration: 4940 loss: 0.0150 lr: 0.005\n",
            "iteration: 4950 loss: 0.0169 lr: 0.005\n",
            "iteration: 4960 loss: 0.0145 lr: 0.005\n",
            "iteration: 4970 loss: 0.0154 lr: 0.005\n",
            "iteration: 4980 loss: 0.0168 lr: 0.005\n",
            "iteration: 4990 loss: 0.0157 lr: 0.005\n",
            "iteration: 5000 loss: 0.0129 lr: 0.005\n",
            "iteration: 5010 loss: 0.0137 lr: 0.005\n",
            "iteration: 5020 loss: 0.0150 lr: 0.005\n",
            "iteration: 5030 loss: 0.0154 lr: 0.005\n",
            "iteration: 5040 loss: 0.0148 lr: 0.005\n",
            "iteration: 5050 loss: 0.0127 lr: 0.005\n",
            "iteration: 5060 loss: 0.0162 lr: 0.005\n",
            "iteration: 5070 loss: 0.0178 lr: 0.005\n",
            "iteration: 5080 loss: 0.0141 lr: 0.005\n",
            "iteration: 5090 loss: 0.0167 lr: 0.005\n",
            "iteration: 5100 loss: 0.0158 lr: 0.005\n",
            "iteration: 5110 loss: 0.0176 lr: 0.005\n",
            "iteration: 5120 loss: 0.0138 lr: 0.005\n",
            "iteration: 5130 loss: 0.0155 lr: 0.005\n",
            "iteration: 5140 loss: 0.0165 lr: 0.005\n",
            "iteration: 5150 loss: 0.0130 lr: 0.005\n",
            "iteration: 5160 loss: 0.0151 lr: 0.005\n",
            "iteration: 5170 loss: 0.0154 lr: 0.005\n",
            "iteration: 5180 loss: 0.0128 lr: 0.005\n",
            "iteration: 5190 loss: 0.0152 lr: 0.005\n",
            "iteration: 5200 loss: 0.0149 lr: 0.005\n",
            "iteration: 5210 loss: 0.0173 lr: 0.005\n",
            "iteration: 5220 loss: 0.0148 lr: 0.005\n",
            "iteration: 5230 loss: 0.0148 lr: 0.005\n",
            "iteration: 5240 loss: 0.0142 lr: 0.005\n",
            "iteration: 5250 loss: 0.0151 lr: 0.005\n",
            "iteration: 5260 loss: 0.0151 lr: 0.005\n",
            "iteration: 5270 loss: 0.0135 lr: 0.005\n",
            "iteration: 5280 loss: 0.0128 lr: 0.005\n",
            "iteration: 5290 loss: 0.0148 lr: 0.005\n",
            "iteration: 5300 loss: 0.0147 lr: 0.005\n",
            "iteration: 5310 loss: 0.0167 lr: 0.005\n",
            "iteration: 5320 loss: 0.0170 lr: 0.005\n",
            "iteration: 5330 loss: 0.0132 lr: 0.005\n",
            "iteration: 5340 loss: 0.0148 lr: 0.005\n",
            "iteration: 5350 loss: 0.0145 lr: 0.005\n",
            "iteration: 5360 loss: 0.0137 lr: 0.005\n",
            "iteration: 5370 loss: 0.0158 lr: 0.005\n",
            "iteration: 5380 loss: 0.0149 lr: 0.005\n",
            "iteration: 5390 loss: 0.0144 lr: 0.005\n",
            "iteration: 5400 loss: 0.0118 lr: 0.005\n",
            "iteration: 5410 loss: 0.0147 lr: 0.005\n",
            "iteration: 5420 loss: 0.0130 lr: 0.005\n",
            "iteration: 5430 loss: 0.0125 lr: 0.005\n",
            "iteration: 5440 loss: 0.0143 lr: 0.005\n",
            "iteration: 5450 loss: 0.0146 lr: 0.005\n",
            "iteration: 5460 loss: 0.0138 lr: 0.005\n",
            "iteration: 5470 loss: 0.0137 lr: 0.005\n",
            "iteration: 5480 loss: 0.0161 lr: 0.005\n",
            "iteration: 5490 loss: 0.0129 lr: 0.005\n",
            "iteration: 5500 loss: 0.0147 lr: 0.005\n",
            "iteration: 5510 loss: 0.0136 lr: 0.005\n",
            "iteration: 5520 loss: 0.0147 lr: 0.005\n",
            "iteration: 5530 loss: 0.0150 lr: 0.005\n",
            "iteration: 5540 loss: 0.0134 lr: 0.005\n",
            "iteration: 5550 loss: 0.0138 lr: 0.005\n",
            "iteration: 5560 loss: 0.0128 lr: 0.005\n",
            "iteration: 5570 loss: 0.0142 lr: 0.005\n",
            "iteration: 5580 loss: 0.0127 lr: 0.005\n",
            "iteration: 5590 loss: 0.0142 lr: 0.005\n",
            "iteration: 5600 loss: 0.0167 lr: 0.005\n",
            "iteration: 5610 loss: 0.0152 lr: 0.005\n",
            "iteration: 5620 loss: 0.0125 lr: 0.005\n",
            "iteration: 5630 loss: 0.0157 lr: 0.005\n",
            "iteration: 5640 loss: 0.0167 lr: 0.005\n",
            "iteration: 5650 loss: 0.0159 lr: 0.005\n",
            "iteration: 5660 loss: 0.0143 lr: 0.005\n",
            "iteration: 5670 loss: 0.0128 lr: 0.005\n",
            "iteration: 5680 loss: 0.0129 lr: 0.005\n",
            "iteration: 5690 loss: 0.0154 lr: 0.005\n",
            "iteration: 5700 loss: 0.0144 lr: 0.005\n",
            "iteration: 5710 loss: 0.0126 lr: 0.005\n",
            "iteration: 5720 loss: 0.0140 lr: 0.005\n",
            "iteration: 5730 loss: 0.0150 lr: 0.005\n",
            "iteration: 5740 loss: 0.0169 lr: 0.005\n",
            "iteration: 5750 loss: 0.0161 lr: 0.005\n",
            "iteration: 5760 loss: 0.0151 lr: 0.005\n",
            "iteration: 5770 loss: 0.0134 lr: 0.005\n",
            "iteration: 5780 loss: 0.0157 lr: 0.005\n",
            "iteration: 5790 loss: 0.0117 lr: 0.005\n",
            "iteration: 5800 loss: 0.0167 lr: 0.005\n",
            "iteration: 5810 loss: 0.0158 lr: 0.005\n",
            "iteration: 5820 loss: 0.0175 lr: 0.005\n",
            "iteration: 5830 loss: 0.0158 lr: 0.005\n",
            "iteration: 5840 loss: 0.0159 lr: 0.005\n",
            "iteration: 5850 loss: 0.0155 lr: 0.005\n",
            "iteration: 5860 loss: 0.0117 lr: 0.005\n",
            "iteration: 5870 loss: 0.0134 lr: 0.005\n",
            "iteration: 5880 loss: 0.0145 lr: 0.005\n",
            "iteration: 5890 loss: 0.0154 lr: 0.005\n",
            "iteration: 5900 loss: 0.0137 lr: 0.005\n",
            "iteration: 5910 loss: 0.0142 lr: 0.005\n",
            "iteration: 5920 loss: 0.0139 lr: 0.005\n",
            "iteration: 5930 loss: 0.0144 lr: 0.005\n",
            "iteration: 5940 loss: 0.0146 lr: 0.005\n",
            "iteration: 5950 loss: 0.0146 lr: 0.005\n",
            "iteration: 5960 loss: 0.0149 lr: 0.005\n",
            "iteration: 5970 loss: 0.0149 lr: 0.005\n",
            "iteration: 5980 loss: 0.0115 lr: 0.005\n",
            "iteration: 5990 loss: 0.0129 lr: 0.005\n",
            "iteration: 6000 loss: 0.0139 lr: 0.005\n",
            "iteration: 6010 loss: 0.0136 lr: 0.005\n",
            "iteration: 6020 loss: 0.0129 lr: 0.005\n",
            "iteration: 6030 loss: 0.0124 lr: 0.005\n",
            "iteration: 6040 loss: 0.0123 lr: 0.005\n",
            "iteration: 6050 loss: 0.0139 lr: 0.005\n",
            "iteration: 6060 loss: 0.0121 lr: 0.005\n",
            "iteration: 6070 loss: 0.0152 lr: 0.005\n",
            "iteration: 6080 loss: 0.0124 lr: 0.005\n",
            "iteration: 6090 loss: 0.0149 lr: 0.005\n",
            "iteration: 6100 loss: 0.0149 lr: 0.005\n",
            "iteration: 6110 loss: 0.0119 lr: 0.005\n",
            "iteration: 6120 loss: 0.0138 lr: 0.005\n",
            "iteration: 6130 loss: 0.0136 lr: 0.005\n",
            "iteration: 6140 loss: 0.0132 lr: 0.005\n",
            "iteration: 6150 loss: 0.0142 lr: 0.005\n",
            "iteration: 6160 loss: 0.0139 lr: 0.005\n",
            "iteration: 6170 loss: 0.0150 lr: 0.005\n",
            "iteration: 6180 loss: 0.0133 lr: 0.005\n",
            "iteration: 6190 loss: 0.0129 lr: 0.005\n",
            "iteration: 6200 loss: 0.0126 lr: 0.005\n",
            "iteration: 6210 loss: 0.0147 lr: 0.005\n",
            "iteration: 6220 loss: 0.0134 lr: 0.005\n",
            "iteration: 6230 loss: 0.0131 lr: 0.005\n",
            "iteration: 6240 loss: 0.0143 lr: 0.005\n",
            "iteration: 6250 loss: 0.0131 lr: 0.005\n",
            "iteration: 6260 loss: 0.0136 lr: 0.005\n",
            "iteration: 6270 loss: 0.0118 lr: 0.005\n",
            "iteration: 6280 loss: 0.0126 lr: 0.005\n",
            "iteration: 6290 loss: 0.0143 lr: 0.005\n",
            "iteration: 6300 loss: 0.0118 lr: 0.005\n",
            "iteration: 6310 loss: 0.0124 lr: 0.005\n",
            "iteration: 6320 loss: 0.0131 lr: 0.005\n",
            "iteration: 6330 loss: 0.0169 lr: 0.005\n",
            "iteration: 6340 loss: 0.0144 lr: 0.005\n",
            "iteration: 6350 loss: 0.0142 lr: 0.005\n",
            "iteration: 6360 loss: 0.0138 lr: 0.005\n",
            "iteration: 6370 loss: 0.0136 lr: 0.005\n",
            "iteration: 6380 loss: 0.0113 lr: 0.005\n",
            "iteration: 6390 loss: 0.0167 lr: 0.005\n",
            "iteration: 6400 loss: 0.0150 lr: 0.005\n",
            "iteration: 6410 loss: 0.0126 lr: 0.005\n",
            "iteration: 6420 loss: 0.0131 lr: 0.005\n",
            "iteration: 6430 loss: 0.0162 lr: 0.005\n",
            "iteration: 6440 loss: 0.0133 lr: 0.005\n",
            "iteration: 6450 loss: 0.0147 lr: 0.005\n",
            "iteration: 6460 loss: 0.0123 lr: 0.005\n",
            "iteration: 6470 loss: 0.0142 lr: 0.005\n",
            "iteration: 6480 loss: 0.0143 lr: 0.005\n",
            "iteration: 6490 loss: 0.0130 lr: 0.005\n",
            "iteration: 6500 loss: 0.0123 lr: 0.005\n",
            "iteration: 6510 loss: 0.0150 lr: 0.005\n",
            "iteration: 6520 loss: 0.0113 lr: 0.005\n",
            "iteration: 6530 loss: 0.0131 lr: 0.005\n",
            "iteration: 6540 loss: 0.0110 lr: 0.005\n",
            "iteration: 6550 loss: 0.0133 lr: 0.005\n",
            "iteration: 6560 loss: 0.0137 lr: 0.005\n",
            "iteration: 6570 loss: 0.0155 lr: 0.005\n",
            "iteration: 6580 loss: 0.0128 lr: 0.005\n",
            "iteration: 6590 loss: 0.0132 lr: 0.005\n",
            "iteration: 6600 loss: 0.0131 lr: 0.005\n",
            "iteration: 6610 loss: 0.0125 lr: 0.005\n",
            "iteration: 6620 loss: 0.0138 lr: 0.005\n",
            "iteration: 6630 loss: 0.0149 lr: 0.005\n",
            "iteration: 6640 loss: 0.0149 lr: 0.005\n",
            "iteration: 6650 loss: 0.0148 lr: 0.005\n",
            "iteration: 6660 loss: 0.0151 lr: 0.005\n",
            "iteration: 6670 loss: 0.0147 lr: 0.005\n",
            "iteration: 6680 loss: 0.0122 lr: 0.005\n",
            "iteration: 6690 loss: 0.0148 lr: 0.005\n",
            "iteration: 6700 loss: 0.0116 lr: 0.005\n",
            "iteration: 6710 loss: 0.0121 lr: 0.005\n",
            "iteration: 6720 loss: 0.0114 lr: 0.005\n",
            "iteration: 6730 loss: 0.0099 lr: 0.005\n",
            "iteration: 6740 loss: 0.0117 lr: 0.005\n",
            "iteration: 6750 loss: 0.0136 lr: 0.005\n",
            "iteration: 6760 loss: 0.0137 lr: 0.005\n",
            "iteration: 6770 loss: 0.0149 lr: 0.005\n",
            "iteration: 6780 loss: 0.0121 lr: 0.005\n",
            "iteration: 6790 loss: 0.0106 lr: 0.005\n",
            "iteration: 6800 loss: 0.0137 lr: 0.005\n",
            "iteration: 6810 loss: 0.0112 lr: 0.005\n",
            "iteration: 6820 loss: 0.0128 lr: 0.005\n",
            "iteration: 6830 loss: 0.0144 lr: 0.005\n",
            "iteration: 6840 loss: 0.0123 lr: 0.005\n",
            "iteration: 6850 loss: 0.0132 lr: 0.005\n",
            "iteration: 6860 loss: 0.0140 lr: 0.005\n",
            "iteration: 6870 loss: 0.0141 lr: 0.005\n",
            "iteration: 6880 loss: 0.0144 lr: 0.005\n",
            "iteration: 6890 loss: 0.0122 lr: 0.005\n",
            "iteration: 6900 loss: 0.0119 lr: 0.005\n",
            "iteration: 6910 loss: 0.0134 lr: 0.005\n",
            "iteration: 6920 loss: 0.0154 lr: 0.005\n",
            "iteration: 6930 loss: 0.0133 lr: 0.005\n",
            "iteration: 6940 loss: 0.0109 lr: 0.005\n",
            "iteration: 6950 loss: 0.0131 lr: 0.005\n",
            "iteration: 6960 loss: 0.0152 lr: 0.005\n",
            "iteration: 6970 loss: 0.0135 lr: 0.005\n",
            "iteration: 6980 loss: 0.0136 lr: 0.005\n",
            "iteration: 6990 loss: 0.0134 lr: 0.005\n",
            "iteration: 7000 loss: 0.0131 lr: 0.005\n",
            "iteration: 7010 loss: 0.0143 lr: 0.005\n",
            "iteration: 7020 loss: 0.0149 lr: 0.005\n",
            "iteration: 7030 loss: 0.0127 lr: 0.005\n",
            "iteration: 7040 loss: 0.0112 lr: 0.005\n",
            "iteration: 7050 loss: 0.0130 lr: 0.005\n",
            "iteration: 7060 loss: 0.0137 lr: 0.005\n",
            "iteration: 7070 loss: 0.0102 lr: 0.005\n",
            "iteration: 7080 loss: 0.0116 lr: 0.005\n",
            "iteration: 7090 loss: 0.0136 lr: 0.005\n",
            "iteration: 7100 loss: 0.0114 lr: 0.005\n",
            "iteration: 7110 loss: 0.0123 lr: 0.005\n",
            "iteration: 7120 loss: 0.0114 lr: 0.005\n",
            "iteration: 7130 loss: 0.0115 lr: 0.005\n",
            "iteration: 7140 loss: 0.0134 lr: 0.005\n",
            "iteration: 7150 loss: 0.0128 lr: 0.005\n",
            "iteration: 7160 loss: 0.0131 lr: 0.005\n",
            "iteration: 7170 loss: 0.0124 lr: 0.005\n",
            "iteration: 7180 loss: 0.0114 lr: 0.005\n",
            "iteration: 7190 loss: 0.0131 lr: 0.005\n",
            "iteration: 7200 loss: 0.0126 lr: 0.005\n",
            "iteration: 7210 loss: 0.0137 lr: 0.005\n",
            "iteration: 7220 loss: 0.0116 lr: 0.005\n",
            "iteration: 7230 loss: 0.0095 lr: 0.005\n",
            "iteration: 7240 loss: 0.0115 lr: 0.005\n",
            "iteration: 7250 loss: 0.0122 lr: 0.005\n",
            "iteration: 7260 loss: 0.0123 lr: 0.005\n",
            "iteration: 7270 loss: 0.0129 lr: 0.005\n",
            "iteration: 7280 loss: 0.0103 lr: 0.005\n",
            "iteration: 7290 loss: 0.0136 lr: 0.005\n",
            "iteration: 7300 loss: 0.0116 lr: 0.005\n",
            "iteration: 7310 loss: 0.0126 lr: 0.005\n",
            "iteration: 7320 loss: 0.0111 lr: 0.005\n",
            "iteration: 7330 loss: 0.0127 lr: 0.005\n",
            "iteration: 7340 loss: 0.0130 lr: 0.005\n",
            "iteration: 7350 loss: 0.0110 lr: 0.005\n",
            "iteration: 7360 loss: 0.0142 lr: 0.005\n",
            "iteration: 7370 loss: 0.0122 lr: 0.005\n",
            "iteration: 7380 loss: 0.0105 lr: 0.005\n",
            "iteration: 7390 loss: 0.0124 lr: 0.005\n",
            "iteration: 7400 loss: 0.0103 lr: 0.005\n",
            "iteration: 7410 loss: 0.0137 lr: 0.005\n",
            "iteration: 7420 loss: 0.0100 lr: 0.005\n",
            "iteration: 7430 loss: 0.0137 lr: 0.005\n",
            "iteration: 7440 loss: 0.0127 lr: 0.005\n",
            "iteration: 7450 loss: 0.0119 lr: 0.005\n",
            "iteration: 7460 loss: 0.0139 lr: 0.005\n",
            "iteration: 7470 loss: 0.0098 lr: 0.005\n",
            "iteration: 7480 loss: 0.0125 lr: 0.005\n",
            "iteration: 7490 loss: 0.0107 lr: 0.005\n",
            "iteration: 7500 loss: 0.0124 lr: 0.005\n",
            "iteration: 7510 loss: 0.0130 lr: 0.005\n",
            "iteration: 7520 loss: 0.0141 lr: 0.005\n",
            "iteration: 7530 loss: 0.0126 lr: 0.005\n",
            "iteration: 7540 loss: 0.0115 lr: 0.005\n",
            "iteration: 7550 loss: 0.0119 lr: 0.005\n",
            "iteration: 7560 loss: 0.0119 lr: 0.005\n",
            "iteration: 7570 loss: 0.0121 lr: 0.005\n",
            "iteration: 7580 loss: 0.0142 lr: 0.005\n",
            "iteration: 7590 loss: 0.0122 lr: 0.005\n",
            "iteration: 7600 loss: 0.0111 lr: 0.005\n",
            "iteration: 7610 loss: 0.0136 lr: 0.005\n",
            "iteration: 7620 loss: 0.0111 lr: 0.005\n",
            "iteration: 7630 loss: 0.0111 lr: 0.005\n",
            "iteration: 7640 loss: 0.0112 lr: 0.005\n",
            "iteration: 7650 loss: 0.0114 lr: 0.005\n",
            "iteration: 7660 loss: 0.0114 lr: 0.005\n",
            "iteration: 7670 loss: 0.0122 lr: 0.005\n",
            "iteration: 7680 loss: 0.0123 lr: 0.005\n",
            "iteration: 7690 loss: 0.0117 lr: 0.005\n",
            "iteration: 7700 loss: 0.0129 lr: 0.005\n",
            "iteration: 7710 loss: 0.0121 lr: 0.005\n",
            "iteration: 7720 loss: 0.0114 lr: 0.005\n",
            "iteration: 7730 loss: 0.0114 lr: 0.005\n",
            "iteration: 7740 loss: 0.0108 lr: 0.005\n",
            "iteration: 7750 loss: 0.0112 lr: 0.005\n",
            "iteration: 7760 loss: 0.0122 lr: 0.005\n",
            "iteration: 7770 loss: 0.0136 lr: 0.005\n",
            "iteration: 7780 loss: 0.0120 lr: 0.005\n",
            "iteration: 7790 loss: 0.0115 lr: 0.005\n",
            "iteration: 7800 loss: 0.0120 lr: 0.005\n",
            "iteration: 7810 loss: 0.0113 lr: 0.005\n",
            "iteration: 7820 loss: 0.0112 lr: 0.005\n",
            "iteration: 7830 loss: 0.0151 lr: 0.005\n",
            "iteration: 7840 loss: 0.0130 lr: 0.005\n",
            "iteration: 7850 loss: 0.0130 lr: 0.005\n",
            "iteration: 7860 loss: 0.0131 lr: 0.005\n",
            "iteration: 7870 loss: 0.0123 lr: 0.005\n",
            "iteration: 7880 loss: 0.0111 lr: 0.005\n",
            "iteration: 7890 loss: 0.0137 lr: 0.005\n",
            "iteration: 7900 loss: 0.0122 lr: 0.005\n",
            "iteration: 7910 loss: 0.0123 lr: 0.005\n",
            "iteration: 7920 loss: 0.0118 lr: 0.005\n",
            "iteration: 7930 loss: 0.0122 lr: 0.005\n",
            "iteration: 7940 loss: 0.0109 lr: 0.005\n",
            "iteration: 7950 loss: 0.0139 lr: 0.005\n",
            "iteration: 7960 loss: 0.0110 lr: 0.005\n",
            "iteration: 7970 loss: 0.0118 lr: 0.005\n",
            "iteration: 7980 loss: 0.0111 lr: 0.005\n",
            "iteration: 7990 loss: 0.0092 lr: 0.005\n",
            "iteration: 8000 loss: 0.0123 lr: 0.005\n",
            "iteration: 8010 loss: 0.0119 lr: 0.005\n",
            "iteration: 8020 loss: 0.0140 lr: 0.005\n",
            "iteration: 8030 loss: 0.0112 lr: 0.005\n",
            "iteration: 8040 loss: 0.0115 lr: 0.005\n",
            "iteration: 8050 loss: 0.0126 lr: 0.005\n",
            "iteration: 8060 loss: 0.0110 lr: 0.005\n",
            "iteration: 8070 loss: 0.0112 lr: 0.005\n",
            "iteration: 8080 loss: 0.0124 lr: 0.005\n",
            "iteration: 8090 loss: 0.0106 lr: 0.005\n",
            "iteration: 8100 loss: 0.0100 lr: 0.005\n",
            "iteration: 8110 loss: 0.0110 lr: 0.005\n",
            "iteration: 8120 loss: 0.0115 lr: 0.005\n",
            "iteration: 8130 loss: 0.0099 lr: 0.005\n",
            "iteration: 8140 loss: 0.0096 lr: 0.005\n",
            "iteration: 8150 loss: 0.0098 lr: 0.005\n",
            "iteration: 8160 loss: 0.0115 lr: 0.005\n",
            "iteration: 8170 loss: 0.0101 lr: 0.005\n",
            "iteration: 8180 loss: 0.0115 lr: 0.005\n",
            "iteration: 8190 loss: 0.0124 lr: 0.005\n",
            "iteration: 8200 loss: 0.0090 lr: 0.005\n",
            "iteration: 8210 loss: 0.0116 lr: 0.005\n",
            "iteration: 8220 loss: 0.0119 lr: 0.005\n",
            "iteration: 8230 loss: 0.0124 lr: 0.005\n",
            "iteration: 8240 loss: 0.0116 lr: 0.005\n",
            "iteration: 8250 loss: 0.0126 lr: 0.005\n",
            "iteration: 8260 loss: 0.0113 lr: 0.005\n",
            "iteration: 8270 loss: 0.0123 lr: 0.005\n",
            "iteration: 8280 loss: 0.0125 lr: 0.005\n",
            "iteration: 8290 loss: 0.0111 lr: 0.005\n",
            "iteration: 8300 loss: 0.0112 lr: 0.005\n",
            "iteration: 8310 loss: 0.0107 lr: 0.005\n",
            "iteration: 8320 loss: 0.0126 lr: 0.005\n",
            "iteration: 8330 loss: 0.0108 lr: 0.005\n",
            "iteration: 8340 loss: 0.0131 lr: 0.005\n",
            "iteration: 8350 loss: 0.0072 lr: 0.005\n",
            "iteration: 8360 loss: 0.0111 lr: 0.005\n",
            "iteration: 8370 loss: 0.0108 lr: 0.005\n",
            "iteration: 8380 loss: 0.0094 lr: 0.005\n",
            "iteration: 8390 loss: 0.0124 lr: 0.005\n",
            "iteration: 8400 loss: 0.0090 lr: 0.005\n",
            "iteration: 8410 loss: 0.0119 lr: 0.005\n",
            "iteration: 8420 loss: 0.0111 lr: 0.005\n",
            "iteration: 8430 loss: 0.0115 lr: 0.005\n",
            "iteration: 8440 loss: 0.0111 lr: 0.005\n",
            "iteration: 8450 loss: 0.0116 lr: 0.005\n",
            "iteration: 8460 loss: 0.0115 lr: 0.005\n",
            "iteration: 8470 loss: 0.0107 lr: 0.005\n",
            "iteration: 8480 loss: 0.0103 lr: 0.005\n",
            "iteration: 8490 loss: 0.0098 lr: 0.005\n",
            "iteration: 8500 loss: 0.0102 lr: 0.005\n",
            "iteration: 8510 loss: 0.0139 lr: 0.005\n",
            "iteration: 8520 loss: 0.0089 lr: 0.005\n",
            "iteration: 8530 loss: 0.0114 lr: 0.005\n",
            "iteration: 8540 loss: 0.0095 lr: 0.005\n",
            "iteration: 8550 loss: 0.0105 lr: 0.005\n",
            "iteration: 8560 loss: 0.0110 lr: 0.005\n",
            "iteration: 8570 loss: 0.0117 lr: 0.005\n",
            "iteration: 8580 loss: 0.0115 lr: 0.005\n",
            "iteration: 8590 loss: 0.0107 lr: 0.005\n",
            "iteration: 8600 loss: 0.0099 lr: 0.005\n",
            "iteration: 8610 loss: 0.0096 lr: 0.005\n",
            "iteration: 8620 loss: 0.0105 lr: 0.005\n",
            "iteration: 8630 loss: 0.0100 lr: 0.005\n",
            "iteration: 8640 loss: 0.0126 lr: 0.005\n",
            "iteration: 8650 loss: 0.0100 lr: 0.005\n",
            "iteration: 8660 loss: 0.0114 lr: 0.005\n",
            "iteration: 8670 loss: 0.0131 lr: 0.005\n",
            "iteration: 8680 loss: 0.0108 lr: 0.005\n",
            "iteration: 8690 loss: 0.0115 lr: 0.005\n",
            "iteration: 8700 loss: 0.0126 lr: 0.005\n",
            "iteration: 8710 loss: 0.0089 lr: 0.005\n",
            "iteration: 8720 loss: 0.0084 lr: 0.005\n",
            "iteration: 8730 loss: 0.0133 lr: 0.005\n",
            "iteration: 8740 loss: 0.0105 lr: 0.005\n",
            "iteration: 8750 loss: 0.0107 lr: 0.005\n",
            "iteration: 8760 loss: 0.0116 lr: 0.005\n",
            "iteration: 8770 loss: 0.0111 lr: 0.005\n",
            "iteration: 8780 loss: 0.0116 lr: 0.005\n",
            "iteration: 8790 loss: 0.0103 lr: 0.005\n",
            "iteration: 8800 loss: 0.0117 lr: 0.005\n",
            "iteration: 8810 loss: 0.0094 lr: 0.005\n",
            "iteration: 8820 loss: 0.0093 lr: 0.005\n",
            "iteration: 8830 loss: 0.0097 lr: 0.005\n",
            "iteration: 8840 loss: 0.0110 lr: 0.005\n",
            "iteration: 8850 loss: 0.0102 lr: 0.005\n",
            "iteration: 8860 loss: 0.0102 lr: 0.005\n",
            "iteration: 8870 loss: 0.0113 lr: 0.005\n",
            "iteration: 8880 loss: 0.0126 lr: 0.005\n",
            "iteration: 8890 loss: 0.0089 lr: 0.005\n",
            "iteration: 8900 loss: 0.0103 lr: 0.005\n",
            "iteration: 8910 loss: 0.0118 lr: 0.005\n",
            "iteration: 8920 loss: 0.0110 lr: 0.005\n",
            "iteration: 8930 loss: 0.0104 lr: 0.005\n",
            "iteration: 8940 loss: 0.0091 lr: 0.005\n",
            "iteration: 8950 loss: 0.0116 lr: 0.005\n",
            "iteration: 8960 loss: 0.0109 lr: 0.005\n",
            "iteration: 8970 loss: 0.0118 lr: 0.005\n",
            "iteration: 8980 loss: 0.0108 lr: 0.005\n",
            "iteration: 8990 loss: 0.0123 lr: 0.005\n",
            "iteration: 9000 loss: 0.0094 lr: 0.005\n",
            "iteration: 9010 loss: 0.0109 lr: 0.005\n",
            "iteration: 9020 loss: 0.0117 lr: 0.005\n",
            "iteration: 9030 loss: 0.0123 lr: 0.005\n",
            "iteration: 9040 loss: 0.0096 lr: 0.005\n",
            "iteration: 9050 loss: 0.0119 lr: 0.005\n",
            "iteration: 9060 loss: 0.0111 lr: 0.005\n",
            "iteration: 9070 loss: 0.0121 lr: 0.005\n",
            "iteration: 9080 loss: 0.0113 lr: 0.005\n",
            "iteration: 9090 loss: 0.0095 lr: 0.005\n",
            "iteration: 9100 loss: 0.0124 lr: 0.005\n",
            "iteration: 9110 loss: 0.0119 lr: 0.005\n",
            "iteration: 9120 loss: 0.0113 lr: 0.005\n",
            "iteration: 9130 loss: 0.0095 lr: 0.005\n",
            "iteration: 9140 loss: 0.0105 lr: 0.005\n",
            "iteration: 9150 loss: 0.0109 lr: 0.005\n",
            "iteration: 9160 loss: 0.0095 lr: 0.005\n",
            "iteration: 9170 loss: 0.0084 lr: 0.005\n",
            "iteration: 9180 loss: 0.0115 lr: 0.005\n",
            "iteration: 9190 loss: 0.0121 lr: 0.005\n",
            "iteration: 9200 loss: 0.0126 lr: 0.005\n",
            "iteration: 9210 loss: 0.0112 lr: 0.005\n",
            "iteration: 9220 loss: 0.0112 lr: 0.005\n",
            "iteration: 9230 loss: 0.0120 lr: 0.005\n",
            "iteration: 9240 loss: 0.0090 lr: 0.005\n",
            "iteration: 9250 loss: 0.0111 lr: 0.005\n",
            "iteration: 9260 loss: 0.0105 lr: 0.005\n",
            "iteration: 9270 loss: 0.0113 lr: 0.005\n",
            "iteration: 9280 loss: 0.0107 lr: 0.005\n",
            "iteration: 9290 loss: 0.0111 lr: 0.005\n",
            "iteration: 9300 loss: 0.0091 lr: 0.005\n",
            "iteration: 9310 loss: 0.0100 lr: 0.005\n",
            "iteration: 9320 loss: 0.0102 lr: 0.005\n",
            "iteration: 9330 loss: 0.0106 lr: 0.005\n",
            "iteration: 9340 loss: 0.0116 lr: 0.005\n",
            "iteration: 9350 loss: 0.0089 lr: 0.005\n",
            "iteration: 9360 loss: 0.0091 lr: 0.005\n",
            "iteration: 9370 loss: 0.0095 lr: 0.005\n",
            "iteration: 9380 loss: 0.0116 lr: 0.005\n",
            "iteration: 9390 loss: 0.0098 lr: 0.005\n",
            "iteration: 9400 loss: 0.0125 lr: 0.005\n",
            "iteration: 9410 loss: 0.0109 lr: 0.005\n",
            "iteration: 9420 loss: 0.0115 lr: 0.005\n",
            "iteration: 9430 loss: 0.0113 lr: 0.005\n",
            "iteration: 9440 loss: 0.0103 lr: 0.005\n",
            "iteration: 9450 loss: 0.0098 lr: 0.005\n",
            "iteration: 9460 loss: 0.0104 lr: 0.005\n",
            "iteration: 9470 loss: 0.0104 lr: 0.005\n",
            "iteration: 9480 loss: 0.0097 lr: 0.005\n",
            "iteration: 9490 loss: 0.0111 lr: 0.005\n",
            "iteration: 9500 loss: 0.0123 lr: 0.005\n",
            "iteration: 9510 loss: 0.0092 lr: 0.005\n",
            "iteration: 9520 loss: 0.0100 lr: 0.005\n",
            "iteration: 9530 loss: 0.0115 lr: 0.005\n",
            "iteration: 9540 loss: 0.0110 lr: 0.005\n",
            "iteration: 9550 loss: 0.0100 lr: 0.005\n",
            "iteration: 9560 loss: 0.0099 lr: 0.005\n",
            "iteration: 9570 loss: 0.0113 lr: 0.005\n",
            "iteration: 9580 loss: 0.0093 lr: 0.005\n",
            "iteration: 9590 loss: 0.0111 lr: 0.005\n",
            "iteration: 9600 loss: 0.0101 lr: 0.005\n",
            "iteration: 9610 loss: 0.0092 lr: 0.005\n",
            "iteration: 9620 loss: 0.0098 lr: 0.005\n",
            "iteration: 9630 loss: 0.0095 lr: 0.005\n",
            "iteration: 9640 loss: 0.0100 lr: 0.005\n",
            "iteration: 9650 loss: 0.0113 lr: 0.005\n",
            "iteration: 9660 loss: 0.0114 lr: 0.005\n",
            "iteration: 9670 loss: 0.0121 lr: 0.005\n",
            "iteration: 9680 loss: 0.0102 lr: 0.005\n",
            "iteration: 9690 loss: 0.0102 lr: 0.005\n",
            "iteration: 9700 loss: 0.0103 lr: 0.005\n",
            "iteration: 9710 loss: 0.0096 lr: 0.005\n",
            "iteration: 9720 loss: 0.0104 lr: 0.005\n",
            "iteration: 9730 loss: 0.0094 lr: 0.005\n",
            "iteration: 9740 loss: 0.0105 lr: 0.005\n",
            "iteration: 9750 loss: 0.0092 lr: 0.005\n",
            "iteration: 9760 loss: 0.0110 lr: 0.005\n",
            "iteration: 9770 loss: 0.0106 lr: 0.005\n",
            "iteration: 9780 loss: 0.0100 lr: 0.005\n",
            "iteration: 9790 loss: 0.0078 lr: 0.005\n",
            "iteration: 9800 loss: 0.0100 lr: 0.005\n",
            "iteration: 9810 loss: 0.0097 lr: 0.005\n",
            "iteration: 9820 loss: 0.0081 lr: 0.005\n",
            "iteration: 9830 loss: 0.0113 lr: 0.005\n",
            "iteration: 9840 loss: 0.0122 lr: 0.005\n",
            "iteration: 9850 loss: 0.0112 lr: 0.005\n",
            "iteration: 9860 loss: 0.0111 lr: 0.005\n",
            "iteration: 9870 loss: 0.0111 lr: 0.005\n",
            "iteration: 9880 loss: 0.0101 lr: 0.005\n",
            "iteration: 9890 loss: 0.0111 lr: 0.005\n",
            "iteration: 9900 loss: 0.0107 lr: 0.005\n",
            "iteration: 9910 loss: 0.0093 lr: 0.005\n",
            "iteration: 9920 loss: 0.0109 lr: 0.005\n",
            "iteration: 9930 loss: 0.0102 lr: 0.005\n",
            "iteration: 9940 loss: 0.0107 lr: 0.005\n",
            "iteration: 9950 loss: 0.0110 lr: 0.005\n",
            "iteration: 9960 loss: 0.0088 lr: 0.005\n",
            "iteration: 9970 loss: 0.0099 lr: 0.005\n",
            "iteration: 9980 loss: 0.0102 lr: 0.005\n",
            "iteration: 9990 loss: 0.0098 lr: 0.005\n",
            "iteration: 10000 loss: 0.0103 lr: 0.005\n",
            "iteration: 10010 loss: 0.0112 lr: 0.02\n",
            "iteration: 10020 loss: 0.0155 lr: 0.02\n",
            "iteration: 10030 loss: 0.0145 lr: 0.02\n",
            "iteration: 10040 loss: 0.0187 lr: 0.02\n",
            "iteration: 10050 loss: 0.0168 lr: 0.02\n",
            "iteration: 10060 loss: 0.0152 lr: 0.02\n",
            "iteration: 10070 loss: 0.0153 lr: 0.02\n",
            "iteration: 10080 loss: 0.0159 lr: 0.02\n",
            "iteration: 10090 loss: 0.0152 lr: 0.02\n",
            "iteration: 10100 loss: 0.0146 lr: 0.02\n",
            "iteration: 10110 loss: 0.0139 lr: 0.02\n",
            "iteration: 10120 loss: 0.0126 lr: 0.02\n",
            "iteration: 10130 loss: 0.0155 lr: 0.02\n",
            "iteration: 10140 loss: 0.0134 lr: 0.02\n",
            "iteration: 10150 loss: 0.0158 lr: 0.02\n",
            "iteration: 10160 loss: 0.0140 lr: 0.02\n",
            "iteration: 10170 loss: 0.0130 lr: 0.02\n",
            "iteration: 10180 loss: 0.0147 lr: 0.02\n",
            "iteration: 10190 loss: 0.0160 lr: 0.02\n",
            "iteration: 10200 loss: 0.0155 lr: 0.02\n",
            "iteration: 10210 loss: 0.0139 lr: 0.02\n",
            "iteration: 10220 loss: 0.0137 lr: 0.02\n",
            "iteration: 10230 loss: 0.0139 lr: 0.02\n",
            "iteration: 10240 loss: 0.0154 lr: 0.02\n",
            "iteration: 10250 loss: 0.0132 lr: 0.02\n",
            "iteration: 10260 loss: 0.0141 lr: 0.02\n",
            "iteration: 10270 loss: 0.0117 lr: 0.02\n",
            "iteration: 10280 loss: 0.0135 lr: 0.02\n",
            "iteration: 10290 loss: 0.0123 lr: 0.02\n",
            "iteration: 10300 loss: 0.0126 lr: 0.02\n",
            "iteration: 10310 loss: 0.0119 lr: 0.02\n",
            "iteration: 10320 loss: 0.0115 lr: 0.02\n",
            "iteration: 10330 loss: 0.0141 lr: 0.02\n",
            "iteration: 10340 loss: 0.0125 lr: 0.02\n",
            "iteration: 10350 loss: 0.0144 lr: 0.02\n",
            "iteration: 10360 loss: 0.0142 lr: 0.02\n",
            "iteration: 10370 loss: 0.0149 lr: 0.02\n",
            "iteration: 10380 loss: 0.0147 lr: 0.02\n",
            "iteration: 10390 loss: 0.0128 lr: 0.02\n",
            "iteration: 10400 loss: 0.0142 lr: 0.02\n",
            "iteration: 10410 loss: 0.0118 lr: 0.02\n",
            "iteration: 10420 loss: 0.0136 lr: 0.02\n",
            "iteration: 10430 loss: 0.0127 lr: 0.02\n",
            "iteration: 10440 loss: 0.0112 lr: 0.02\n",
            "iteration: 10450 loss: 0.0127 lr: 0.02\n",
            "iteration: 10460 loss: 0.0124 lr: 0.02\n",
            "iteration: 10470 loss: 0.0116 lr: 0.02\n",
            "iteration: 10480 loss: 0.0117 lr: 0.02\n",
            "iteration: 10490 loss: 0.0115 lr: 0.02\n",
            "iteration: 10500 loss: 0.0121 lr: 0.02\n",
            "iteration: 10510 loss: 0.0137 lr: 0.02\n",
            "iteration: 10520 loss: 0.0117 lr: 0.02\n",
            "iteration: 10530 loss: 0.0099 lr: 0.02\n",
            "iteration: 10540 loss: 0.0124 lr: 0.02\n",
            "iteration: 10550 loss: 0.0117 lr: 0.02\n",
            "iteration: 10560 loss: 0.0103 lr: 0.02\n",
            "iteration: 10570 loss: 0.0124 lr: 0.02\n",
            "iteration: 10580 loss: 0.0107 lr: 0.02\n",
            "iteration: 10590 loss: 0.0122 lr: 0.02\n",
            "iteration: 10600 loss: 0.0114 lr: 0.02\n",
            "iteration: 10610 loss: 0.0135 lr: 0.02\n",
            "iteration: 10620 loss: 0.0128 lr: 0.02\n",
            "iteration: 10630 loss: 0.0116 lr: 0.02\n",
            "iteration: 10640 loss: 0.0105 lr: 0.02\n",
            "iteration: 10650 loss: 0.0120 lr: 0.02\n",
            "iteration: 10660 loss: 0.0092 lr: 0.02\n",
            "iteration: 10670 loss: 0.0115 lr: 0.02\n",
            "iteration: 10680 loss: 0.0101 lr: 0.02\n",
            "iteration: 10690 loss: 0.0132 lr: 0.02\n",
            "iteration: 10700 loss: 0.0130 lr: 0.02\n",
            "iteration: 10710 loss: 0.0114 lr: 0.02\n",
            "iteration: 10720 loss: 0.0124 lr: 0.02\n",
            "iteration: 10730 loss: 0.0099 lr: 0.02\n",
            "iteration: 10740 loss: 0.0099 lr: 0.02\n",
            "iteration: 10750 loss: 0.0128 lr: 0.02\n",
            "iteration: 10760 loss: 0.0126 lr: 0.02\n",
            "iteration: 10770 loss: 0.0138 lr: 0.02\n",
            "iteration: 10780 loss: 0.0144 lr: 0.02\n",
            "iteration: 10790 loss: 0.0129 lr: 0.02\n",
            "iteration: 10800 loss: 0.0116 lr: 0.02\n",
            "iteration: 10810 loss: 0.0115 lr: 0.02\n",
            "iteration: 10820 loss: 0.0115 lr: 0.02\n",
            "iteration: 10830 loss: 0.0134 lr: 0.02\n",
            "iteration: 10840 loss: 0.0113 lr: 0.02\n",
            "iteration: 10850 loss: 0.0130 lr: 0.02\n",
            "iteration: 10860 loss: 0.0117 lr: 0.02\n",
            "iteration: 10870 loss: 0.0117 lr: 0.02\n",
            "iteration: 10880 loss: 0.0117 lr: 0.02\n",
            "iteration: 10890 loss: 0.0115 lr: 0.02\n",
            "iteration: 10900 loss: 0.0121 lr: 0.02\n",
            "iteration: 10910 loss: 0.0121 lr: 0.02\n",
            "iteration: 10920 loss: 0.0118 lr: 0.02\n",
            "iteration: 10930 loss: 0.0131 lr: 0.02\n",
            "iteration: 10940 loss: 0.0120 lr: 0.02\n",
            "iteration: 10950 loss: 0.0114 lr: 0.02\n",
            "iteration: 10960 loss: 0.0108 lr: 0.02\n",
            "iteration: 10970 loss: 0.0115 lr: 0.02\n",
            "iteration: 10980 loss: 0.0121 lr: 0.02\n",
            "iteration: 10990 loss: 0.0109 lr: 0.02\n",
            "iteration: 11000 loss: 0.0105 lr: 0.02\n",
            "iteration: 11010 loss: 0.0118 lr: 0.02\n",
            "iteration: 11020 loss: 0.0119 lr: 0.02\n",
            "iteration: 11030 loss: 0.0087 lr: 0.02\n",
            "iteration: 11040 loss: 0.0125 lr: 0.02\n",
            "iteration: 11050 loss: 0.0121 lr: 0.02\n",
            "iteration: 11060 loss: 0.0098 lr: 0.02\n",
            "iteration: 11070 loss: 0.0107 lr: 0.02\n",
            "iteration: 11080 loss: 0.0117 lr: 0.02\n",
            "iteration: 11090 loss: 0.0120 lr: 0.02\n",
            "iteration: 11100 loss: 0.0100 lr: 0.02\n",
            "iteration: 11110 loss: 0.0118 lr: 0.02\n",
            "iteration: 11120 loss: 0.0124 lr: 0.02\n",
            "iteration: 11130 loss: 0.0118 lr: 0.02\n",
            "iteration: 11140 loss: 0.0125 lr: 0.02\n",
            "iteration: 11150 loss: 0.0130 lr: 0.02\n",
            "iteration: 11160 loss: 0.0120 lr: 0.02\n",
            "iteration: 11170 loss: 0.0097 lr: 0.02\n",
            "iteration: 11180 loss: 0.0104 lr: 0.02\n",
            "iteration: 11190 loss: 0.0118 lr: 0.02\n",
            "iteration: 11200 loss: 0.0110 lr: 0.02\n",
            "iteration: 11210 loss: 0.0112 lr: 0.02\n",
            "iteration: 11220 loss: 0.0216 lr: 0.02\n",
            "iteration: 11230 loss: 0.0173 lr: 0.02\n",
            "iteration: 11240 loss: 0.0141 lr: 0.02\n",
            "iteration: 11250 loss: 0.0157 lr: 0.02\n",
            "iteration: 11260 loss: 0.0134 lr: 0.02\n",
            "iteration: 11270 loss: 0.0112 lr: 0.02\n",
            "iteration: 11280 loss: 0.0109 lr: 0.02\n",
            "iteration: 11290 loss: 0.0122 lr: 0.02\n",
            "iteration: 11300 loss: 0.0134 lr: 0.02\n",
            "iteration: 11310 loss: 0.0106 lr: 0.02\n",
            "iteration: 11320 loss: 0.0132 lr: 0.02\n",
            "iteration: 11330 loss: 0.0140 lr: 0.02\n",
            "iteration: 11340 loss: 0.0137 lr: 0.02\n",
            "iteration: 11350 loss: 0.0109 lr: 0.02\n",
            "iteration: 11360 loss: 0.0107 lr: 0.02\n",
            "iteration: 11370 loss: 0.0132 lr: 0.02\n",
            "iteration: 11380 loss: 0.0117 lr: 0.02\n",
            "iteration: 11390 loss: 0.0112 lr: 0.02\n",
            "iteration: 11400 loss: 0.0110 lr: 0.02\n",
            "iteration: 11410 loss: 0.0101 lr: 0.02\n",
            "iteration: 11420 loss: 0.0102 lr: 0.02\n",
            "iteration: 11430 loss: 0.0119 lr: 0.02\n",
            "iteration: 11440 loss: 0.0130 lr: 0.02\n",
            "iteration: 11450 loss: 0.0104 lr: 0.02\n",
            "iteration: 11460 loss: 0.0126 lr: 0.02\n",
            "iteration: 11470 loss: 0.0095 lr: 0.02\n",
            "iteration: 11480 loss: 0.0090 lr: 0.02\n",
            "iteration: 11490 loss: 0.0091 lr: 0.02\n",
            "iteration: 11500 loss: 0.0105 lr: 0.02\n",
            "iteration: 11510 loss: 0.0104 lr: 0.02\n",
            "iteration: 11520 loss: 0.0121 lr: 0.02\n",
            "iteration: 11530 loss: 0.0112 lr: 0.02\n",
            "iteration: 11540 loss: 0.0119 lr: 0.02\n",
            "iteration: 11550 loss: 0.0095 lr: 0.02\n",
            "iteration: 11560 loss: 0.0101 lr: 0.02\n",
            "iteration: 11570 loss: 0.0087 lr: 0.02\n",
            "iteration: 11580 loss: 0.0102 lr: 0.02\n",
            "iteration: 11590 loss: 0.0106 lr: 0.02\n",
            "iteration: 11600 loss: 0.0105 lr: 0.02\n",
            "iteration: 11610 loss: 0.0111 lr: 0.02\n",
            "iteration: 11620 loss: 0.0118 lr: 0.02\n",
            "iteration: 11630 loss: 0.0104 lr: 0.02\n",
            "iteration: 11640 loss: 0.0088 lr: 0.02\n",
            "iteration: 11650 loss: 0.0102 lr: 0.02\n",
            "iteration: 11660 loss: 0.0096 lr: 0.02\n",
            "iteration: 11670 loss: 0.0109 lr: 0.02\n",
            "iteration: 11680 loss: 0.0115 lr: 0.02\n",
            "iteration: 11690 loss: 0.0093 lr: 0.02\n",
            "iteration: 11700 loss: 0.0101 lr: 0.02\n",
            "iteration: 11710 loss: 0.0097 lr: 0.02\n",
            "iteration: 11720 loss: 0.0087 lr: 0.02\n",
            "iteration: 11730 loss: 0.0095 lr: 0.02\n",
            "iteration: 11740 loss: 0.0096 lr: 0.02\n",
            "iteration: 11750 loss: 0.0095 lr: 0.02\n",
            "iteration: 11760 loss: 0.0097 lr: 0.02\n",
            "iteration: 11770 loss: 0.0090 lr: 0.02\n",
            "iteration: 11780 loss: 0.0099 lr: 0.02\n",
            "iteration: 11790 loss: 0.0107 lr: 0.02\n",
            "iteration: 11800 loss: 0.0087 lr: 0.02\n",
            "iteration: 11810 loss: 0.0126 lr: 0.02\n",
            "iteration: 11820 loss: 0.0084 lr: 0.02\n",
            "iteration: 11830 loss: 0.0122 lr: 0.02\n",
            "iteration: 11840 loss: 0.0082 lr: 0.02\n",
            "iteration: 11850 loss: 0.0098 lr: 0.02\n",
            "iteration: 11860 loss: 0.0098 lr: 0.02\n",
            "iteration: 11870 loss: 0.0108 lr: 0.02\n",
            "iteration: 11880 loss: 0.0099 lr: 0.02\n",
            "iteration: 11890 loss: 0.0112 lr: 0.02\n",
            "iteration: 11900 loss: 0.0099 lr: 0.02\n",
            "iteration: 11910 loss: 0.0100 lr: 0.02\n",
            "iteration: 11920 loss: 0.0110 lr: 0.02\n",
            "iteration: 11930 loss: 0.0081 lr: 0.02\n",
            "iteration: 11940 loss: 0.0088 lr: 0.02\n",
            "iteration: 11950 loss: 0.0098 lr: 0.02\n",
            "iteration: 11960 loss: 0.0096 lr: 0.02\n",
            "iteration: 11970 loss: 0.0103 lr: 0.02\n",
            "iteration: 11980 loss: 0.0087 lr: 0.02\n",
            "iteration: 11990 loss: 0.0094 lr: 0.02\n",
            "iteration: 12000 loss: 0.0090 lr: 0.02\n",
            "iteration: 12010 loss: 0.0087 lr: 0.02\n",
            "iteration: 12020 loss: 0.0094 lr: 0.02\n",
            "iteration: 12030 loss: 0.0125 lr: 0.02\n",
            "iteration: 12040 loss: 0.0103 lr: 0.02\n",
            "iteration: 12050 loss: 0.0111 lr: 0.02\n",
            "iteration: 12060 loss: 0.0088 lr: 0.02\n",
            "iteration: 12070 loss: 0.0075 lr: 0.02\n",
            "iteration: 12080 loss: 0.0096 lr: 0.02\n",
            "iteration: 12090 loss: 0.0072 lr: 0.02\n",
            "iteration: 12100 loss: 0.0097 lr: 0.02\n",
            "iteration: 12110 loss: 0.0080 lr: 0.02\n",
            "iteration: 12120 loss: 0.0086 lr: 0.02\n",
            "iteration: 12130 loss: 0.0111 lr: 0.02\n",
            "iteration: 12140 loss: 0.0107 lr: 0.02\n",
            "iteration: 12150 loss: 0.0108 lr: 0.02\n",
            "iteration: 12160 loss: 0.0097 lr: 0.02\n",
            "iteration: 12170 loss: 0.0094 lr: 0.02\n",
            "iteration: 12180 loss: 0.0093 lr: 0.02\n",
            "iteration: 12190 loss: 0.0081 lr: 0.02\n",
            "iteration: 12200 loss: 0.0089 lr: 0.02\n",
            "iteration: 12210 loss: 0.0091 lr: 0.02\n",
            "iteration: 12220 loss: 0.0084 lr: 0.02\n",
            "iteration: 12230 loss: 0.0092 lr: 0.02\n",
            "iteration: 12240 loss: 0.0091 lr: 0.02\n",
            "iteration: 12250 loss: 0.0080 lr: 0.02\n",
            "iteration: 12260 loss: 0.0083 lr: 0.02\n",
            "iteration: 12270 loss: 0.0112 lr: 0.02\n",
            "iteration: 12280 loss: 0.0102 lr: 0.02\n",
            "iteration: 12290 loss: 0.0104 lr: 0.02\n",
            "iteration: 12300 loss: 0.0085 lr: 0.02\n",
            "iteration: 12310 loss: 0.0086 lr: 0.02\n",
            "iteration: 12320 loss: 0.0105 lr: 0.02\n",
            "iteration: 12330 loss: 0.0094 lr: 0.02\n",
            "iteration: 12340 loss: 0.0091 lr: 0.02\n",
            "iteration: 12350 loss: 0.0093 lr: 0.02\n",
            "iteration: 12360 loss: 0.0076 lr: 0.02\n",
            "iteration: 12370 loss: 0.0084 lr: 0.02\n",
            "iteration: 12380 loss: 0.0097 lr: 0.02\n",
            "iteration: 12390 loss: 0.0095 lr: 0.02\n",
            "iteration: 12400 loss: 0.0078 lr: 0.02\n",
            "iteration: 12410 loss: 0.0097 lr: 0.02\n",
            "iteration: 12420 loss: 0.0106 lr: 0.02\n",
            "iteration: 12430 loss: 0.0087 lr: 0.02\n",
            "iteration: 12440 loss: 0.0090 lr: 0.02\n",
            "iteration: 12450 loss: 0.0089 lr: 0.02\n",
            "iteration: 12460 loss: 0.0092 lr: 0.02\n",
            "iteration: 12470 loss: 0.0088 lr: 0.02\n",
            "iteration: 12480 loss: 0.0098 lr: 0.02\n",
            "iteration: 12490 loss: 0.0074 lr: 0.02\n",
            "iteration: 12500 loss: 0.0086 lr: 0.02\n",
            "iteration: 12510 loss: 0.0102 lr: 0.02\n",
            "iteration: 12520 loss: 0.0088 lr: 0.02\n",
            "iteration: 12530 loss: 0.0063 lr: 0.02\n",
            "iteration: 12540 loss: 0.0099 lr: 0.02\n",
            "iteration: 12550 loss: 0.0063 lr: 0.02\n",
            "iteration: 12560 loss: 0.0068 lr: 0.02\n",
            "iteration: 12570 loss: 0.0102 lr: 0.02\n",
            "iteration: 12580 loss: 0.0086 lr: 0.02\n",
            "iteration: 12590 loss: 0.0092 lr: 0.02\n",
            "iteration: 12600 loss: 0.0078 lr: 0.02\n",
            "iteration: 12610 loss: 0.0096 lr: 0.02\n",
            "iteration: 12620 loss: 0.0087 lr: 0.02\n",
            "iteration: 12630 loss: 0.0090 lr: 0.02\n",
            "iteration: 12640 loss: 0.0097 lr: 0.02\n",
            "iteration: 12650 loss: 0.0092 lr: 0.02\n",
            "iteration: 12660 loss: 0.0083 lr: 0.02\n",
            "iteration: 12670 loss: 0.0077 lr: 0.02\n",
            "iteration: 12680 loss: 0.0092 lr: 0.02\n",
            "iteration: 12690 loss: 0.0086 lr: 0.02\n",
            "iteration: 12700 loss: 0.0080 lr: 0.02\n",
            "iteration: 12710 loss: 0.0098 lr: 0.02\n",
            "iteration: 12720 loss: 0.0086 lr: 0.02\n",
            "iteration: 12730 loss: 0.0097 lr: 0.02\n",
            "iteration: 12740 loss: 0.0075 lr: 0.02\n",
            "iteration: 12750 loss: 0.0087 lr: 0.02\n",
            "iteration: 12760 loss: 0.0093 lr: 0.02\n",
            "iteration: 12770 loss: 0.0076 lr: 0.02\n",
            "iteration: 12780 loss: 0.0085 lr: 0.02\n",
            "iteration: 12790 loss: 0.0083 lr: 0.02\n",
            "iteration: 12800 loss: 0.0100 lr: 0.02\n",
            "iteration: 12810 loss: 0.0066 lr: 0.02\n",
            "iteration: 12820 loss: 0.0092 lr: 0.02\n",
            "iteration: 12830 loss: 0.0082 lr: 0.02\n",
            "iteration: 12840 loss: 0.0065 lr: 0.02\n",
            "iteration: 12850 loss: 0.0101 lr: 0.02\n",
            "iteration: 12860 loss: 0.0065 lr: 0.02\n",
            "iteration: 12870 loss: 0.0089 lr: 0.02\n",
            "iteration: 12880 loss: 0.0080 lr: 0.02\n",
            "iteration: 12890 loss: 0.0087 lr: 0.02\n",
            "iteration: 12900 loss: 0.0077 lr: 0.02\n",
            "iteration: 12910 loss: 0.0092 lr: 0.02\n",
            "iteration: 12920 loss: 0.0073 lr: 0.02\n",
            "iteration: 12930 loss: 0.0091 lr: 0.02\n",
            "iteration: 12940 loss: 0.0071 lr: 0.02\n",
            "iteration: 12950 loss: 0.0084 lr: 0.02\n",
            "iteration: 12960 loss: 0.0094 lr: 0.02\n",
            "iteration: 12970 loss: 0.0098 lr: 0.02\n",
            "iteration: 12980 loss: 0.0076 lr: 0.02\n",
            "iteration: 12990 loss: 0.0068 lr: 0.02\n",
            "iteration: 13000 loss: 0.0075 lr: 0.02\n",
            "iteration: 13010 loss: 0.0093 lr: 0.02\n",
            "iteration: 13020 loss: 0.0089 lr: 0.02\n",
            "iteration: 13030 loss: 0.0075 lr: 0.02\n",
            "iteration: 13040 loss: 0.0079 lr: 0.02\n",
            "iteration: 13050 loss: 0.0078 lr: 0.02\n",
            "iteration: 13060 loss: 0.0085 lr: 0.02\n",
            "iteration: 13070 loss: 0.0075 lr: 0.02\n",
            "iteration: 13080 loss: 0.0078 lr: 0.02\n",
            "iteration: 13090 loss: 0.0086 lr: 0.02\n",
            "iteration: 13100 loss: 0.0101 lr: 0.02\n",
            "iteration: 13110 loss: 0.0076 lr: 0.02\n",
            "iteration: 13120 loss: 0.0085 lr: 0.02\n",
            "iteration: 13130 loss: 0.0099 lr: 0.02\n",
            "iteration: 13140 loss: 0.0090 lr: 0.02\n",
            "iteration: 13150 loss: 0.0072 lr: 0.02\n",
            "iteration: 13160 loss: 0.0083 lr: 0.02\n",
            "iteration: 13170 loss: 0.0078 lr: 0.02\n",
            "iteration: 13180 loss: 0.0081 lr: 0.02\n",
            "iteration: 13190 loss: 0.0071 lr: 0.02\n",
            "iteration: 13200 loss: 0.0088 lr: 0.02\n",
            "iteration: 13210 loss: 0.0067 lr: 0.02\n",
            "iteration: 13220 loss: 0.0094 lr: 0.02\n",
            "iteration: 13230 loss: 0.0093 lr: 0.02\n",
            "iteration: 13240 loss: 0.0066 lr: 0.02\n",
            "iteration: 13250 loss: 0.0072 lr: 0.02\n",
            "iteration: 13260 loss: 0.0082 lr: 0.02\n",
            "iteration: 13270 loss: 0.0068 lr: 0.02\n",
            "iteration: 13280 loss: 0.0077 lr: 0.02\n",
            "iteration: 13290 loss: 0.0088 lr: 0.02\n",
            "iteration: 13300 loss: 0.0095 lr: 0.02\n",
            "iteration: 13310 loss: 0.0068 lr: 0.02\n",
            "iteration: 13320 loss: 0.0082 lr: 0.02\n",
            "iteration: 13330 loss: 0.0057 lr: 0.02\n",
            "iteration: 13340 loss: 0.0072 lr: 0.02\n",
            "iteration: 13350 loss: 0.0068 lr: 0.02\n",
            "iteration: 13360 loss: 0.0107 lr: 0.02\n",
            "iteration: 13370 loss: 0.0081 lr: 0.02\n",
            "iteration: 13380 loss: 0.0080 lr: 0.02\n",
            "iteration: 13390 loss: 0.0077 lr: 0.02\n",
            "iteration: 13400 loss: 0.0068 lr: 0.02\n",
            "iteration: 13410 loss: 0.0085 lr: 0.02\n",
            "iteration: 13420 loss: 0.0094 lr: 0.02\n",
            "iteration: 13430 loss: 0.0079 lr: 0.02\n",
            "iteration: 13440 loss: 0.0083 lr: 0.02\n",
            "iteration: 13450 loss: 0.0074 lr: 0.02\n",
            "iteration: 13460 loss: 0.0085 lr: 0.02\n",
            "iteration: 13470 loss: 0.0084 lr: 0.02\n",
            "iteration: 13480 loss: 0.0082 lr: 0.02\n",
            "iteration: 13490 loss: 0.0099 lr: 0.02\n",
            "iteration: 13500 loss: 0.0081 lr: 0.02\n",
            "iteration: 13510 loss: 0.0064 lr: 0.02\n",
            "iteration: 13520 loss: 0.0082 lr: 0.02\n",
            "iteration: 13530 loss: 0.0093 lr: 0.02\n",
            "iteration: 13540 loss: 0.0081 lr: 0.02\n",
            "iteration: 13550 loss: 0.0079 lr: 0.02\n",
            "iteration: 13560 loss: 0.0094 lr: 0.02\n",
            "iteration: 13570 loss: 0.0077 lr: 0.02\n",
            "iteration: 13580 loss: 0.0070 lr: 0.02\n",
            "iteration: 13590 loss: 0.0092 lr: 0.02\n",
            "iteration: 13600 loss: 0.0072 lr: 0.02\n",
            "iteration: 13610 loss: 0.0072 lr: 0.02\n",
            "iteration: 13620 loss: 0.0077 lr: 0.02\n",
            "iteration: 13630 loss: 0.0075 lr: 0.02\n",
            "iteration: 13640 loss: 0.0058 lr: 0.02\n",
            "iteration: 13650 loss: 0.0084 lr: 0.02\n",
            "iteration: 13660 loss: 0.0087 lr: 0.02\n",
            "iteration: 13670 loss: 0.0080 lr: 0.02\n",
            "iteration: 13680 loss: 0.0076 lr: 0.02\n",
            "iteration: 13690 loss: 0.0079 lr: 0.02\n",
            "iteration: 13700 loss: 0.0084 lr: 0.02\n",
            "iteration: 13710 loss: 0.0074 lr: 0.02\n",
            "iteration: 13720 loss: 0.0081 lr: 0.02\n",
            "iteration: 13730 loss: 0.0072 lr: 0.02\n",
            "iteration: 13740 loss: 0.0060 lr: 0.02\n",
            "iteration: 13750 loss: 0.0084 lr: 0.02\n",
            "iteration: 13760 loss: 0.0061 lr: 0.02\n",
            "iteration: 13770 loss: 0.0065 lr: 0.02\n",
            "iteration: 13780 loss: 0.0064 lr: 0.02\n",
            "iteration: 13790 loss: 0.0091 lr: 0.02\n",
            "iteration: 13800 loss: 0.0082 lr: 0.02\n",
            "iteration: 13810 loss: 0.0077 lr: 0.02\n",
            "iteration: 13820 loss: 0.0072 lr: 0.02\n",
            "iteration: 13830 loss: 0.0078 lr: 0.02\n",
            "iteration: 13840 loss: 0.0080 lr: 0.02\n",
            "iteration: 13850 loss: 0.0072 lr: 0.02\n",
            "iteration: 13860 loss: 0.0067 lr: 0.02\n",
            "iteration: 13870 loss: 0.0071 lr: 0.02\n",
            "iteration: 13880 loss: 0.0075 lr: 0.02\n",
            "iteration: 13890 loss: 0.0074 lr: 0.02\n",
            "iteration: 13900 loss: 0.0088 lr: 0.02\n",
            "iteration: 13910 loss: 0.0095 lr: 0.02\n",
            "iteration: 13920 loss: 0.0082 lr: 0.02\n",
            "iteration: 13930 loss: 0.0066 lr: 0.02\n",
            "iteration: 13940 loss: 0.0077 lr: 0.02\n",
            "iteration: 13950 loss: 0.0079 lr: 0.02\n",
            "iteration: 13960 loss: 0.0083 lr: 0.02\n",
            "iteration: 13970 loss: 0.0070 lr: 0.02\n",
            "iteration: 13980 loss: 0.0059 lr: 0.02\n",
            "iteration: 13990 loss: 0.0054 lr: 0.02\n",
            "iteration: 14000 loss: 0.0072 lr: 0.02\n",
            "iteration: 14010 loss: 0.0088 lr: 0.02\n",
            "iteration: 14020 loss: 0.0082 lr: 0.02\n",
            "iteration: 14030 loss: 0.0073 lr: 0.02\n",
            "iteration: 14040 loss: 0.0077 lr: 0.02\n",
            "iteration: 14050 loss: 0.0058 lr: 0.02\n",
            "iteration: 14060 loss: 0.0064 lr: 0.02\n",
            "iteration: 14070 loss: 0.0080 lr: 0.02\n",
            "iteration: 14080 loss: 0.0068 lr: 0.02\n",
            "iteration: 14090 loss: 0.0063 lr: 0.02\n",
            "iteration: 14100 loss: 0.0070 lr: 0.02\n",
            "iteration: 14110 loss: 0.0065 lr: 0.02\n",
            "iteration: 14120 loss: 0.0082 lr: 0.02\n",
            "iteration: 14130 loss: 0.0078 lr: 0.02\n",
            "iteration: 14140 loss: 0.0076 lr: 0.02\n",
            "iteration: 14150 loss: 0.0067 lr: 0.02\n",
            "iteration: 14160 loss: 0.0071 lr: 0.02\n",
            "iteration: 14170 loss: 0.0073 lr: 0.02\n",
            "iteration: 14180 loss: 0.0069 lr: 0.02\n",
            "iteration: 14190 loss: 0.0077 lr: 0.02\n",
            "iteration: 14200 loss: 0.0077 lr: 0.02\n",
            "iteration: 14210 loss: 0.0068 lr: 0.02\n",
            "iteration: 14220 loss: 0.0076 lr: 0.02\n",
            "iteration: 14230 loss: 0.0086 lr: 0.02\n",
            "iteration: 14240 loss: 0.0078 lr: 0.02\n",
            "iteration: 14250 loss: 0.0095 lr: 0.02\n",
            "iteration: 14260 loss: 0.0055 lr: 0.02\n",
            "iteration: 14270 loss: 0.0073 lr: 0.02\n",
            "iteration: 14280 loss: 0.0087 lr: 0.02\n",
            "iteration: 14290 loss: 0.0091 lr: 0.02\n",
            "iteration: 14300 loss: 0.0095 lr: 0.02\n",
            "iteration: 14310 loss: 0.0077 lr: 0.02\n",
            "iteration: 14320 loss: 0.0054 lr: 0.02\n",
            "iteration: 14330 loss: 0.0067 lr: 0.02\n",
            "iteration: 14340 loss: 0.0067 lr: 0.02\n",
            "iteration: 14350 loss: 0.0065 lr: 0.02\n",
            "iteration: 14360 loss: 0.0072 lr: 0.02\n",
            "iteration: 14370 loss: 0.0066 lr: 0.02\n",
            "iteration: 14380 loss: 0.0072 lr: 0.02\n",
            "iteration: 14390 loss: 0.0083 lr: 0.02\n",
            "iteration: 14400 loss: 0.0073 lr: 0.02\n",
            "iteration: 14410 loss: 0.0068 lr: 0.02\n",
            "iteration: 14420 loss: 0.0078 lr: 0.02\n",
            "iteration: 14430 loss: 0.0098 lr: 0.02\n",
            "iteration: 14440 loss: 0.0083 lr: 0.02\n",
            "iteration: 14450 loss: 0.0078 lr: 0.02\n",
            "iteration: 14460 loss: 0.0077 lr: 0.02\n",
            "iteration: 14470 loss: 0.0069 lr: 0.02\n",
            "iteration: 14480 loss: 0.0092 lr: 0.02\n",
            "iteration: 14490 loss: 0.0085 lr: 0.02\n",
            "iteration: 14500 loss: 0.0072 lr: 0.02\n",
            "iteration: 14510 loss: 0.0061 lr: 0.02\n",
            "iteration: 14520 loss: 0.0083 lr: 0.02\n",
            "iteration: 14530 loss: 0.0082 lr: 0.02\n",
            "iteration: 14540 loss: 0.0075 lr: 0.02\n",
            "iteration: 14550 loss: 0.0058 lr: 0.02\n",
            "iteration: 14560 loss: 0.0052 lr: 0.02\n",
            "iteration: 14570 loss: 0.0069 lr: 0.02\n",
            "iteration: 14580 loss: 0.0065 lr: 0.02\n",
            "iteration: 14590 loss: 0.0075 lr: 0.02\n",
            "iteration: 14600 loss: 0.0086 lr: 0.02\n",
            "iteration: 14610 loss: 0.0073 lr: 0.02\n",
            "iteration: 14620 loss: 0.0066 lr: 0.02\n",
            "iteration: 14630 loss: 0.0064 lr: 0.02\n",
            "iteration: 14640 loss: 0.0069 lr: 0.02\n",
            "iteration: 14650 loss: 0.0061 lr: 0.02\n",
            "iteration: 14660 loss: 0.0095 lr: 0.02\n",
            "iteration: 14670 loss: 0.0071 lr: 0.02\n",
            "iteration: 14680 loss: 0.0058 lr: 0.02\n",
            "iteration: 14690 loss: 0.0070 lr: 0.02\n",
            "iteration: 14700 loss: 0.0068 lr: 0.02\n",
            "iteration: 14710 loss: 0.0079 lr: 0.02\n",
            "iteration: 14720 loss: 0.0078 lr: 0.02\n",
            "iteration: 14730 loss: 0.0082 lr: 0.02\n",
            "iteration: 14740 loss: 0.0062 lr: 0.02\n",
            "iteration: 14750 loss: 0.0067 lr: 0.02\n",
            "iteration: 14760 loss: 0.0068 lr: 0.02\n",
            "iteration: 14770 loss: 0.0071 lr: 0.02\n",
            "iteration: 14780 loss: 0.0066 lr: 0.02\n",
            "iteration: 14790 loss: 0.0070 lr: 0.02\n",
            "iteration: 14800 loss: 0.0070 lr: 0.02\n",
            "iteration: 14810 loss: 0.0063 lr: 0.02\n",
            "iteration: 14820 loss: 0.0064 lr: 0.02\n",
            "iteration: 14830 loss: 0.0067 lr: 0.02\n",
            "iteration: 14840 loss: 0.0071 lr: 0.02\n",
            "iteration: 14850 loss: 0.0084 lr: 0.02\n",
            "iteration: 14860 loss: 0.0062 lr: 0.02\n",
            "iteration: 14870 loss: 0.0053 lr: 0.02\n",
            "iteration: 14880 loss: 0.0058 lr: 0.02\n",
            "iteration: 14890 loss: 0.0060 lr: 0.02\n",
            "iteration: 14900 loss: 0.0072 lr: 0.02\n",
            "iteration: 14910 loss: 0.0065 lr: 0.02\n",
            "iteration: 14920 loss: 0.0063 lr: 0.02\n",
            "iteration: 14930 loss: 0.0062 lr: 0.02\n",
            "iteration: 14940 loss: 0.0070 lr: 0.02\n",
            "iteration: 14950 loss: 0.0059 lr: 0.02\n",
            "iteration: 14960 loss: 0.0058 lr: 0.02\n",
            "iteration: 14970 loss: 0.0065 lr: 0.02\n",
            "iteration: 14980 loss: 0.0059 lr: 0.02\n",
            "iteration: 14990 loss: 0.0049 lr: 0.02\n",
            "iteration: 15000 loss: 0.0068 lr: 0.02\n",
            "iteration: 15010 loss: 0.0065 lr: 0.02\n",
            "iteration: 15020 loss: 0.0073 lr: 0.02\n",
            "iteration: 15030 loss: 0.0087 lr: 0.02\n",
            "iteration: 15040 loss: 0.0067 lr: 0.02\n",
            "iteration: 15050 loss: 0.0060 lr: 0.02\n",
            "iteration: 15060 loss: 0.0069 lr: 0.02\n",
            "iteration: 15070 loss: 0.0053 lr: 0.02\n",
            "iteration: 15080 loss: 0.0058 lr: 0.02\n",
            "iteration: 15090 loss: 0.0070 lr: 0.02\n",
            "iteration: 15100 loss: 0.0068 lr: 0.02\n",
            "iteration: 15110 loss: 0.0070 lr: 0.02\n",
            "iteration: 15120 loss: 0.0075 lr: 0.02\n",
            "iteration: 15130 loss: 0.0076 lr: 0.02\n",
            "iteration: 15140 loss: 0.0069 lr: 0.02\n",
            "iteration: 15150 loss: 0.0064 lr: 0.02\n",
            "iteration: 15160 loss: 0.0076 lr: 0.02\n",
            "iteration: 15170 loss: 0.0073 lr: 0.02\n",
            "iteration: 15180 loss: 0.0055 lr: 0.02\n",
            "iteration: 15190 loss: 0.0057 lr: 0.02\n",
            "iteration: 15200 loss: 0.0070 lr: 0.02\n",
            "iteration: 15210 loss: 0.0052 lr: 0.02\n",
            "iteration: 15220 loss: 0.0071 lr: 0.02\n",
            "iteration: 15230 loss: 0.0064 lr: 0.02\n",
            "iteration: 15240 loss: 0.0067 lr: 0.02\n",
            "iteration: 15250 loss: 0.0076 lr: 0.02\n",
            "iteration: 15260 loss: 0.0075 lr: 0.02\n",
            "iteration: 15270 loss: 0.0065 lr: 0.02\n",
            "iteration: 15280 loss: 0.0069 lr: 0.02\n",
            "iteration: 15290 loss: 0.0047 lr: 0.02\n",
            "iteration: 15300 loss: 0.0073 lr: 0.02\n",
            "iteration: 15310 loss: 0.0064 lr: 0.02\n",
            "iteration: 15320 loss: 0.0062 lr: 0.02\n",
            "iteration: 15330 loss: 0.0065 lr: 0.02\n",
            "iteration: 15340 loss: 0.0065 lr: 0.02\n",
            "iteration: 15350 loss: 0.0070 lr: 0.02\n",
            "iteration: 15360 loss: 0.0068 lr: 0.02\n",
            "iteration: 15370 loss: 0.0067 lr: 0.02\n",
            "iteration: 15380 loss: 0.0078 lr: 0.02\n",
            "iteration: 15390 loss: 0.0057 lr: 0.02\n",
            "iteration: 15400 loss: 0.0064 lr: 0.02\n",
            "iteration: 15410 loss: 0.0048 lr: 0.02\n",
            "iteration: 15420 loss: 0.0069 lr: 0.02\n",
            "iteration: 15430 loss: 0.0062 lr: 0.02\n",
            "iteration: 15440 loss: 0.0075 lr: 0.02\n",
            "iteration: 15450 loss: 0.0064 lr: 0.02\n",
            "iteration: 15460 loss: 0.0054 lr: 0.02\n",
            "iteration: 15470 loss: 0.0066 lr: 0.02\n",
            "iteration: 15480 loss: 0.0069 lr: 0.02\n",
            "iteration: 15490 loss: 0.0050 lr: 0.02\n",
            "iteration: 15500 loss: 0.0063 lr: 0.02\n",
            "iteration: 15510 loss: 0.0063 lr: 0.02\n",
            "iteration: 15520 loss: 0.0055 lr: 0.02\n",
            "iteration: 15530 loss: 0.0051 lr: 0.02\n",
            "iteration: 15540 loss: 0.0057 lr: 0.02\n",
            "iteration: 15550 loss: 0.0066 lr: 0.02\n",
            "iteration: 15560 loss: 0.0061 lr: 0.02\n",
            "iteration: 15570 loss: 0.0047 lr: 0.02\n",
            "iteration: 15580 loss: 0.0074 lr: 0.02\n",
            "iteration: 15590 loss: 0.0064 lr: 0.02\n",
            "iteration: 15600 loss: 0.0057 lr: 0.02\n",
            "iteration: 15610 loss: 0.0071 lr: 0.02\n",
            "iteration: 15620 loss: 0.0070 lr: 0.02\n",
            "iteration: 15630 loss: 0.0064 lr: 0.02\n",
            "iteration: 15640 loss: 0.0059 lr: 0.02\n",
            "iteration: 15650 loss: 0.0048 lr: 0.02\n",
            "iteration: 15660 loss: 0.0065 lr: 0.02\n",
            "iteration: 15670 loss: 0.0054 lr: 0.02\n",
            "iteration: 15680 loss: 0.0062 lr: 0.02\n",
            "iteration: 15690 loss: 0.0072 lr: 0.02\n",
            "iteration: 15700 loss: 0.0061 lr: 0.02\n",
            "iteration: 15710 loss: 0.0054 lr: 0.02\n",
            "iteration: 15720 loss: 0.0055 lr: 0.02\n",
            "iteration: 15730 loss: 0.0055 lr: 0.02\n",
            "iteration: 15740 loss: 0.0058 lr: 0.02\n",
            "iteration: 15750 loss: 0.0096 lr: 0.02\n",
            "iteration: 15760 loss: 0.0054 lr: 0.02\n",
            "iteration: 15770 loss: 0.0080 lr: 0.02\n",
            "iteration: 15780 loss: 0.0082 lr: 0.02\n",
            "iteration: 15790 loss: 0.0077 lr: 0.02\n",
            "iteration: 15800 loss: 0.0065 lr: 0.02\n",
            "iteration: 15810 loss: 0.0049 lr: 0.02\n",
            "iteration: 15820 loss: 0.0074 lr: 0.02\n",
            "iteration: 15830 loss: 0.0082 lr: 0.02\n",
            "iteration: 15840 loss: 0.0055 lr: 0.02\n",
            "iteration: 15850 loss: 0.0061 lr: 0.02\n",
            "iteration: 15860 loss: 0.0062 lr: 0.02\n",
            "iteration: 15870 loss: 0.0060 lr: 0.02\n",
            "iteration: 15880 loss: 0.0061 lr: 0.02\n",
            "iteration: 15890 loss: 0.0077 lr: 0.02\n",
            "iteration: 15900 loss: 0.0049 lr: 0.02\n",
            "iteration: 15910 loss: 0.0055 lr: 0.02\n",
            "iteration: 15920 loss: 0.0079 lr: 0.02\n",
            "iteration: 15930 loss: 0.0072 lr: 0.02\n",
            "iteration: 15940 loss: 0.0078 lr: 0.02\n",
            "iteration: 15950 loss: 0.0064 lr: 0.02\n",
            "iteration: 15960 loss: 0.0067 lr: 0.02\n",
            "iteration: 15970 loss: 0.0053 lr: 0.02\n",
            "iteration: 15980 loss: 0.0073 lr: 0.02\n",
            "iteration: 15990 loss: 0.0069 lr: 0.02\n",
            "iteration: 16000 loss: 0.0052 lr: 0.02\n",
            "iteration: 16010 loss: 0.0060 lr: 0.02\n",
            "iteration: 16020 loss: 0.0070 lr: 0.02\n",
            "iteration: 16030 loss: 0.0061 lr: 0.02\n",
            "iteration: 16040 loss: 0.0078 lr: 0.02\n",
            "iteration: 16050 loss: 0.0071 lr: 0.02\n",
            "iteration: 16060 loss: 0.0047 lr: 0.02\n",
            "iteration: 16070 loss: 0.0056 lr: 0.02\n",
            "iteration: 16080 loss: 0.0082 lr: 0.02\n",
            "iteration: 16090 loss: 0.0052 lr: 0.02\n",
            "iteration: 16100 loss: 0.0060 lr: 0.02\n",
            "iteration: 16110 loss: 0.0059 lr: 0.02\n",
            "iteration: 16120 loss: 0.0063 lr: 0.02\n",
            "iteration: 16130 loss: 0.0071 lr: 0.02\n",
            "iteration: 16140 loss: 0.0064 lr: 0.02\n",
            "iteration: 16150 loss: 0.0069 lr: 0.02\n",
            "iteration: 16160 loss: 0.0056 lr: 0.02\n",
            "iteration: 16170 loss: 0.0053 lr: 0.02\n",
            "iteration: 16180 loss: 0.0051 lr: 0.02\n",
            "iteration: 16190 loss: 0.0053 lr: 0.02\n",
            "iteration: 16200 loss: 0.0060 lr: 0.02\n",
            "iteration: 16210 loss: 0.0070 lr: 0.02\n",
            "iteration: 16220 loss: 0.0060 lr: 0.02\n",
            "iteration: 16230 loss: 0.0058 lr: 0.02\n",
            "iteration: 16240 loss: 0.0057 lr: 0.02\n",
            "iteration: 16250 loss: 0.0058 lr: 0.02\n",
            "iteration: 16260 loss: 0.0050 lr: 0.02\n",
            "iteration: 16270 loss: 0.0054 lr: 0.02\n",
            "iteration: 16280 loss: 0.0052 lr: 0.02\n",
            "iteration: 16290 loss: 0.0055 lr: 0.02\n",
            "iteration: 16300 loss: 0.0067 lr: 0.02\n",
            "iteration: 16310 loss: 0.0069 lr: 0.02\n",
            "iteration: 16320 loss: 0.0074 lr: 0.02\n",
            "iteration: 16330 loss: 0.0064 lr: 0.02\n",
            "iteration: 16340 loss: 0.0059 lr: 0.02\n",
            "iteration: 16350 loss: 0.0056 lr: 0.02\n",
            "iteration: 16360 loss: 0.0065 lr: 0.02\n",
            "iteration: 16370 loss: 0.0053 lr: 0.02\n",
            "iteration: 16380 loss: 0.0054 lr: 0.02\n",
            "iteration: 16390 loss: 0.0071 lr: 0.02\n",
            "iteration: 16400 loss: 0.0048 lr: 0.02\n",
            "iteration: 16410 loss: 0.0052 lr: 0.02\n",
            "iteration: 16420 loss: 0.0056 lr: 0.02\n",
            "iteration: 16430 loss: 0.0043 lr: 0.02\n",
            "iteration: 16440 loss: 0.0054 lr: 0.02\n",
            "iteration: 16450 loss: 0.0058 lr: 0.02\n",
            "iteration: 16460 loss: 0.0055 lr: 0.02\n",
            "iteration: 16470 loss: 0.0061 lr: 0.02\n",
            "iteration: 16480 loss: 0.0049 lr: 0.02\n",
            "iteration: 16490 loss: 0.0056 lr: 0.02\n",
            "iteration: 16500 loss: 0.0059 lr: 0.02\n",
            "iteration: 16510 loss: 0.0060 lr: 0.02\n",
            "iteration: 16520 loss: 0.0061 lr: 0.02\n",
            "iteration: 16530 loss: 0.0051 lr: 0.02\n",
            "iteration: 16540 loss: 0.0064 lr: 0.02\n",
            "iteration: 16550 loss: 0.0047 lr: 0.02\n",
            "iteration: 16560 loss: 0.0059 lr: 0.02\n",
            "iteration: 16570 loss: 0.0053 lr: 0.02\n",
            "iteration: 16580 loss: 0.0058 lr: 0.02\n",
            "iteration: 16590 loss: 0.0062 lr: 0.02\n",
            "iteration: 16600 loss: 0.0048 lr: 0.02\n",
            "iteration: 16610 loss: 0.0053 lr: 0.02\n",
            "iteration: 16620 loss: 0.0053 lr: 0.02\n",
            "iteration: 16630 loss: 0.0056 lr: 0.02\n",
            "iteration: 16640 loss: 0.0053 lr: 0.02\n",
            "iteration: 16650 loss: 0.0061 lr: 0.02\n",
            "iteration: 16660 loss: 0.0060 lr: 0.02\n",
            "iteration: 16670 loss: 0.0052 lr: 0.02\n",
            "iteration: 16680 loss: 0.0067 lr: 0.02\n",
            "iteration: 16690 loss: 0.0057 lr: 0.02\n",
            "iteration: 16700 loss: 0.0063 lr: 0.02\n",
            "iteration: 16710 loss: 0.0049 lr: 0.02\n",
            "iteration: 16720 loss: 0.0063 lr: 0.02\n",
            "iteration: 16730 loss: 0.0057 lr: 0.02\n",
            "iteration: 16740 loss: 0.0052 lr: 0.02\n",
            "iteration: 16750 loss: 0.0052 lr: 0.02\n",
            "iteration: 16760 loss: 0.0058 lr: 0.02\n",
            "iteration: 16770 loss: 0.0068 lr: 0.02\n",
            "iteration: 16780 loss: 0.0047 lr: 0.02\n",
            "iteration: 16790 loss: 0.0048 lr: 0.02\n",
            "iteration: 16800 loss: 0.0071 lr: 0.02\n",
            "iteration: 16810 loss: 0.0062 lr: 0.02\n",
            "iteration: 16820 loss: 0.0062 lr: 0.02\n",
            "iteration: 16830 loss: 0.0052 lr: 0.02\n",
            "iteration: 16840 loss: 0.0069 lr: 0.02\n",
            "iteration: 16850 loss: 0.0055 lr: 0.02\n",
            "iteration: 16860 loss: 0.0042 lr: 0.02\n",
            "iteration: 16870 loss: 0.0050 lr: 0.02\n",
            "iteration: 16880 loss: 0.0057 lr: 0.02\n",
            "iteration: 16890 loss: 0.0060 lr: 0.02\n",
            "iteration: 16900 loss: 0.0052 lr: 0.02\n",
            "iteration: 16910 loss: 0.0045 lr: 0.02\n",
            "iteration: 16920 loss: 0.0050 lr: 0.02\n",
            "iteration: 16930 loss: 0.0055 lr: 0.02\n",
            "iteration: 16940 loss: 0.0057 lr: 0.02\n",
            "iteration: 16950 loss: 0.0059 lr: 0.02\n",
            "iteration: 16960 loss: 0.0063 lr: 0.02\n",
            "iteration: 16970 loss: 0.0061 lr: 0.02\n",
            "iteration: 16980 loss: 0.0048 lr: 0.02\n",
            "iteration: 16990 loss: 0.0058 lr: 0.02\n",
            "iteration: 17000 loss: 0.0062 lr: 0.02\n",
            "iteration: 17010 loss: 0.0061 lr: 0.02\n",
            "iteration: 17020 loss: 0.0058 lr: 0.02\n",
            "iteration: 17030 loss: 0.0068 lr: 0.02\n",
            "iteration: 17040 loss: 0.0053 lr: 0.02\n",
            "iteration: 17050 loss: 0.0048 lr: 0.02\n",
            "iteration: 17060 loss: 0.0055 lr: 0.02\n",
            "iteration: 17070 loss: 0.0065 lr: 0.02\n",
            "iteration: 17080 loss: 0.0054 lr: 0.02\n",
            "iteration: 17090 loss: 0.0049 lr: 0.02\n",
            "iteration: 17100 loss: 0.0059 lr: 0.02\n",
            "iteration: 17110 loss: 0.0055 lr: 0.02\n",
            "iteration: 17120 loss: 0.0061 lr: 0.02\n",
            "iteration: 17130 loss: 0.0056 lr: 0.02\n",
            "iteration: 17140 loss: 0.0058 lr: 0.02\n",
            "iteration: 17150 loss: 0.0053 lr: 0.02\n",
            "iteration: 17160 loss: 0.0056 lr: 0.02\n",
            "iteration: 17170 loss: 0.0049 lr: 0.02\n",
            "iteration: 17180 loss: 0.0047 lr: 0.02\n",
            "iteration: 17190 loss: 0.0053 lr: 0.02\n",
            "iteration: 17200 loss: 0.0060 lr: 0.02\n",
            "iteration: 17210 loss: 0.0048 lr: 0.02\n",
            "iteration: 17220 loss: 0.0061 lr: 0.02\n",
            "iteration: 17230 loss: 0.0061 lr: 0.02\n",
            "iteration: 17240 loss: 0.0055 lr: 0.02\n",
            "iteration: 17250 loss: 0.0059 lr: 0.02\n",
            "iteration: 17260 loss: 0.0050 lr: 0.02\n",
            "iteration: 17270 loss: 0.0058 lr: 0.02\n",
            "iteration: 17280 loss: 0.0061 lr: 0.02\n",
            "iteration: 17290 loss: 0.0055 lr: 0.02\n",
            "iteration: 17300 loss: 0.0054 lr: 0.02\n",
            "iteration: 17310 loss: 0.0048 lr: 0.02\n",
            "iteration: 17320 loss: 0.0049 lr: 0.02\n",
            "iteration: 17330 loss: 0.0067 lr: 0.02\n",
            "iteration: 17340 loss: 0.0069 lr: 0.02\n",
            "iteration: 17350 loss: 0.0057 lr: 0.02\n",
            "iteration: 17360 loss: 0.0054 lr: 0.02\n",
            "iteration: 17370 loss: 0.0048 lr: 0.02\n",
            "iteration: 17380 loss: 0.0077 lr: 0.02\n",
            "iteration: 17390 loss: 0.0069 lr: 0.02\n",
            "iteration: 17400 loss: 0.0057 lr: 0.02\n",
            "iteration: 17410 loss: 0.0047 lr: 0.02\n",
            "iteration: 17420 loss: 0.0063 lr: 0.02\n",
            "iteration: 17430 loss: 0.0050 lr: 0.02\n",
            "iteration: 17440 loss: 0.0048 lr: 0.02\n",
            "iteration: 17450 loss: 0.0058 lr: 0.02\n",
            "iteration: 17460 loss: 0.0056 lr: 0.02\n",
            "iteration: 17470 loss: 0.0056 lr: 0.02\n",
            "iteration: 17480 loss: 0.0068 lr: 0.02\n",
            "iteration: 17490 loss: 0.0049 lr: 0.02\n",
            "iteration: 17500 loss: 0.0057 lr: 0.02\n",
            "iteration: 17510 loss: 0.0060 lr: 0.02\n",
            "iteration: 17520 loss: 0.0041 lr: 0.02\n",
            "iteration: 17530 loss: 0.0051 lr: 0.02\n",
            "iteration: 17540 loss: 0.0054 lr: 0.02\n",
            "iteration: 17550 loss: 0.0064 lr: 0.02\n",
            "iteration: 17560 loss: 0.0057 lr: 0.02\n",
            "iteration: 17570 loss: 0.0058 lr: 0.02\n",
            "iteration: 17580 loss: 0.0056 lr: 0.02\n",
            "iteration: 17590 loss: 0.0046 lr: 0.02\n",
            "iteration: 17600 loss: 0.0053 lr: 0.02\n",
            "iteration: 17610 loss: 0.0057 lr: 0.02\n",
            "iteration: 17620 loss: 0.0067 lr: 0.02\n",
            "iteration: 17630 loss: 0.0056 lr: 0.02\n",
            "iteration: 17640 loss: 0.0044 lr: 0.02\n",
            "iteration: 17650 loss: 0.0052 lr: 0.02\n",
            "iteration: 17660 loss: 0.0063 lr: 0.02\n",
            "iteration: 17670 loss: 0.0071 lr: 0.02\n",
            "iteration: 17680 loss: 0.0054 lr: 0.02\n",
            "iteration: 17690 loss: 0.0067 lr: 0.02\n",
            "iteration: 17700 loss: 0.0053 lr: 0.02\n",
            "iteration: 17710 loss: 0.0042 lr: 0.02\n",
            "iteration: 17720 loss: 0.0056 lr: 0.02\n",
            "iteration: 17730 loss: 0.0056 lr: 0.02\n",
            "iteration: 17740 loss: 0.0052 lr: 0.02\n",
            "iteration: 17750 loss: 0.0050 lr: 0.02\n",
            "iteration: 17760 loss: 0.0074 lr: 0.02\n",
            "iteration: 17770 loss: 0.0074 lr: 0.02\n",
            "iteration: 17780 loss: 0.0062 lr: 0.02\n",
            "iteration: 17790 loss: 0.0050 lr: 0.02\n",
            "iteration: 17800 loss: 0.0048 lr: 0.02\n",
            "iteration: 17810 loss: 0.0050 lr: 0.02\n",
            "iteration: 17820 loss: 0.0056 lr: 0.02\n",
            "iteration: 17830 loss: 0.0057 lr: 0.02\n",
            "iteration: 17840 loss: 0.0053 lr: 0.02\n",
            "iteration: 17850 loss: 0.0050 lr: 0.02\n",
            "iteration: 17860 loss: 0.0073 lr: 0.02\n",
            "iteration: 17870 loss: 0.0052 lr: 0.02\n",
            "iteration: 17880 loss: 0.0050 lr: 0.02\n",
            "iteration: 17890 loss: 0.0055 lr: 0.02\n",
            "iteration: 17900 loss: 0.0053 lr: 0.02\n",
            "iteration: 17910 loss: 0.0053 lr: 0.02\n",
            "iteration: 17920 loss: 0.0053 lr: 0.02\n",
            "iteration: 17930 loss: 0.0061 lr: 0.02\n",
            "iteration: 17940 loss: 0.0049 lr: 0.02\n",
            "iteration: 17950 loss: 0.0053 lr: 0.02\n",
            "iteration: 17960 loss: 0.0050 lr: 0.02\n",
            "iteration: 17970 loss: 0.0057 lr: 0.02\n",
            "iteration: 17980 loss: 0.0058 lr: 0.02\n",
            "iteration: 17990 loss: 0.0055 lr: 0.02\n",
            "iteration: 18000 loss: 0.0057 lr: 0.02\n",
            "iteration: 18010 loss: 0.0051 lr: 0.02\n",
            "iteration: 18020 loss: 0.0045 lr: 0.02\n",
            "iteration: 18030 loss: 0.0053 lr: 0.02\n",
            "iteration: 18040 loss: 0.0042 lr: 0.02\n",
            "iteration: 18050 loss: 0.0039 lr: 0.02\n",
            "iteration: 18060 loss: 0.0047 lr: 0.02\n",
            "iteration: 18070 loss: 0.0065 lr: 0.02\n",
            "iteration: 18080 loss: 0.0059 lr: 0.02\n",
            "iteration: 18090 loss: 0.0050 lr: 0.02\n",
            "iteration: 18100 loss: 0.0053 lr: 0.02\n",
            "iteration: 18110 loss: 0.0059 lr: 0.02\n",
            "iteration: 18120 loss: 0.0054 lr: 0.02\n",
            "iteration: 18130 loss: 0.0050 lr: 0.02\n",
            "iteration: 18140 loss: 0.0056 lr: 0.02\n",
            "iteration: 18150 loss: 0.0055 lr: 0.02\n",
            "iteration: 18160 loss: 0.0051 lr: 0.02\n",
            "iteration: 18170 loss: 0.0058 lr: 0.02\n",
            "iteration: 18180 loss: 0.0048 lr: 0.02\n",
            "iteration: 18190 loss: 0.0041 lr: 0.02\n",
            "iteration: 18200 loss: 0.0054 lr: 0.02\n",
            "iteration: 18210 loss: 0.0052 lr: 0.02\n",
            "iteration: 18220 loss: 0.0048 lr: 0.02\n",
            "iteration: 18230 loss: 0.0047 lr: 0.02\n",
            "iteration: 18240 loss: 0.0051 lr: 0.02\n",
            "iteration: 18250 loss: 0.0053 lr: 0.02\n",
            "iteration: 18260 loss: 0.0062 lr: 0.02\n",
            "iteration: 18270 loss: 0.0054 lr: 0.02\n",
            "iteration: 18280 loss: 0.0054 lr: 0.02\n",
            "iteration: 18290 loss: 0.0053 lr: 0.02\n",
            "iteration: 18300 loss: 0.0071 lr: 0.02\n",
            "iteration: 18310 loss: 0.0073 lr: 0.02\n",
            "iteration: 18320 loss: 0.0070 lr: 0.02\n",
            "iteration: 18330 loss: 0.0058 lr: 0.02\n",
            "iteration: 18340 loss: 0.0063 lr: 0.02\n",
            "iteration: 18350 loss: 0.0038 lr: 0.02\n",
            "iteration: 18360 loss: 0.0051 lr: 0.02\n",
            "iteration: 18370 loss: 0.0055 lr: 0.02\n",
            "iteration: 18380 loss: 0.0048 lr: 0.02\n",
            "iteration: 18390 loss: 0.0059 lr: 0.02\n",
            "iteration: 18400 loss: 0.0050 lr: 0.02\n",
            "iteration: 18410 loss: 0.0043 lr: 0.02\n",
            "iteration: 18420 loss: 0.0042 lr: 0.02\n",
            "iteration: 18430 loss: 0.0047 lr: 0.02\n",
            "iteration: 18440 loss: 0.0053 lr: 0.02\n",
            "iteration: 18450 loss: 0.0037 lr: 0.02\n",
            "iteration: 18460 loss: 0.0065 lr: 0.02\n",
            "iteration: 18470 loss: 0.0044 lr: 0.02\n",
            "iteration: 18480 loss: 0.0039 lr: 0.02\n",
            "iteration: 18490 loss: 0.0049 lr: 0.02\n",
            "iteration: 18500 loss: 0.0055 lr: 0.02\n",
            "iteration: 18510 loss: 0.0054 lr: 0.02\n",
            "iteration: 18520 loss: 0.0065 lr: 0.02\n",
            "iteration: 18530 loss: 0.0057 lr: 0.02\n",
            "iteration: 18540 loss: 0.0051 lr: 0.02\n",
            "iteration: 18550 loss: 0.0054 lr: 0.02\n",
            "iteration: 18560 loss: 0.0056 lr: 0.02\n",
            "iteration: 18570 loss: 0.0047 lr: 0.02\n",
            "iteration: 18580 loss: 0.0043 lr: 0.02\n",
            "iteration: 18590 loss: 0.0054 lr: 0.02\n",
            "iteration: 18600 loss: 0.0041 lr: 0.02\n",
            "iteration: 18610 loss: 0.0040 lr: 0.02\n",
            "iteration: 18620 loss: 0.0061 lr: 0.02\n",
            "iteration: 18630 loss: 0.0041 lr: 0.02\n",
            "iteration: 18640 loss: 0.0049 lr: 0.02\n",
            "iteration: 18650 loss: 0.0049 lr: 0.02\n",
            "iteration: 18660 loss: 0.0043 lr: 0.02\n",
            "iteration: 18670 loss: 0.0045 lr: 0.02\n",
            "iteration: 18680 loss: 0.0046 lr: 0.02\n",
            "iteration: 18690 loss: 0.0049 lr: 0.02\n",
            "iteration: 18700 loss: 0.0043 lr: 0.02\n",
            "iteration: 18710 loss: 0.0056 lr: 0.02\n",
            "iteration: 18720 loss: 0.0057 lr: 0.02\n",
            "iteration: 18730 loss: 0.0061 lr: 0.02\n",
            "iteration: 18740 loss: 0.0051 lr: 0.02\n",
            "iteration: 18750 loss: 0.0047 lr: 0.02\n",
            "iteration: 18760 loss: 0.0036 lr: 0.02\n",
            "iteration: 18770 loss: 0.0038 lr: 0.02\n",
            "iteration: 18780 loss: 0.0048 lr: 0.02\n",
            "iteration: 18790 loss: 0.0051 lr: 0.02\n",
            "iteration: 18800 loss: 0.0046 lr: 0.02\n",
            "iteration: 18810 loss: 0.0053 lr: 0.02\n",
            "iteration: 18820 loss: 0.0053 lr: 0.02\n",
            "iteration: 18830 loss: 0.0053 lr: 0.02\n",
            "iteration: 18840 loss: 0.0082 lr: 0.02\n",
            "iteration: 18850 loss: 0.0046 lr: 0.02\n",
            "iteration: 18860 loss: 0.0048 lr: 0.02\n",
            "iteration: 18870 loss: 0.0047 lr: 0.02\n",
            "iteration: 18880 loss: 0.0054 lr: 0.02\n",
            "iteration: 18890 loss: 0.0054 lr: 0.02\n",
            "iteration: 18900 loss: 0.0053 lr: 0.02\n",
            "iteration: 18910 loss: 0.0038 lr: 0.02\n",
            "iteration: 18920 loss: 0.0048 lr: 0.02\n",
            "iteration: 18930 loss: 0.0063 lr: 0.02\n",
            "iteration: 18940 loss: 0.0058 lr: 0.02\n",
            "iteration: 18950 loss: 0.0044 lr: 0.02\n",
            "iteration: 18960 loss: 0.0053 lr: 0.02\n",
            "iteration: 18970 loss: 0.0048 lr: 0.02\n",
            "iteration: 18980 loss: 0.0052 lr: 0.02\n",
            "iteration: 18990 loss: 0.0058 lr: 0.02\n",
            "iteration: 19000 loss: 0.0046 lr: 0.02\n",
            "iteration: 19010 loss: 0.0042 lr: 0.02\n",
            "iteration: 19020 loss: 0.0057 lr: 0.02\n",
            "iteration: 19030 loss: 0.0058 lr: 0.02\n",
            "iteration: 19040 loss: 0.0065 lr: 0.02\n",
            "iteration: 19050 loss: 0.0044 lr: 0.02\n",
            "iteration: 19060 loss: 0.0038 lr: 0.02\n",
            "iteration: 19070 loss: 0.0044 lr: 0.02\n",
            "iteration: 19080 loss: 0.0047 lr: 0.02\n",
            "iteration: 19090 loss: 0.0048 lr: 0.02\n",
            "iteration: 19100 loss: 0.0047 lr: 0.02\n",
            "iteration: 19110 loss: 0.0051 lr: 0.02\n",
            "iteration: 19120 loss: 0.0068 lr: 0.02\n",
            "iteration: 19130 loss: 0.0061 lr: 0.02\n",
            "iteration: 19140 loss: 0.0048 lr: 0.02\n",
            "iteration: 19150 loss: 0.0046 lr: 0.02\n",
            "iteration: 19160 loss: 0.0051 lr: 0.02\n",
            "iteration: 19170 loss: 0.0049 lr: 0.02\n",
            "iteration: 19180 loss: 0.0047 lr: 0.02\n",
            "iteration: 19190 loss: 0.0040 lr: 0.02\n",
            "iteration: 19200 loss: 0.0041 lr: 0.02\n",
            "iteration: 19210 loss: 0.0056 lr: 0.02\n",
            "iteration: 19220 loss: 0.0039 lr: 0.02\n",
            "iteration: 19230 loss: 0.0040 lr: 0.02\n",
            "iteration: 19240 loss: 0.0054 lr: 0.02\n",
            "iteration: 19250 loss: 0.0047 lr: 0.02\n",
            "iteration: 19260 loss: 0.0051 lr: 0.02\n",
            "iteration: 19270 loss: 0.0053 lr: 0.02\n",
            "iteration: 19280 loss: 0.0055 lr: 0.02\n",
            "iteration: 19290 loss: 0.0040 lr: 0.02\n",
            "iteration: 19300 loss: 0.0051 lr: 0.02\n",
            "iteration: 19310 loss: 0.0046 lr: 0.02\n",
            "iteration: 19320 loss: 0.0046 lr: 0.02\n",
            "iteration: 19330 loss: 0.0043 lr: 0.02\n",
            "iteration: 19340 loss: 0.0047 lr: 0.02\n",
            "iteration: 19350 loss: 0.0057 lr: 0.02\n",
            "iteration: 19360 loss: 0.0034 lr: 0.02\n",
            "iteration: 19370 loss: 0.0050 lr: 0.02\n",
            "iteration: 19380 loss: 0.0049 lr: 0.02\n",
            "iteration: 19390 loss: 0.0045 lr: 0.02\n",
            "iteration: 19400 loss: 0.0040 lr: 0.02\n",
            "iteration: 19410 loss: 0.0051 lr: 0.02\n",
            "iteration: 19420 loss: 0.0040 lr: 0.02\n",
            "iteration: 19430 loss: 0.0047 lr: 0.02\n",
            "iteration: 19440 loss: 0.0043 lr: 0.02\n",
            "iteration: 19450 loss: 0.0050 lr: 0.02\n",
            "iteration: 19460 loss: 0.0042 lr: 0.02\n",
            "iteration: 19470 loss: 0.0053 lr: 0.02\n",
            "iteration: 19480 loss: 0.0049 lr: 0.02\n",
            "iteration: 19490 loss: 0.0056 lr: 0.02\n",
            "iteration: 19500 loss: 0.0039 lr: 0.02\n",
            "iteration: 19510 loss: 0.0056 lr: 0.02\n",
            "iteration: 19520 loss: 0.0067 lr: 0.02\n",
            "iteration: 19530 loss: 0.0039 lr: 0.02\n",
            "iteration: 19540 loss: 0.0054 lr: 0.02\n",
            "iteration: 19550 loss: 0.0043 lr: 0.02\n",
            "iteration: 19560 loss: 0.0045 lr: 0.02\n",
            "iteration: 19570 loss: 0.0046 lr: 0.02\n",
            "iteration: 19580 loss: 0.0047 lr: 0.02\n",
            "iteration: 19590 loss: 0.0041 lr: 0.02\n",
            "iteration: 19600 loss: 0.0059 lr: 0.02\n",
            "iteration: 19610 loss: 0.0059 lr: 0.02\n",
            "iteration: 19620 loss: 0.0068 lr: 0.02\n",
            "iteration: 19630 loss: 0.0056 lr: 0.02\n",
            "iteration: 19640 loss: 0.0049 lr: 0.02\n",
            "iteration: 19650 loss: 0.0050 lr: 0.02\n",
            "iteration: 19660 loss: 0.0051 lr: 0.02\n",
            "iteration: 19670 loss: 0.0041 lr: 0.02\n",
            "iteration: 19680 loss: 0.0055 lr: 0.02\n",
            "iteration: 19690 loss: 0.0047 lr: 0.02\n",
            "iteration: 19700 loss: 0.0051 lr: 0.02\n",
            "iteration: 19710 loss: 0.0048 lr: 0.02\n",
            "iteration: 19720 loss: 0.0054 lr: 0.02\n",
            "iteration: 19730 loss: 0.0044 lr: 0.02\n",
            "iteration: 19740 loss: 0.0045 lr: 0.02\n",
            "iteration: 19750 loss: 0.0046 lr: 0.02\n",
            "iteration: 19760 loss: 0.0060 lr: 0.02\n",
            "iteration: 19770 loss: 0.0045 lr: 0.02\n",
            "iteration: 19780 loss: 0.0037 lr: 0.02\n",
            "iteration: 19790 loss: 0.0042 lr: 0.02\n",
            "iteration: 19800 loss: 0.0041 lr: 0.02\n",
            "iteration: 19810 loss: 0.0063 lr: 0.02\n",
            "iteration: 19820 loss: 0.0055 lr: 0.02\n",
            "iteration: 19830 loss: 0.0045 lr: 0.02\n",
            "iteration: 19840 loss: 0.0044 lr: 0.02\n",
            "iteration: 19850 loss: 0.0038 lr: 0.02\n",
            "iteration: 19860 loss: 0.0049 lr: 0.02\n",
            "iteration: 19870 loss: 0.0054 lr: 0.02\n",
            "iteration: 19880 loss: 0.0054 lr: 0.02\n",
            "iteration: 19890 loss: 0.0040 lr: 0.02\n",
            "iteration: 19900 loss: 0.0048 lr: 0.02\n",
            "iteration: 19910 loss: 0.0045 lr: 0.02\n",
            "iteration: 19920 loss: 0.0067 lr: 0.02\n",
            "iteration: 19930 loss: 0.0071 lr: 0.02\n",
            "iteration: 19940 loss: 0.0051 lr: 0.02\n",
            "iteration: 19950 loss: 0.0054 lr: 0.02\n",
            "iteration: 19960 loss: 0.0055 lr: 0.02\n",
            "iteration: 19970 loss: 0.0063 lr: 0.02\n",
            "iteration: 19980 loss: 0.0050 lr: 0.02\n",
            "iteration: 19990 loss: 0.0055 lr: 0.02\n",
            "iteration: 20000 loss: 0.0048 lr: 0.02\n",
            "iteration: 20010 loss: 0.0050 lr: 0.02\n",
            "iteration: 20020 loss: 0.0039 lr: 0.02\n",
            "iteration: 20030 loss: 0.0050 lr: 0.02\n",
            "iteration: 20040 loss: 0.0035 lr: 0.02\n",
            "iteration: 20050 loss: 0.0041 lr: 0.02\n",
            "iteration: 20060 loss: 0.0038 lr: 0.02\n",
            "iteration: 20070 loss: 0.0040 lr: 0.02\n",
            "iteration: 20080 loss: 0.0050 lr: 0.02\n",
            "iteration: 20090 loss: 0.0046 lr: 0.02\n",
            "iteration: 20100 loss: 0.0067 lr: 0.02\n",
            "iteration: 20110 loss: 0.0063 lr: 0.02\n",
            "iteration: 20120 loss: 0.0049 lr: 0.02\n",
            "iteration: 20130 loss: 0.0041 lr: 0.02\n",
            "iteration: 20140 loss: 0.0046 lr: 0.02\n",
            "iteration: 20150 loss: 0.0047 lr: 0.02\n",
            "iteration: 20160 loss: 0.0043 lr: 0.02\n",
            "iteration: 20170 loss: 0.0055 lr: 0.02\n",
            "iteration: 20180 loss: 0.0042 lr: 0.02\n",
            "iteration: 20190 loss: 0.0055 lr: 0.02\n",
            "iteration: 20200 loss: 0.0043 lr: 0.02\n",
            "iteration: 20210 loss: 0.0038 lr: 0.02\n",
            "iteration: 20220 loss: 0.0052 lr: 0.02\n",
            "iteration: 20230 loss: 0.0057 lr: 0.02\n",
            "iteration: 20240 loss: 0.0036 lr: 0.02\n",
            "iteration: 20250 loss: 0.0043 lr: 0.02\n",
            "iteration: 20260 loss: 0.0041 lr: 0.02\n",
            "iteration: 20270 loss: 0.0050 lr: 0.02\n",
            "iteration: 20280 loss: 0.0052 lr: 0.02\n",
            "iteration: 20290 loss: 0.0046 lr: 0.02\n",
            "iteration: 20300 loss: 0.0058 lr: 0.02\n",
            "iteration: 20310 loss: 0.0056 lr: 0.02\n",
            "iteration: 20320 loss: 0.0049 lr: 0.02\n",
            "iteration: 20330 loss: 0.0050 lr: 0.02\n",
            "iteration: 20340 loss: 0.0046 lr: 0.02\n",
            "iteration: 20350 loss: 0.0044 lr: 0.02\n",
            "iteration: 20360 loss: 0.0036 lr: 0.02\n",
            "iteration: 20370 loss: 0.0044 lr: 0.02\n",
            "iteration: 20380 loss: 0.0067 lr: 0.02\n",
            "iteration: 20390 loss: 0.0058 lr: 0.02\n",
            "iteration: 20400 loss: 0.0055 lr: 0.02\n",
            "iteration: 20410 loss: 0.0050 lr: 0.02\n",
            "iteration: 20420 loss: 0.0046 lr: 0.02\n",
            "iteration: 20430 loss: 0.0057 lr: 0.02\n",
            "iteration: 20440 loss: 0.0046 lr: 0.02\n",
            "iteration: 20450 loss: 0.0043 lr: 0.02\n",
            "iteration: 20460 loss: 0.0039 lr: 0.02\n",
            "iteration: 20470 loss: 0.0055 lr: 0.02\n",
            "iteration: 20480 loss: 0.0044 lr: 0.02\n",
            "iteration: 20490 loss: 0.0046 lr: 0.02\n",
            "iteration: 20500 loss: 0.0041 lr: 0.02\n",
            "iteration: 20510 loss: 0.0040 lr: 0.02\n",
            "iteration: 20520 loss: 0.0064 lr: 0.02\n",
            "iteration: 20530 loss: 0.0042 lr: 0.02\n",
            "iteration: 20540 loss: 0.0035 lr: 0.02\n",
            "iteration: 20550 loss: 0.0066 lr: 0.02\n",
            "iteration: 20560 loss: 0.0045 lr: 0.02\n",
            "iteration: 20570 loss: 0.0038 lr: 0.02\n",
            "iteration: 20580 loss: 0.0051 lr: 0.02\n",
            "iteration: 20590 loss: 0.0043 lr: 0.02\n",
            "iteration: 20600 loss: 0.0043 lr: 0.02\n",
            "iteration: 20610 loss: 0.0045 lr: 0.02\n",
            "iteration: 20620 loss: 0.0059 lr: 0.02\n",
            "iteration: 20630 loss: 0.0040 lr: 0.02\n",
            "iteration: 20640 loss: 0.0066 lr: 0.02\n",
            "iteration: 20650 loss: 0.0044 lr: 0.02\n",
            "iteration: 20660 loss: 0.0050 lr: 0.02\n",
            "iteration: 20670 loss: 0.0045 lr: 0.02\n",
            "iteration: 20680 loss: 0.0046 lr: 0.02\n",
            "iteration: 20690 loss: 0.0040 lr: 0.02\n",
            "iteration: 20700 loss: 0.0042 lr: 0.02\n",
            "iteration: 20710 loss: 0.0045 lr: 0.02\n",
            "iteration: 20720 loss: 0.0043 lr: 0.02\n",
            "iteration: 20730 loss: 0.0034 lr: 0.02\n",
            "iteration: 20740 loss: 0.0044 lr: 0.02\n",
            "iteration: 20750 loss: 0.0041 lr: 0.02\n",
            "iteration: 20760 loss: 0.0045 lr: 0.02\n",
            "iteration: 20770 loss: 0.0042 lr: 0.02\n",
            "iteration: 20780 loss: 0.0049 lr: 0.02\n",
            "iteration: 20790 loss: 0.0061 lr: 0.02\n",
            "iteration: 20800 loss: 0.0055 lr: 0.02\n",
            "iteration: 20810 loss: 0.0040 lr: 0.02\n",
            "iteration: 20820 loss: 0.0060 lr: 0.02\n",
            "iteration: 20830 loss: 0.0040 lr: 0.02\n",
            "iteration: 20840 loss: 0.0050 lr: 0.02\n",
            "iteration: 20850 loss: 0.0038 lr: 0.02\n",
            "iteration: 20860 loss: 0.0041 lr: 0.02\n",
            "iteration: 20870 loss: 0.0055 lr: 0.02\n",
            "iteration: 20880 loss: 0.0052 lr: 0.02\n",
            "iteration: 20890 loss: 0.0050 lr: 0.02\n",
            "iteration: 20900 loss: 0.0038 lr: 0.02\n",
            "iteration: 20910 loss: 0.0041 lr: 0.02\n",
            "iteration: 20920 loss: 0.0047 lr: 0.02\n",
            "iteration: 20930 loss: 0.0044 lr: 0.02\n",
            "iteration: 20940 loss: 0.0042 lr: 0.02\n",
            "iteration: 20950 loss: 0.0053 lr: 0.02\n",
            "iteration: 20960 loss: 0.0060 lr: 0.02\n",
            "iteration: 20970 loss: 0.0042 lr: 0.02\n",
            "iteration: 20980 loss: 0.0053 lr: 0.02\n",
            "iteration: 20990 loss: 0.0055 lr: 0.02\n",
            "iteration: 21000 loss: 0.0044 lr: 0.02\n",
            "iteration: 21010 loss: 0.0059 lr: 0.02\n",
            "iteration: 21020 loss: 0.0038 lr: 0.02\n",
            "iteration: 21030 loss: 0.0041 lr: 0.02\n",
            "iteration: 21040 loss: 0.0041 lr: 0.02\n",
            "iteration: 21050 loss: 0.0043 lr: 0.02\n",
            "iteration: 21060 loss: 0.0043 lr: 0.02\n",
            "iteration: 21070 loss: 0.0037 lr: 0.02\n",
            "iteration: 21080 loss: 0.0056 lr: 0.02\n",
            "iteration: 21090 loss: 0.0035 lr: 0.02\n",
            "iteration: 21100 loss: 0.0040 lr: 0.02\n",
            "iteration: 21110 loss: 0.0041 lr: 0.02\n",
            "iteration: 21120 loss: 0.0046 lr: 0.02\n",
            "iteration: 21130 loss: 0.0051 lr: 0.02\n",
            "iteration: 21140 loss: 0.0044 lr: 0.02\n",
            "iteration: 21150 loss: 0.0039 lr: 0.02\n",
            "iteration: 21160 loss: 0.0043 lr: 0.02\n",
            "iteration: 21170 loss: 0.0041 lr: 0.02\n",
            "iteration: 21180 loss: 0.0049 lr: 0.02\n",
            "iteration: 21190 loss: 0.0056 lr: 0.02\n",
            "iteration: 21200 loss: 0.0040 lr: 0.02\n",
            "iteration: 21210 loss: 0.0043 lr: 0.02\n",
            "iteration: 21220 loss: 0.0050 lr: 0.02\n",
            "iteration: 21230 loss: 0.0046 lr: 0.02\n",
            "iteration: 21240 loss: 0.0051 lr: 0.02\n",
            "iteration: 21250 loss: 0.0044 lr: 0.02\n",
            "iteration: 21260 loss: 0.0058 lr: 0.02\n",
            "iteration: 21270 loss: 0.0051 lr: 0.02\n",
            "iteration: 21280 loss: 0.0042 lr: 0.02\n",
            "iteration: 21290 loss: 0.0034 lr: 0.02\n",
            "iteration: 21300 loss: 0.0047 lr: 0.02\n",
            "iteration: 21310 loss: 0.0047 lr: 0.02\n",
            "iteration: 21320 loss: 0.0034 lr: 0.02\n",
            "iteration: 21330 loss: 0.0042 lr: 0.02\n",
            "iteration: 21340 loss: 0.0037 lr: 0.02\n",
            "iteration: 21350 loss: 0.0050 lr: 0.02\n",
            "iteration: 21360 loss: 0.0052 lr: 0.02\n",
            "iteration: 21370 loss: 0.0055 lr: 0.02\n",
            "iteration: 21380 loss: 0.0046 lr: 0.02\n",
            "iteration: 21390 loss: 0.0041 lr: 0.02\n",
            "iteration: 21400 loss: 0.0046 lr: 0.02\n",
            "iteration: 21410 loss: 0.0044 lr: 0.02\n",
            "iteration: 21420 loss: 0.0037 lr: 0.02\n",
            "iteration: 21430 loss: 0.0055 lr: 0.02\n",
            "iteration: 21440 loss: 0.0039 lr: 0.02\n",
            "iteration: 21450 loss: 0.0062 lr: 0.02\n",
            "iteration: 21460 loss: 0.0049 lr: 0.02\n",
            "iteration: 21470 loss: 0.0047 lr: 0.02\n",
            "iteration: 21480 loss: 0.0050 lr: 0.02\n",
            "iteration: 21490 loss: 0.0038 lr: 0.02\n",
            "iteration: 21500 loss: 0.0040 lr: 0.02\n",
            "iteration: 21510 loss: 0.0044 lr: 0.02\n",
            "iteration: 21520 loss: 0.0042 lr: 0.02\n",
            "iteration: 21530 loss: 0.0043 lr: 0.02\n",
            "iteration: 21540 loss: 0.0046 lr: 0.02\n",
            "iteration: 21550 loss: 0.0034 lr: 0.02\n",
            "iteration: 21560 loss: 0.0047 lr: 0.02\n",
            "iteration: 21570 loss: 0.0040 lr: 0.02\n",
            "iteration: 21580 loss: 0.0039 lr: 0.02\n",
            "iteration: 21590 loss: 0.0042 lr: 0.02\n",
            "iteration: 21600 loss: 0.0039 lr: 0.02\n",
            "iteration: 21610 loss: 0.0050 lr: 0.02\n",
            "iteration: 21620 loss: 0.0034 lr: 0.02\n",
            "iteration: 21630 loss: 0.0042 lr: 0.02\n",
            "iteration: 21640 loss: 0.0043 lr: 0.02\n",
            "iteration: 21650 loss: 0.0043 lr: 0.02\n",
            "iteration: 21660 loss: 0.0043 lr: 0.02\n",
            "iteration: 21670 loss: 0.0042 lr: 0.02\n",
            "iteration: 21680 loss: 0.0055 lr: 0.02\n",
            "iteration: 21690 loss: 0.0047 lr: 0.02\n",
            "iteration: 21700 loss: 0.0043 lr: 0.02\n",
            "iteration: 21710 loss: 0.0060 lr: 0.02\n",
            "iteration: 21720 loss: 0.0054 lr: 0.02\n",
            "iteration: 21730 loss: 0.0054 lr: 0.02\n",
            "iteration: 21740 loss: 0.0047 lr: 0.02\n",
            "iteration: 21750 loss: 0.0043 lr: 0.02\n",
            "iteration: 21760 loss: 0.0044 lr: 0.02\n",
            "iteration: 21770 loss: 0.0044 lr: 0.02\n",
            "iteration: 21780 loss: 0.0041 lr: 0.02\n",
            "iteration: 21790 loss: 0.0043 lr: 0.02\n",
            "iteration: 21800 loss: 0.0045 lr: 0.02\n",
            "iteration: 21810 loss: 0.0036 lr: 0.02\n",
            "iteration: 21820 loss: 0.0040 lr: 0.02\n",
            "iteration: 21830 loss: 0.0041 lr: 0.02\n",
            "iteration: 21840 loss: 0.0040 lr: 0.02\n",
            "iteration: 21850 loss: 0.0039 lr: 0.02\n",
            "iteration: 21860 loss: 0.0074 lr: 0.02\n",
            "iteration: 21870 loss: 0.0041 lr: 0.02\n",
            "iteration: 21880 loss: 0.0052 lr: 0.02\n",
            "iteration: 21890 loss: 0.0046 lr: 0.02\n",
            "iteration: 21900 loss: 0.0034 lr: 0.02\n",
            "iteration: 21910 loss: 0.0046 lr: 0.02\n",
            "iteration: 21920 loss: 0.0045 lr: 0.02\n",
            "iteration: 21930 loss: 0.0043 lr: 0.02\n",
            "iteration: 21940 loss: 0.0062 lr: 0.02\n",
            "iteration: 21950 loss: 0.0046 lr: 0.02\n",
            "iteration: 21960 loss: 0.0043 lr: 0.02\n",
            "iteration: 21970 loss: 0.0053 lr: 0.02\n",
            "iteration: 21980 loss: 0.0039 lr: 0.02\n",
            "iteration: 21990 loss: 0.0043 lr: 0.02\n",
            "iteration: 22000 loss: 0.0048 lr: 0.02\n",
            "iteration: 22010 loss: 0.0036 lr: 0.02\n",
            "iteration: 22020 loss: 0.0053 lr: 0.02\n",
            "iteration: 22030 loss: 0.0037 lr: 0.02\n",
            "iteration: 22040 loss: 0.0044 lr: 0.02\n",
            "iteration: 22050 loss: 0.0043 lr: 0.02\n",
            "iteration: 22060 loss: 0.0039 lr: 0.02\n",
            "iteration: 22070 loss: 0.0050 lr: 0.02\n",
            "iteration: 22080 loss: 0.0049 lr: 0.02\n",
            "iteration: 22090 loss: 0.0060 lr: 0.02\n",
            "iteration: 22100 loss: 0.0051 lr: 0.02\n",
            "iteration: 22110 loss: 0.0052 lr: 0.02\n",
            "iteration: 22120 loss: 0.0065 lr: 0.02\n",
            "iteration: 22130 loss: 0.0044 lr: 0.02\n",
            "iteration: 22140 loss: 0.0052 lr: 0.02\n",
            "iteration: 22150 loss: 0.0052 lr: 0.02\n",
            "iteration: 22160 loss: 0.0038 lr: 0.02\n",
            "iteration: 22170 loss: 0.0058 lr: 0.02\n",
            "iteration: 22180 loss: 0.0043 lr: 0.02\n",
            "iteration: 22190 loss: 0.0050 lr: 0.02\n",
            "iteration: 22200 loss: 0.0047 lr: 0.02\n",
            "iteration: 22210 loss: 0.0044 lr: 0.02\n",
            "iteration: 22220 loss: 0.0051 lr: 0.02\n",
            "iteration: 22230 loss: 0.0034 lr: 0.02\n",
            "iteration: 22240 loss: 0.0034 lr: 0.02\n",
            "iteration: 22250 loss: 0.0041 lr: 0.02\n",
            "iteration: 22260 loss: 0.0043 lr: 0.02\n",
            "iteration: 22270 loss: 0.0045 lr: 0.02\n",
            "iteration: 22280 loss: 0.0051 lr: 0.02\n",
            "iteration: 22290 loss: 0.0042 lr: 0.02\n",
            "iteration: 22300 loss: 0.0043 lr: 0.02\n",
            "iteration: 22310 loss: 0.0046 lr: 0.02\n",
            "iteration: 22320 loss: 0.0042 lr: 0.02\n",
            "iteration: 22330 loss: 0.0049 lr: 0.02\n",
            "iteration: 22340 loss: 0.0040 lr: 0.02\n",
            "iteration: 22350 loss: 0.0040 lr: 0.02\n",
            "iteration: 22360 loss: 0.0038 lr: 0.02\n",
            "iteration: 22370 loss: 0.0039 lr: 0.02\n",
            "iteration: 22380 loss: 0.0043 lr: 0.02\n",
            "iteration: 22390 loss: 0.0037 lr: 0.02\n",
            "iteration: 22400 loss: 0.0051 lr: 0.02\n",
            "iteration: 22410 loss: 0.0043 lr: 0.02\n",
            "iteration: 22420 loss: 0.0046 lr: 0.02\n",
            "iteration: 22430 loss: 0.0051 lr: 0.02\n",
            "iteration: 22440 loss: 0.0042 lr: 0.02\n",
            "iteration: 22450 loss: 0.0056 lr: 0.02\n",
            "iteration: 22460 loss: 0.0039 lr: 0.02\n",
            "iteration: 22470 loss: 0.0057 lr: 0.02\n",
            "iteration: 22480 loss: 0.0046 lr: 0.02\n",
            "iteration: 22490 loss: 0.0074 lr: 0.02\n",
            "iteration: 22500 loss: 0.0049 lr: 0.02\n",
            "iteration: 22510 loss: 0.0040 lr: 0.02\n",
            "iteration: 22520 loss: 0.0039 lr: 0.02\n",
            "iteration: 22530 loss: 0.0041 lr: 0.02\n",
            "iteration: 22540 loss: 0.0052 lr: 0.02\n",
            "iteration: 22550 loss: 0.0042 lr: 0.02\n",
            "iteration: 22560 loss: 0.0040 lr: 0.02\n",
            "iteration: 22570 loss: 0.0042 lr: 0.02\n",
            "iteration: 22580 loss: 0.0045 lr: 0.02\n",
            "iteration: 22590 loss: 0.0041 lr: 0.02\n",
            "iteration: 22600 loss: 0.0040 lr: 0.02\n",
            "iteration: 22610 loss: 0.0041 lr: 0.02\n",
            "iteration: 22620 loss: 0.0050 lr: 0.02\n",
            "iteration: 22630 loss: 0.0035 lr: 0.02\n",
            "iteration: 22640 loss: 0.0029 lr: 0.02\n",
            "iteration: 22650 loss: 0.0047 lr: 0.02\n",
            "iteration: 22660 loss: 0.0033 lr: 0.02\n",
            "iteration: 22670 loss: 0.0031 lr: 0.02\n",
            "iteration: 22680 loss: 0.0037 lr: 0.02\n",
            "iteration: 22690 loss: 0.0047 lr: 0.02\n",
            "iteration: 22700 loss: 0.0050 lr: 0.02\n",
            "iteration: 22710 loss: 0.0035 lr: 0.02\n",
            "iteration: 22720 loss: 0.0043 lr: 0.02\n",
            "iteration: 22730 loss: 0.0048 lr: 0.02\n",
            "iteration: 22740 loss: 0.0046 lr: 0.02\n",
            "iteration: 22750 loss: 0.0050 lr: 0.02\n",
            "iteration: 22760 loss: 0.0035 lr: 0.02\n",
            "iteration: 22770 loss: 0.0039 lr: 0.02\n",
            "iteration: 22780 loss: 0.0044 lr: 0.02\n",
            "iteration: 22790 loss: 0.0041 lr: 0.02\n",
            "iteration: 22800 loss: 0.0043 lr: 0.02\n",
            "iteration: 22810 loss: 0.0049 lr: 0.02\n",
            "iteration: 22820 loss: 0.0045 lr: 0.02\n",
            "iteration: 22830 loss: 0.0040 lr: 0.02\n",
            "iteration: 22840 loss: 0.0046 lr: 0.02\n",
            "iteration: 22850 loss: 0.0061 lr: 0.02\n",
            "iteration: 22860 loss: 0.0047 lr: 0.02\n",
            "iteration: 22870 loss: 0.0050 lr: 0.02\n",
            "iteration: 22880 loss: 0.0039 lr: 0.02\n",
            "iteration: 22890 loss: 0.0038 lr: 0.02\n",
            "iteration: 22900 loss: 0.0038 lr: 0.02\n",
            "iteration: 22910 loss: 0.0043 lr: 0.02\n",
            "iteration: 22920 loss: 0.0042 lr: 0.02\n",
            "iteration: 22930 loss: 0.0039 lr: 0.02\n",
            "iteration: 22940 loss: 0.0047 lr: 0.02\n",
            "iteration: 22950 loss: 0.0046 lr: 0.02\n",
            "iteration: 22960 loss: 0.0051 lr: 0.02\n",
            "iteration: 22970 loss: 0.0044 lr: 0.02\n",
            "iteration: 22980 loss: 0.0038 lr: 0.02\n",
            "iteration: 22990 loss: 0.0043 lr: 0.02\n",
            "iteration: 23000 loss: 0.0052 lr: 0.02\n",
            "iteration: 23010 loss: 0.0046 lr: 0.02\n",
            "iteration: 23020 loss: 0.0036 lr: 0.02\n",
            "iteration: 23030 loss: 0.0044 lr: 0.02\n",
            "iteration: 23040 loss: 0.0038 lr: 0.02\n",
            "iteration: 23050 loss: 0.0032 lr: 0.02\n",
            "iteration: 23060 loss: 0.0040 lr: 0.02\n",
            "iteration: 23070 loss: 0.0040 lr: 0.02\n",
            "iteration: 23080 loss: 0.0034 lr: 0.02\n",
            "iteration: 23090 loss: 0.0037 lr: 0.02\n",
            "iteration: 23100 loss: 0.0045 lr: 0.02\n",
            "iteration: 23110 loss: 0.0033 lr: 0.02\n",
            "iteration: 23120 loss: 0.0032 lr: 0.02\n",
            "iteration: 23130 loss: 0.0033 lr: 0.02\n",
            "iteration: 23140 loss: 0.0038 lr: 0.02\n",
            "iteration: 23150 loss: 0.0057 lr: 0.02\n",
            "iteration: 23160 loss: 0.0134 lr: 0.02\n",
            "iteration: 23170 loss: 0.0100 lr: 0.02\n",
            "iteration: 23180 loss: 0.0101 lr: 0.02\n",
            "iteration: 23190 loss: 0.0083 lr: 0.02\n",
            "iteration: 23200 loss: 0.0054 lr: 0.02\n",
            "iteration: 23210 loss: 0.0052 lr: 0.02\n",
            "iteration: 23220 loss: 0.0066 lr: 0.02\n",
            "iteration: 23230 loss: 0.0054 lr: 0.02\n",
            "iteration: 23240 loss: 0.0042 lr: 0.02\n",
            "iteration: 23250 loss: 0.0065 lr: 0.02\n",
            "iteration: 23260 loss: 0.0047 lr: 0.02\n",
            "iteration: 23270 loss: 0.0049 lr: 0.02\n",
            "iteration: 23280 loss: 0.0048 lr: 0.02\n",
            "iteration: 23290 loss: 0.0071 lr: 0.02\n",
            "iteration: 23300 loss: 0.0044 lr: 0.02\n",
            "iteration: 23310 loss: 0.0049 lr: 0.02\n",
            "iteration: 23320 loss: 0.0033 lr: 0.02\n",
            "iteration: 23330 loss: 0.0034 lr: 0.02\n",
            "iteration: 23340 loss: 0.0042 lr: 0.02\n",
            "iteration: 23350 loss: 0.0046 lr: 0.02\n",
            "iteration: 23360 loss: 0.0042 lr: 0.02\n",
            "iteration: 23370 loss: 0.0042 lr: 0.02\n",
            "iteration: 23380 loss: 0.0034 lr: 0.02\n",
            "iteration: 23390 loss: 0.0042 lr: 0.02\n",
            "iteration: 23400 loss: 0.0064 lr: 0.02\n",
            "iteration: 23410 loss: 0.0052 lr: 0.02\n",
            "iteration: 23420 loss: 0.0053 lr: 0.02\n",
            "iteration: 23430 loss: 0.0040 lr: 0.02\n",
            "iteration: 23440 loss: 0.0055 lr: 0.02\n",
            "iteration: 23450 loss: 0.0055 lr: 0.02\n",
            "iteration: 23460 loss: 0.0053 lr: 0.02\n",
            "iteration: 23470 loss: 0.0042 lr: 0.02\n",
            "iteration: 23480 loss: 0.0044 lr: 0.02\n",
            "iteration: 23490 loss: 0.0044 lr: 0.02\n",
            "iteration: 23500 loss: 0.0046 lr: 0.02\n",
            "iteration: 23510 loss: 0.0048 lr: 0.02\n",
            "iteration: 23520 loss: 0.0037 lr: 0.02\n",
            "iteration: 23530 loss: 0.0037 lr: 0.02\n",
            "iteration: 23540 loss: 0.0034 lr: 0.02\n",
            "iteration: 23550 loss: 0.0041 lr: 0.02\n",
            "iteration: 23560 loss: 0.0038 lr: 0.02\n",
            "iteration: 23570 loss: 0.0039 lr: 0.02\n",
            "iteration: 23580 loss: 0.0044 lr: 0.02\n",
            "iteration: 23590 loss: 0.0047 lr: 0.02\n",
            "iteration: 23600 loss: 0.0047 lr: 0.02\n",
            "iteration: 23610 loss: 0.0047 lr: 0.02\n",
            "iteration: 23620 loss: 0.0036 lr: 0.02\n",
            "iteration: 23630 loss: 0.0042 lr: 0.02\n",
            "iteration: 23640 loss: 0.0044 lr: 0.02\n",
            "iteration: 23650 loss: 0.0040 lr: 0.02\n",
            "iteration: 23660 loss: 0.0038 lr: 0.02\n",
            "iteration: 23670 loss: 0.0046 lr: 0.02\n",
            "iteration: 23680 loss: 0.0048 lr: 0.02\n",
            "iteration: 23690 loss: 0.0041 lr: 0.02\n",
            "iteration: 23700 loss: 0.0043 lr: 0.02\n",
            "iteration: 23710 loss: 0.0037 lr: 0.02\n",
            "iteration: 23720 loss: 0.0028 lr: 0.02\n",
            "iteration: 23730 loss: 0.0041 lr: 0.02\n",
            "iteration: 23740 loss: 0.0043 lr: 0.02\n",
            "iteration: 23750 loss: 0.0044 lr: 0.02\n",
            "iteration: 23760 loss: 0.0043 lr: 0.02\n",
            "iteration: 23770 loss: 0.0042 lr: 0.02\n",
            "iteration: 23780 loss: 0.0036 lr: 0.02\n",
            "iteration: 23790 loss: 0.0035 lr: 0.02\n",
            "iteration: 23800 loss: 0.0039 lr: 0.02\n",
            "iteration: 23810 loss: 0.0045 lr: 0.02\n",
            "iteration: 23820 loss: 0.0055 lr: 0.02\n",
            "iteration: 23830 loss: 0.0039 lr: 0.02\n",
            "iteration: 23840 loss: 0.0045 lr: 0.02\n",
            "iteration: 23850 loss: 0.0046 lr: 0.02\n",
            "iteration: 23860 loss: 0.0046 lr: 0.02\n",
            "iteration: 23870 loss: 0.0045 lr: 0.02\n",
            "iteration: 23880 loss: 0.0048 lr: 0.02\n",
            "iteration: 23890 loss: 0.0035 lr: 0.02\n",
            "iteration: 23900 loss: 0.0034 lr: 0.02\n",
            "iteration: 23910 loss: 0.0033 lr: 0.02\n",
            "iteration: 23920 loss: 0.0035 lr: 0.02\n",
            "iteration: 23930 loss: 0.0035 lr: 0.02\n",
            "iteration: 23940 loss: 0.0034 lr: 0.02\n",
            "iteration: 23950 loss: 0.0041 lr: 0.02\n",
            "iteration: 23960 loss: 0.0043 lr: 0.02\n",
            "iteration: 23970 loss: 0.0048 lr: 0.02\n",
            "iteration: 23980 loss: 0.0037 lr: 0.02\n",
            "iteration: 23990 loss: 0.0037 lr: 0.02\n",
            "iteration: 24000 loss: 0.0038 lr: 0.02\n",
            "iteration: 24010 loss: 0.0038 lr: 0.02\n",
            "iteration: 24020 loss: 0.0046 lr: 0.02\n",
            "iteration: 24030 loss: 0.0044 lr: 0.02\n",
            "iteration: 24040 loss: 0.0048 lr: 0.02\n",
            "iteration: 24050 loss: 0.0042 lr: 0.02\n",
            "iteration: 24060 loss: 0.0041 lr: 0.02\n",
            "iteration: 24070 loss: 0.0042 lr: 0.02\n",
            "iteration: 24080 loss: 0.0045 lr: 0.02\n",
            "iteration: 24090 loss: 0.0029 lr: 0.02\n",
            "iteration: 24100 loss: 0.0052 lr: 0.02\n",
            "iteration: 24110 loss: 0.0038 lr: 0.02\n",
            "iteration: 24120 loss: 0.0043 lr: 0.02\n",
            "iteration: 24130 loss: 0.0042 lr: 0.02\n",
            "iteration: 24140 loss: 0.0033 lr: 0.02\n",
            "iteration: 24150 loss: 0.0038 lr: 0.02\n",
            "iteration: 24160 loss: 0.0036 lr: 0.02\n",
            "iteration: 24170 loss: 0.0048 lr: 0.02\n",
            "iteration: 24180 loss: 0.0037 lr: 0.02\n",
            "iteration: 24190 loss: 0.0034 lr: 0.02\n",
            "iteration: 24200 loss: 0.0049 lr: 0.02\n",
            "iteration: 24210 loss: 0.0034 lr: 0.02\n",
            "iteration: 24220 loss: 0.0030 lr: 0.02\n",
            "iteration: 24230 loss: 0.0037 lr: 0.02\n",
            "iteration: 24240 loss: 0.0046 lr: 0.02\n",
            "iteration: 24250 loss: 0.0041 lr: 0.02\n",
            "iteration: 24260 loss: 0.0038 lr: 0.02\n",
            "iteration: 24270 loss: 0.0047 lr: 0.02\n",
            "iteration: 24280 loss: 0.0034 lr: 0.02\n",
            "iteration: 24290 loss: 0.0039 lr: 0.02\n",
            "iteration: 24300 loss: 0.0043 lr: 0.02\n",
            "iteration: 24310 loss: 0.0038 lr: 0.02\n",
            "iteration: 24320 loss: 0.0040 lr: 0.02\n",
            "iteration: 24330 loss: 0.0040 lr: 0.02\n",
            "iteration: 24340 loss: 0.0044 lr: 0.02\n",
            "iteration: 24350 loss: 0.0030 lr: 0.02\n",
            "iteration: 24360 loss: 0.0044 lr: 0.02\n",
            "iteration: 24370 loss: 0.0038 lr: 0.02\n",
            "iteration: 24380 loss: 0.0034 lr: 0.02\n",
            "iteration: 24390 loss: 0.0041 lr: 0.02\n",
            "iteration: 24400 loss: 0.0033 lr: 0.02\n",
            "iteration: 24410 loss: 0.0046 lr: 0.02\n",
            "iteration: 24420 loss: 0.0046 lr: 0.02\n",
            "iteration: 24430 loss: 0.0036 lr: 0.02\n",
            "iteration: 24440 loss: 0.0036 lr: 0.02\n",
            "iteration: 24450 loss: 0.0042 lr: 0.02\n",
            "iteration: 24460 loss: 0.0053 lr: 0.02\n",
            "iteration: 24470 loss: 0.0046 lr: 0.02\n",
            "iteration: 24480 loss: 0.0044 lr: 0.02\n",
            "iteration: 24490 loss: 0.0034 lr: 0.02\n",
            "iteration: 24500 loss: 0.0042 lr: 0.02\n",
            "iteration: 24510 loss: 0.0036 lr: 0.02\n",
            "iteration: 24520 loss: 0.0042 lr: 0.02\n",
            "iteration: 24530 loss: 0.0045 lr: 0.02\n",
            "iteration: 24540 loss: 0.0033 lr: 0.02\n",
            "iteration: 24550 loss: 0.0033 lr: 0.02\n",
            "iteration: 24560 loss: 0.0046 lr: 0.02\n",
            "iteration: 24570 loss: 0.0035 lr: 0.02\n",
            "iteration: 24580 loss: 0.0034 lr: 0.02\n",
            "iteration: 24590 loss: 0.0040 lr: 0.02\n",
            "iteration: 24600 loss: 0.0029 lr: 0.02\n",
            "iteration: 24610 loss: 0.0042 lr: 0.02\n",
            "iteration: 24620 loss: 0.0027 lr: 0.02\n",
            "iteration: 24630 loss: 0.0034 lr: 0.02\n",
            "iteration: 24640 loss: 0.0039 lr: 0.02\n",
            "iteration: 24650 loss: 0.0040 lr: 0.02\n",
            "iteration: 24660 loss: 0.0046 lr: 0.02\n",
            "iteration: 24670 loss: 0.0037 lr: 0.02\n",
            "iteration: 24680 loss: 0.0036 lr: 0.02\n",
            "iteration: 24690 loss: 0.0037 lr: 0.02\n",
            "iteration: 24700 loss: 0.0044 lr: 0.02\n",
            "iteration: 24710 loss: 0.0039 lr: 0.02\n",
            "iteration: 24720 loss: 0.0030 lr: 0.02\n",
            "iteration: 24730 loss: 0.0038 lr: 0.02\n",
            "iteration: 24740 loss: 0.0035 lr: 0.02\n",
            "iteration: 24750 loss: 0.0038 lr: 0.02\n",
            "iteration: 24760 loss: 0.0032 lr: 0.02\n",
            "iteration: 24770 loss: 0.0033 lr: 0.02\n",
            "iteration: 24780 loss: 0.0027 lr: 0.02\n",
            "iteration: 24790 loss: 0.0038 lr: 0.02\n",
            "iteration: 24800 loss: 0.0036 lr: 0.02\n",
            "iteration: 24810 loss: 0.0045 lr: 0.02\n",
            "iteration: 24820 loss: 0.0042 lr: 0.02\n",
            "iteration: 24830 loss: 0.0040 lr: 0.02\n",
            "iteration: 24840 loss: 0.0056 lr: 0.02\n",
            "iteration: 24850 loss: 0.0045 lr: 0.02\n",
            "iteration: 24860 loss: 0.0030 lr: 0.02\n",
            "iteration: 24870 loss: 0.0033 lr: 0.02\n",
            "iteration: 24880 loss: 0.0034 lr: 0.02\n",
            "iteration: 24890 loss: 0.0037 lr: 0.02\n",
            "iteration: 24900 loss: 0.0038 lr: 0.02\n",
            "iteration: 24910 loss: 0.0047 lr: 0.02\n",
            "iteration: 24920 loss: 0.0035 lr: 0.02\n",
            "iteration: 24930 loss: 0.0050 lr: 0.02\n",
            "iteration: 24940 loss: 0.0041 lr: 0.02\n",
            "iteration: 24950 loss: 0.0038 lr: 0.02\n",
            "iteration: 24960 loss: 0.0038 lr: 0.02\n",
            "iteration: 24970 loss: 0.0044 lr: 0.02\n",
            "iteration: 24980 loss: 0.0044 lr: 0.02\n",
            "iteration: 24990 loss: 0.0029 lr: 0.02\n",
            "iteration: 25000 loss: 0.0040 lr: 0.02\n",
            "iteration: 25010 loss: 0.0036 lr: 0.02\n",
            "iteration: 25020 loss: 0.0040 lr: 0.02\n",
            "iteration: 25030 loss: 0.0051 lr: 0.02\n",
            "iteration: 25040 loss: 0.0048 lr: 0.02\n",
            "iteration: 25050 loss: 0.0030 lr: 0.02\n",
            "iteration: 25060 loss: 0.0048 lr: 0.02\n",
            "iteration: 25070 loss: 0.0034 lr: 0.02\n",
            "iteration: 25080 loss: 0.0031 lr: 0.02\n",
            "iteration: 25090 loss: 0.0032 lr: 0.02\n",
            "iteration: 25100 loss: 0.0037 lr: 0.02\n",
            "iteration: 25110 loss: 0.0040 lr: 0.02\n",
            "iteration: 25120 loss: 0.0041 lr: 0.02\n",
            "iteration: 25130 loss: 0.0032 lr: 0.02\n",
            "iteration: 25140 loss: 0.0026 lr: 0.02\n",
            "iteration: 25150 loss: 0.0052 lr: 0.02\n",
            "iteration: 25160 loss: 0.0040 lr: 0.02\n",
            "iteration: 25170 loss: 0.0032 lr: 0.02\n",
            "iteration: 25180 loss: 0.0031 lr: 0.02\n",
            "iteration: 25190 loss: 0.0038 lr: 0.02\n",
            "iteration: 25200 loss: 0.0036 lr: 0.02\n",
            "iteration: 25210 loss: 0.0033 lr: 0.02\n",
            "iteration: 25220 loss: 0.0039 lr: 0.02\n",
            "iteration: 25230 loss: 0.0035 lr: 0.02\n",
            "iteration: 25240 loss: 0.0047 lr: 0.02\n",
            "iteration: 25250 loss: 0.0037 lr: 0.02\n",
            "iteration: 25260 loss: 0.0036 lr: 0.02\n",
            "iteration: 25270 loss: 0.0039 lr: 0.02\n",
            "iteration: 25280 loss: 0.0041 lr: 0.02\n",
            "iteration: 25290 loss: 0.0037 lr: 0.02\n",
            "iteration: 25300 loss: 0.0037 lr: 0.02\n",
            "iteration: 25310 loss: 0.0031 lr: 0.02\n",
            "iteration: 25320 loss: 0.0039 lr: 0.02\n",
            "iteration: 25330 loss: 0.0032 lr: 0.02\n",
            "iteration: 25340 loss: 0.0029 lr: 0.02\n",
            "iteration: 25350 loss: 0.0031 lr: 0.02\n",
            "iteration: 25360 loss: 0.0043 lr: 0.02\n",
            "iteration: 25370 loss: 0.0039 lr: 0.02\n",
            "iteration: 25380 loss: 0.0043 lr: 0.02\n",
            "iteration: 25390 loss: 0.0036 lr: 0.02\n",
            "iteration: 25400 loss: 0.0030 lr: 0.02\n",
            "iteration: 25410 loss: 0.0035 lr: 0.02\n",
            "iteration: 25420 loss: 0.0051 lr: 0.02\n",
            "iteration: 25430 loss: 0.0056 lr: 0.02\n",
            "iteration: 25440 loss: 0.0051 lr: 0.02\n",
            "iteration: 25450 loss: 0.0042 lr: 0.02\n",
            "iteration: 25460 loss: 0.0040 lr: 0.02\n",
            "iteration: 25470 loss: 0.0039 lr: 0.02\n",
            "iteration: 25480 loss: 0.0034 lr: 0.02\n",
            "iteration: 25490 loss: 0.0035 lr: 0.02\n",
            "iteration: 25500 loss: 0.0036 lr: 0.02\n",
            "iteration: 25510 loss: 0.0032 lr: 0.02\n",
            "iteration: 25520 loss: 0.0041 lr: 0.02\n",
            "iteration: 25530 loss: 0.0034 lr: 0.02\n",
            "iteration: 25540 loss: 0.0036 lr: 0.02\n",
            "iteration: 25550 loss: 0.0043 lr: 0.02\n",
            "iteration: 25560 loss: 0.0037 lr: 0.02\n",
            "iteration: 25570 loss: 0.0031 lr: 0.02\n",
            "iteration: 25580 loss: 0.0041 lr: 0.02\n",
            "iteration: 25590 loss: 0.0044 lr: 0.02\n",
            "iteration: 25600 loss: 0.0041 lr: 0.02\n",
            "iteration: 25610 loss: 0.0043 lr: 0.02\n",
            "iteration: 25620 loss: 0.0039 lr: 0.02\n",
            "iteration: 25630 loss: 0.0039 lr: 0.02\n",
            "iteration: 25640 loss: 0.0043 lr: 0.02\n",
            "iteration: 25650 loss: 0.0034 lr: 0.02\n",
            "iteration: 25660 loss: 0.0035 lr: 0.02\n",
            "iteration: 25670 loss: 0.0032 lr: 0.02\n",
            "iteration: 25680 loss: 0.0039 lr: 0.02\n",
            "iteration: 25690 loss: 0.0041 lr: 0.02\n",
            "iteration: 25700 loss: 0.0040 lr: 0.02\n",
            "iteration: 25710 loss: 0.0040 lr: 0.02\n",
            "iteration: 25720 loss: 0.0033 lr: 0.02\n",
            "iteration: 25730 loss: 0.0035 lr: 0.02\n",
            "iteration: 25740 loss: 0.0032 lr: 0.02\n",
            "iteration: 25750 loss: 0.0038 lr: 0.02\n",
            "iteration: 25760 loss: 0.0042 lr: 0.02\n",
            "iteration: 25770 loss: 0.0045 lr: 0.02\n",
            "iteration: 25780 loss: 0.0037 lr: 0.02\n",
            "iteration: 25790 loss: 0.0039 lr: 0.02\n",
            "iteration: 25800 loss: 0.0053 lr: 0.02\n",
            "iteration: 25810 loss: 0.0036 lr: 0.02\n",
            "iteration: 25820 loss: 0.0034 lr: 0.02\n",
            "iteration: 25830 loss: 0.0041 lr: 0.02\n",
            "iteration: 25840 loss: 0.0037 lr: 0.02\n",
            "iteration: 25850 loss: 0.0032 lr: 0.02\n",
            "iteration: 25860 loss: 0.0031 lr: 0.02\n",
            "iteration: 25870 loss: 0.0047 lr: 0.02\n",
            "iteration: 25880 loss: 0.0031 lr: 0.02\n",
            "iteration: 25890 loss: 0.0035 lr: 0.02\n",
            "iteration: 25900 loss: 0.0029 lr: 0.02\n",
            "iteration: 25910 loss: 0.0041 lr: 0.02\n",
            "iteration: 25920 loss: 0.0031 lr: 0.02\n",
            "iteration: 25930 loss: 0.0031 lr: 0.02\n",
            "iteration: 25940 loss: 0.0028 lr: 0.02\n",
            "iteration: 25950 loss: 0.0046 lr: 0.02\n",
            "iteration: 25960 loss: 0.0039 lr: 0.02\n",
            "iteration: 25970 loss: 0.0036 lr: 0.02\n",
            "iteration: 25980 loss: 0.0038 lr: 0.02\n",
            "iteration: 25990 loss: 0.0032 lr: 0.02\n",
            "iteration: 26000 loss: 0.0033 lr: 0.02\n",
            "iteration: 26010 loss: 0.0057 lr: 0.02\n",
            "iteration: 26020 loss: 0.0031 lr: 0.02\n",
            "iteration: 26030 loss: 0.0039 lr: 0.02\n",
            "iteration: 26040 loss: 0.0042 lr: 0.02\n",
            "iteration: 26050 loss: 0.0029 lr: 0.02\n",
            "iteration: 26060 loss: 0.0033 lr: 0.02\n",
            "iteration: 26070 loss: 0.0046 lr: 0.02\n",
            "iteration: 26080 loss: 0.0039 lr: 0.02\n",
            "iteration: 26090 loss: 0.0038 lr: 0.02\n",
            "iteration: 26100 loss: 0.0041 lr: 0.02\n",
            "iteration: 26110 loss: 0.0044 lr: 0.02\n",
            "iteration: 26120 loss: 0.0024 lr: 0.02\n",
            "iteration: 26130 loss: 0.0051 lr: 0.02\n",
            "iteration: 26140 loss: 0.0057 lr: 0.02\n",
            "iteration: 26150 loss: 0.0035 lr: 0.02\n",
            "iteration: 26160 loss: 0.0037 lr: 0.02\n",
            "iteration: 26170 loss: 0.0060 lr: 0.02\n",
            "iteration: 26180 loss: 0.0035 lr: 0.02\n",
            "iteration: 26190 loss: 0.0036 lr: 0.02\n",
            "iteration: 26200 loss: 0.0039 lr: 0.02\n",
            "iteration: 26210 loss: 0.0035 lr: 0.02\n",
            "iteration: 26220 loss: 0.0038 lr: 0.02\n",
            "iteration: 26230 loss: 0.0042 lr: 0.02\n",
            "iteration: 26240 loss: 0.0048 lr: 0.02\n",
            "iteration: 26250 loss: 0.0033 lr: 0.02\n",
            "iteration: 26260 loss: 0.0034 lr: 0.02\n",
            "iteration: 26270 loss: 0.0035 lr: 0.02\n",
            "iteration: 26280 loss: 0.0040 lr: 0.02\n",
            "iteration: 26290 loss: 0.0037 lr: 0.02\n",
            "iteration: 26300 loss: 0.0037 lr: 0.02\n",
            "iteration: 26310 loss: 0.0043 lr: 0.02\n",
            "iteration: 26320 loss: 0.0032 lr: 0.02\n",
            "iteration: 26330 loss: 0.0053 lr: 0.02\n",
            "iteration: 26340 loss: 0.0037 lr: 0.02\n",
            "iteration: 26350 loss: 0.0046 lr: 0.02\n",
            "iteration: 26360 loss: 0.0041 lr: 0.02\n",
            "iteration: 26370 loss: 0.0030 lr: 0.02\n",
            "iteration: 26380 loss: 0.0037 lr: 0.02\n",
            "iteration: 26390 loss: 0.0037 lr: 0.02\n",
            "iteration: 26400 loss: 0.0037 lr: 0.02\n",
            "iteration: 26410 loss: 0.0035 lr: 0.02\n",
            "iteration: 26420 loss: 0.0042 lr: 0.02\n",
            "iteration: 26430 loss: 0.0040 lr: 0.02\n",
            "iteration: 26440 loss: 0.0033 lr: 0.02\n",
            "iteration: 26450 loss: 0.0036 lr: 0.02\n",
            "iteration: 26460 loss: 0.0027 lr: 0.02\n",
            "iteration: 26470 loss: 0.0034 lr: 0.02\n",
            "iteration: 26480 loss: 0.0034 lr: 0.02\n",
            "iteration: 26490 loss: 0.0028 lr: 0.02\n",
            "iteration: 26500 loss: 0.0060 lr: 0.02\n",
            "iteration: 26510 loss: 0.0051 lr: 0.02\n",
            "iteration: 26520 loss: 0.0032 lr: 0.02\n",
            "iteration: 26530 loss: 0.0040 lr: 0.02\n",
            "iteration: 26540 loss: 0.0038 lr: 0.02\n",
            "iteration: 26550 loss: 0.0039 lr: 0.02\n",
            "iteration: 26560 loss: 0.0038 lr: 0.02\n",
            "iteration: 26570 loss: 0.0033 lr: 0.02\n",
            "iteration: 26580 loss: 0.0034 lr: 0.02\n",
            "iteration: 26590 loss: 0.0037 lr: 0.02\n",
            "iteration: 26600 loss: 0.0037 lr: 0.02\n",
            "iteration: 26610 loss: 0.0038 lr: 0.02\n",
            "iteration: 26620 loss: 0.0028 lr: 0.02\n",
            "iteration: 26630 loss: 0.0032 lr: 0.02\n",
            "iteration: 26640 loss: 0.0032 lr: 0.02\n",
            "iteration: 26650 loss: 0.0035 lr: 0.02\n",
            "iteration: 26660 loss: 0.0048 lr: 0.02\n",
            "iteration: 26670 loss: 0.0041 lr: 0.02\n",
            "iteration: 26680 loss: 0.0027 lr: 0.02\n",
            "iteration: 26690 loss: 0.0042 lr: 0.02\n",
            "iteration: 26700 loss: 0.0056 lr: 0.02\n",
            "iteration: 26710 loss: 0.0036 lr: 0.02\n",
            "iteration: 26720 loss: 0.0033 lr: 0.02\n",
            "iteration: 26730 loss: 0.0044 lr: 0.02\n",
            "iteration: 26740 loss: 0.0026 lr: 0.02\n",
            "iteration: 26750 loss: 0.0039 lr: 0.02\n",
            "iteration: 26760 loss: 0.0029 lr: 0.02\n",
            "iteration: 26770 loss: 0.0034 lr: 0.02\n",
            "iteration: 26780 loss: 0.0037 lr: 0.02\n",
            "iteration: 26790 loss: 0.0037 lr: 0.02\n",
            "iteration: 26800 loss: 0.0043 lr: 0.02\n",
            "iteration: 26810 loss: 0.0044 lr: 0.02\n",
            "iteration: 26820 loss: 0.0038 lr: 0.02\n",
            "iteration: 26830 loss: 0.0027 lr: 0.02\n",
            "iteration: 26840 loss: 0.0032 lr: 0.02\n",
            "iteration: 26850 loss: 0.0038 lr: 0.02\n",
            "iteration: 26860 loss: 0.0033 lr: 0.02\n",
            "iteration: 26870 loss: 0.0036 lr: 0.02\n",
            "iteration: 26880 loss: 0.0033 lr: 0.02\n",
            "iteration: 26890 loss: 0.0030 lr: 0.02\n",
            "iteration: 26900 loss: 0.0025 lr: 0.02\n",
            "iteration: 26910 loss: 0.0045 lr: 0.02\n",
            "iteration: 26920 loss: 0.0042 lr: 0.02\n",
            "iteration: 26930 loss: 0.0033 lr: 0.02\n",
            "iteration: 26940 loss: 0.0038 lr: 0.02\n",
            "iteration: 26950 loss: 0.0036 lr: 0.02\n",
            "iteration: 26960 loss: 0.0033 lr: 0.02\n",
            "iteration: 26970 loss: 0.0038 lr: 0.02\n",
            "iteration: 26980 loss: 0.0026 lr: 0.02\n",
            "iteration: 26990 loss: 0.0034 lr: 0.02\n",
            "iteration: 27000 loss: 0.0041 lr: 0.02\n",
            "iteration: 27010 loss: 0.0035 lr: 0.02\n",
            "iteration: 27020 loss: 0.0045 lr: 0.02\n",
            "iteration: 27030 loss: 0.0040 lr: 0.02\n",
            "iteration: 27040 loss: 0.0033 lr: 0.02\n",
            "iteration: 27050 loss: 0.0031 lr: 0.02\n",
            "iteration: 27060 loss: 0.0032 lr: 0.02\n",
            "iteration: 27070 loss: 0.0038 lr: 0.02\n",
            "iteration: 27080 loss: 0.0034 lr: 0.02\n",
            "iteration: 27090 loss: 0.0036 lr: 0.02\n",
            "iteration: 27100 loss: 0.0035 lr: 0.02\n",
            "iteration: 27110 loss: 0.0040 lr: 0.02\n",
            "iteration: 27120 loss: 0.0038 lr: 0.02\n",
            "iteration: 27130 loss: 0.0032 lr: 0.02\n",
            "iteration: 27140 loss: 0.0033 lr: 0.02\n",
            "iteration: 27150 loss: 0.0040 lr: 0.02\n",
            "iteration: 27160 loss: 0.0028 lr: 0.02\n",
            "iteration: 27170 loss: 0.0032 lr: 0.02\n",
            "iteration: 27180 loss: 0.0031 lr: 0.02\n",
            "iteration: 27190 loss: 0.0033 lr: 0.02\n",
            "iteration: 27200 loss: 0.0044 lr: 0.02\n",
            "iteration: 27210 loss: 0.0030 lr: 0.02\n",
            "iteration: 27220 loss: 0.0033 lr: 0.02\n",
            "iteration: 27230 loss: 0.0025 lr: 0.02\n",
            "iteration: 27240 loss: 0.0032 lr: 0.02\n",
            "iteration: 27250 loss: 0.0033 lr: 0.02\n",
            "iteration: 27260 loss: 0.0033 lr: 0.02\n",
            "iteration: 27270 loss: 0.0049 lr: 0.02\n",
            "iteration: 27280 loss: 0.0034 lr: 0.02\n",
            "iteration: 27290 loss: 0.0037 lr: 0.02\n",
            "iteration: 27300 loss: 0.0046 lr: 0.02\n",
            "iteration: 27310 loss: 0.0039 lr: 0.02\n",
            "iteration: 27320 loss: 0.0042 lr: 0.02\n",
            "iteration: 27330 loss: 0.0032 lr: 0.02\n",
            "iteration: 27340 loss: 0.0023 lr: 0.02\n",
            "iteration: 27350 loss: 0.0045 lr: 0.02\n",
            "iteration: 27360 loss: 0.0041 lr: 0.02\n",
            "iteration: 27370 loss: 0.0031 lr: 0.02\n",
            "iteration: 27380 loss: 0.0032 lr: 0.02\n",
            "iteration: 27390 loss: 0.0040 lr: 0.02\n",
            "iteration: 27400 loss: 0.0028 lr: 0.02\n",
            "iteration: 27410 loss: 0.0028 lr: 0.02\n",
            "iteration: 27420 loss: 0.0037 lr: 0.02\n",
            "iteration: 27430 loss: 0.0032 lr: 0.02\n",
            "iteration: 27440 loss: 0.0040 lr: 0.02\n",
            "iteration: 27450 loss: 0.0033 lr: 0.02\n",
            "iteration: 27460 loss: 0.0029 lr: 0.02\n",
            "iteration: 27470 loss: 0.0032 lr: 0.02\n",
            "iteration: 27480 loss: 0.0045 lr: 0.02\n",
            "iteration: 27490 loss: 0.0033 lr: 0.02\n",
            "iteration: 27500 loss: 0.0031 lr: 0.02\n",
            "iteration: 27510 loss: 0.0040 lr: 0.02\n",
            "iteration: 27520 loss: 0.0036 lr: 0.02\n",
            "iteration: 27530 loss: 0.0037 lr: 0.02\n",
            "iteration: 27540 loss: 0.0037 lr: 0.02\n",
            "iteration: 27550 loss: 0.0037 lr: 0.02\n",
            "iteration: 27560 loss: 0.0033 lr: 0.02\n",
            "iteration: 27570 loss: 0.0032 lr: 0.02\n",
            "iteration: 27580 loss: 0.0031 lr: 0.02\n",
            "iteration: 27590 loss: 0.0041 lr: 0.02\n",
            "iteration: 27600 loss: 0.0034 lr: 0.02\n",
            "iteration: 27610 loss: 0.0037 lr: 0.02\n",
            "iteration: 27620 loss: 0.0030 lr: 0.02\n",
            "iteration: 27630 loss: 0.0034 lr: 0.02\n",
            "iteration: 27640 loss: 0.0034 lr: 0.02\n",
            "iteration: 27650 loss: 0.0045 lr: 0.02\n",
            "iteration: 27660 loss: 0.0038 lr: 0.02\n",
            "iteration: 27670 loss: 0.0040 lr: 0.02\n",
            "iteration: 27680 loss: 0.0035 lr: 0.02\n",
            "iteration: 27690 loss: 0.0045 lr: 0.02\n",
            "iteration: 27700 loss: 0.0034 lr: 0.02\n",
            "iteration: 27710 loss: 0.0044 lr: 0.02\n",
            "iteration: 27720 loss: 0.0032 lr: 0.02\n",
            "iteration: 27730 loss: 0.0045 lr: 0.02\n",
            "iteration: 27740 loss: 0.0049 lr: 0.02\n",
            "iteration: 27750 loss: 0.0043 lr: 0.02\n",
            "iteration: 27760 loss: 0.0042 lr: 0.02\n",
            "iteration: 27770 loss: 0.0031 lr: 0.02\n",
            "iteration: 27780 loss: 0.0032 lr: 0.02\n",
            "iteration: 27790 loss: 0.0027 lr: 0.02\n",
            "iteration: 27800 loss: 0.0037 lr: 0.02\n",
            "iteration: 27810 loss: 0.0041 lr: 0.02\n",
            "iteration: 27820 loss: 0.0034 lr: 0.02\n",
            "iteration: 27830 loss: 0.0031 lr: 0.02\n",
            "iteration: 27840 loss: 0.0033 lr: 0.02\n",
            "iteration: 27850 loss: 0.0039 lr: 0.02\n",
            "iteration: 27860 loss: 0.0033 lr: 0.02\n",
            "iteration: 27870 loss: 0.0036 lr: 0.02\n",
            "iteration: 27880 loss: 0.0033 lr: 0.02\n",
            "iteration: 27890 loss: 0.0033 lr: 0.02\n",
            "iteration: 27900 loss: 0.0042 lr: 0.02\n",
            "iteration: 27910 loss: 0.0032 lr: 0.02\n",
            "iteration: 27920 loss: 0.0025 lr: 0.02\n",
            "iteration: 27930 loss: 0.0031 lr: 0.02\n",
            "iteration: 27940 loss: 0.0024 lr: 0.02\n",
            "iteration: 27950 loss: 0.0031 lr: 0.02\n",
            "iteration: 27960 loss: 0.0037 lr: 0.02\n",
            "iteration: 27970 loss: 0.0028 lr: 0.02\n",
            "iteration: 27980 loss: 0.0041 lr: 0.02\n",
            "iteration: 27990 loss: 0.0030 lr: 0.02\n",
            "iteration: 28000 loss: 0.0032 lr: 0.02\n",
            "iteration: 28010 loss: 0.0028 lr: 0.02\n",
            "iteration: 28020 loss: 0.0030 lr: 0.02\n",
            "iteration: 28030 loss: 0.0023 lr: 0.02\n",
            "iteration: 28040 loss: 0.0038 lr: 0.02\n",
            "iteration: 28050 loss: 0.0035 lr: 0.02\n",
            "iteration: 28060 loss: 0.0045 lr: 0.02\n",
            "iteration: 28070 loss: 0.0031 lr: 0.02\n",
            "iteration: 28080 loss: 0.0026 lr: 0.02\n",
            "iteration: 28090 loss: 0.0033 lr: 0.02\n",
            "iteration: 28100 loss: 0.0029 lr: 0.02\n",
            "iteration: 28110 loss: 0.0031 lr: 0.02\n",
            "iteration: 28120 loss: 0.0034 lr: 0.02\n",
            "iteration: 28130 loss: 0.0033 lr: 0.02\n",
            "iteration: 28140 loss: 0.0028 lr: 0.02\n",
            "iteration: 28150 loss: 0.0037 lr: 0.02\n",
            "iteration: 28160 loss: 0.0032 lr: 0.02\n",
            "iteration: 28170 loss: 0.0046 lr: 0.02\n",
            "iteration: 28180 loss: 0.0028 lr: 0.02\n",
            "iteration: 28190 loss: 0.0042 lr: 0.02\n",
            "iteration: 28200 loss: 0.0042 lr: 0.02\n",
            "iteration: 28210 loss: 0.0045 lr: 0.02\n",
            "iteration: 28220 loss: 0.0028 lr: 0.02\n",
            "iteration: 28230 loss: 0.0030 lr: 0.02\n",
            "iteration: 28240 loss: 0.0043 lr: 0.02\n",
            "iteration: 28250 loss: 0.0027 lr: 0.02\n",
            "iteration: 28260 loss: 0.0029 lr: 0.02\n",
            "iteration: 28270 loss: 0.0029 lr: 0.02\n",
            "iteration: 28280 loss: 0.0038 lr: 0.02\n",
            "iteration: 28290 loss: 0.0031 lr: 0.02\n",
            "iteration: 28300 loss: 0.0036 lr: 0.02\n",
            "iteration: 28310 loss: 0.0042 lr: 0.02\n",
            "iteration: 28320 loss: 0.0049 lr: 0.02\n",
            "iteration: 28330 loss: 0.0031 lr: 0.02\n",
            "iteration: 28340 loss: 0.0033 lr: 0.02\n",
            "iteration: 28350 loss: 0.0030 lr: 0.02\n",
            "iteration: 28360 loss: 0.0032 lr: 0.02\n",
            "iteration: 28370 loss: 0.0026 lr: 0.02\n",
            "iteration: 28380 loss: 0.0027 lr: 0.02\n",
            "iteration: 28390 loss: 0.0028 lr: 0.02\n",
            "iteration: 28400 loss: 0.0027 lr: 0.02\n",
            "iteration: 28410 loss: 0.0031 lr: 0.02\n",
            "iteration: 28420 loss: 0.0032 lr: 0.02\n",
            "iteration: 28430 loss: 0.0047 lr: 0.02\n",
            "iteration: 28440 loss: 0.0031 lr: 0.02\n",
            "iteration: 28450 loss: 0.0045 lr: 0.02\n",
            "iteration: 28460 loss: 0.0032 lr: 0.02\n",
            "iteration: 28470 loss: 0.0031 lr: 0.02\n",
            "iteration: 28480 loss: 0.0032 lr: 0.02\n",
            "iteration: 28490 loss: 0.0028 lr: 0.02\n",
            "iteration: 28500 loss: 0.0031 lr: 0.02\n",
            "iteration: 28510 loss: 0.0039 lr: 0.02\n",
            "iteration: 28520 loss: 0.0032 lr: 0.02\n",
            "iteration: 28530 loss: 0.0037 lr: 0.02\n",
            "iteration: 28540 loss: 0.0032 lr: 0.02\n",
            "iteration: 28550 loss: 0.0036 lr: 0.02\n",
            "iteration: 28560 loss: 0.0055 lr: 0.02\n",
            "iteration: 28570 loss: 0.0057 lr: 0.02\n",
            "iteration: 28580 loss: 0.0036 lr: 0.02\n",
            "iteration: 28590 loss: 0.0035 lr: 0.02\n",
            "iteration: 28600 loss: 0.0037 lr: 0.02\n",
            "iteration: 28610 loss: 0.0032 lr: 0.02\n",
            "iteration: 28620 loss: 0.0038 lr: 0.02\n",
            "iteration: 28630 loss: 0.0031 lr: 0.02\n",
            "iteration: 28640 loss: 0.0025 lr: 0.02\n",
            "iteration: 28650 loss: 0.0030 lr: 0.02\n",
            "iteration: 28660 loss: 0.0028 lr: 0.02\n",
            "iteration: 28670 loss: 0.0024 lr: 0.02\n",
            "iteration: 28680 loss: 0.0027 lr: 0.02\n",
            "iteration: 28690 loss: 0.0042 lr: 0.02\n",
            "iteration: 28700 loss: 0.0031 lr: 0.02\n",
            "iteration: 28710 loss: 0.0031 lr: 0.02\n",
            "iteration: 28720 loss: 0.0041 lr: 0.02\n",
            "iteration: 28730 loss: 0.0025 lr: 0.02\n",
            "iteration: 28740 loss: 0.0034 lr: 0.02\n",
            "iteration: 28750 loss: 0.0033 lr: 0.02\n",
            "iteration: 28760 loss: 0.0029 lr: 0.02\n",
            "iteration: 28770 loss: 0.0034 lr: 0.02\n",
            "iteration: 28780 loss: 0.0034 lr: 0.02\n",
            "iteration: 28790 loss: 0.0036 lr: 0.02\n",
            "iteration: 28800 loss: 0.0040 lr: 0.02\n",
            "iteration: 28810 loss: 0.0040 lr: 0.02\n",
            "iteration: 28820 loss: 0.0035 lr: 0.02\n",
            "iteration: 28830 loss: 0.0034 lr: 0.02\n",
            "iteration: 28840 loss: 0.0028 lr: 0.02\n",
            "iteration: 28850 loss: 0.0033 lr: 0.02\n",
            "iteration: 28860 loss: 0.0026 lr: 0.02\n",
            "iteration: 28870 loss: 0.0028 lr: 0.02\n",
            "iteration: 28880 loss: 0.0033 lr: 0.02\n",
            "iteration: 28890 loss: 0.0039 lr: 0.02\n",
            "iteration: 28900 loss: 0.0034 lr: 0.02\n",
            "iteration: 28910 loss: 0.0031 lr: 0.02\n",
            "iteration: 28920 loss: 0.0023 lr: 0.02\n",
            "iteration: 28930 loss: 0.0025 lr: 0.02\n",
            "iteration: 28940 loss: 0.0028 lr: 0.02\n",
            "iteration: 28950 loss: 0.0043 lr: 0.02\n",
            "iteration: 28960 loss: 0.0064 lr: 0.02\n",
            "iteration: 28970 loss: 0.0037 lr: 0.02\n",
            "iteration: 28980 loss: 0.0027 lr: 0.02\n",
            "iteration: 28990 loss: 0.0025 lr: 0.02\n",
            "iteration: 29000 loss: 0.0038 lr: 0.02\n",
            "iteration: 29010 loss: 0.0034 lr: 0.02\n",
            "iteration: 29020 loss: 0.0030 lr: 0.02\n",
            "iteration: 29030 loss: 0.0036 lr: 0.02\n",
            "iteration: 29040 loss: 0.0027 lr: 0.02\n",
            "iteration: 29050 loss: 0.0033 lr: 0.02\n",
            "iteration: 29060 loss: 0.0034 lr: 0.02\n",
            "iteration: 29070 loss: 0.0037 lr: 0.02\n",
            "iteration: 29080 loss: 0.0028 lr: 0.02\n",
            "iteration: 29090 loss: 0.0036 lr: 0.02\n",
            "iteration: 29100 loss: 0.0038 lr: 0.02\n",
            "iteration: 29110 loss: 0.0027 lr: 0.02\n",
            "iteration: 29120 loss: 0.0031 lr: 0.02\n",
            "iteration: 29130 loss: 0.0038 lr: 0.02\n",
            "iteration: 29140 loss: 0.0038 lr: 0.02\n",
            "iteration: 29150 loss: 0.0026 lr: 0.02\n",
            "iteration: 29160 loss: 0.0029 lr: 0.02\n",
            "iteration: 29170 loss: 0.0029 lr: 0.02\n",
            "iteration: 29180 loss: 0.0029 lr: 0.02\n",
            "iteration: 29190 loss: 0.0045 lr: 0.02\n",
            "iteration: 29200 loss: 0.0027 lr: 0.02\n",
            "iteration: 29210 loss: 0.0030 lr: 0.02\n",
            "iteration: 29220 loss: 0.0028 lr: 0.02\n",
            "iteration: 29230 loss: 0.0038 lr: 0.02\n",
            "iteration: 29240 loss: 0.0028 lr: 0.02\n",
            "iteration: 29250 loss: 0.0025 lr: 0.02\n",
            "iteration: 29260 loss: 0.0036 lr: 0.02\n",
            "iteration: 29270 loss: 0.0033 lr: 0.02\n",
            "iteration: 29280 loss: 0.0031 lr: 0.02\n",
            "iteration: 29290 loss: 0.0031 lr: 0.02\n",
            "iteration: 29300 loss: 0.0035 lr: 0.02\n",
            "iteration: 29310 loss: 0.0034 lr: 0.02\n",
            "iteration: 29320 loss: 0.0043 lr: 0.02\n",
            "iteration: 29330 loss: 0.0038 lr: 0.02\n",
            "iteration: 29340 loss: 0.0034 lr: 0.02\n",
            "iteration: 29350 loss: 0.0028 lr: 0.02\n",
            "iteration: 29360 loss: 0.0026 lr: 0.02\n",
            "iteration: 29370 loss: 0.0045 lr: 0.02\n",
            "iteration: 29380 loss: 0.0032 lr: 0.02\n",
            "iteration: 29390 loss: 0.0040 lr: 0.02\n",
            "iteration: 29400 loss: 0.0031 lr: 0.02\n",
            "iteration: 29410 loss: 0.0037 lr: 0.02\n",
            "iteration: 29420 loss: 0.0027 lr: 0.02\n",
            "iteration: 29430 loss: 0.0034 lr: 0.02\n",
            "iteration: 29440 loss: 0.0028 lr: 0.02\n",
            "iteration: 29450 loss: 0.0027 lr: 0.02\n",
            "iteration: 29460 loss: 0.0026 lr: 0.02\n",
            "iteration: 29470 loss: 0.0030 lr: 0.02\n",
            "iteration: 29480 loss: 0.0030 lr: 0.02\n",
            "iteration: 29490 loss: 0.0034 lr: 0.02\n",
            "iteration: 29500 loss: 0.0030 lr: 0.02\n",
            "iteration: 29510 loss: 0.0036 lr: 0.02\n",
            "iteration: 29520 loss: 0.0032 lr: 0.02\n",
            "iteration: 29530 loss: 0.0030 lr: 0.02\n",
            "iteration: 29540 loss: 0.0034 lr: 0.02\n",
            "iteration: 29550 loss: 0.0032 lr: 0.02\n",
            "iteration: 29560 loss: 0.0027 lr: 0.02\n",
            "iteration: 29570 loss: 0.0043 lr: 0.02\n",
            "iteration: 29580 loss: 0.0039 lr: 0.02\n",
            "iteration: 29590 loss: 0.0026 lr: 0.02\n",
            "iteration: 29600 loss: 0.0028 lr: 0.02\n",
            "iteration: 29610 loss: 0.0027 lr: 0.02\n",
            "iteration: 29620 loss: 0.0031 lr: 0.02\n",
            "iteration: 29630 loss: 0.0026 lr: 0.02\n",
            "iteration: 29640 loss: 0.0027 lr: 0.02\n",
            "iteration: 29650 loss: 0.0025 lr: 0.02\n",
            "iteration: 29660 loss: 0.0031 lr: 0.02\n",
            "iteration: 29670 loss: 0.0042 lr: 0.02\n",
            "iteration: 29680 loss: 0.0029 lr: 0.02\n",
            "iteration: 29690 loss: 0.0029 lr: 0.02\n",
            "iteration: 29700 loss: 0.0037 lr: 0.02\n",
            "iteration: 29710 loss: 0.0030 lr: 0.02\n",
            "iteration: 29720 loss: 0.0025 lr: 0.02\n",
            "iteration: 29730 loss: 0.0029 lr: 0.02\n",
            "iteration: 29740 loss: 0.0022 lr: 0.02\n",
            "iteration: 29750 loss: 0.0032 lr: 0.02\n",
            "iteration: 29760 loss: 0.0040 lr: 0.02\n",
            "iteration: 29770 loss: 0.0044 lr: 0.02\n",
            "iteration: 29780 loss: 0.0026 lr: 0.02\n",
            "iteration: 29790 loss: 0.0031 lr: 0.02\n",
            "iteration: 29800 loss: 0.0030 lr: 0.02\n",
            "iteration: 29810 loss: 0.0034 lr: 0.02\n",
            "iteration: 29820 loss: 0.0037 lr: 0.02\n",
            "iteration: 29830 loss: 0.0029 lr: 0.02\n",
            "iteration: 29840 loss: 0.0029 lr: 0.02\n",
            "iteration: 29850 loss: 0.0027 lr: 0.02\n",
            "iteration: 29860 loss: 0.0027 lr: 0.02\n",
            "iteration: 29870 loss: 0.0026 lr: 0.02\n",
            "iteration: 29880 loss: 0.0026 lr: 0.02\n",
            "iteration: 29890 loss: 0.0027 lr: 0.02\n",
            "iteration: 29900 loss: 0.0032 lr: 0.02\n",
            "iteration: 29910 loss: 0.0028 lr: 0.02\n",
            "iteration: 29920 loss: 0.0028 lr: 0.02\n",
            "iteration: 29930 loss: 0.0032 lr: 0.02\n",
            "iteration: 29940 loss: 0.0026 lr: 0.02\n",
            "iteration: 29950 loss: 0.0033 lr: 0.02\n",
            "iteration: 29960 loss: 0.0025 lr: 0.02\n",
            "iteration: 29970 loss: 0.0025 lr: 0.02\n",
            "iteration: 29980 loss: 0.0035 lr: 0.02\n",
            "iteration: 29990 loss: 0.0033 lr: 0.02\n",
            "iteration: 30000 loss: 0.0040 lr: 0.02\n",
            "iteration: 30010 loss: 0.0033 lr: 0.02\n",
            "iteration: 30020 loss: 0.0031 lr: 0.02\n",
            "iteration: 30030 loss: 0.0030 lr: 0.02\n",
            "iteration: 30040 loss: 0.0041 lr: 0.02\n",
            "iteration: 30050 loss: 0.0028 lr: 0.02\n",
            "iteration: 30060 loss: 0.0031 lr: 0.02\n",
            "iteration: 30070 loss: 0.0028 lr: 0.02\n",
            "iteration: 30080 loss: 0.0032 lr: 0.02\n",
            "iteration: 30090 loss: 0.0024 lr: 0.02\n",
            "iteration: 30100 loss: 0.0039 lr: 0.02\n",
            "iteration: 30110 loss: 0.0038 lr: 0.02\n",
            "iteration: 30120 loss: 0.0034 lr: 0.02\n",
            "iteration: 30130 loss: 0.0033 lr: 0.02\n",
            "iteration: 30140 loss: 0.0037 lr: 0.02\n",
            "iteration: 30150 loss: 0.0041 lr: 0.02\n",
            "iteration: 30160 loss: 0.0037 lr: 0.02\n",
            "iteration: 30170 loss: 0.0036 lr: 0.02\n",
            "iteration: 30180 loss: 0.0022 lr: 0.02\n",
            "iteration: 30190 loss: 0.0051 lr: 0.02\n",
            "iteration: 30200 loss: 0.0028 lr: 0.02\n",
            "iteration: 30210 loss: 0.0029 lr: 0.02\n",
            "iteration: 30220 loss: 0.0035 lr: 0.02\n",
            "iteration: 30230 loss: 0.0038 lr: 0.02\n",
            "iteration: 30240 loss: 0.0032 lr: 0.02\n",
            "iteration: 30250 loss: 0.0049 lr: 0.02\n",
            "iteration: 30260 loss: 0.0030 lr: 0.02\n",
            "iteration: 30270 loss: 0.0038 lr: 0.02\n",
            "iteration: 30280 loss: 0.0034 lr: 0.02\n",
            "iteration: 30290 loss: 0.0036 lr: 0.02\n",
            "iteration: 30300 loss: 0.0034 lr: 0.02\n",
            "iteration: 30310 loss: 0.0029 lr: 0.02\n",
            "iteration: 30320 loss: 0.0023 lr: 0.02\n",
            "iteration: 30330 loss: 0.0033 lr: 0.02\n",
            "iteration: 30340 loss: 0.0035 lr: 0.02\n",
            "iteration: 30350 loss: 0.0027 lr: 0.02\n",
            "iteration: 30360 loss: 0.0026 lr: 0.02\n",
            "iteration: 30370 loss: 0.0028 lr: 0.02\n",
            "iteration: 30380 loss: 0.0031 lr: 0.02\n",
            "iteration: 30390 loss: 0.0029 lr: 0.02\n",
            "iteration: 30400 loss: 0.0035 lr: 0.02\n",
            "iteration: 30410 loss: 0.0041 lr: 0.02\n",
            "iteration: 30420 loss: 0.0028 lr: 0.02\n",
            "iteration: 30430 loss: 0.0030 lr: 0.02\n",
            "iteration: 30440 loss: 0.0029 lr: 0.02\n",
            "iteration: 30450 loss: 0.0036 lr: 0.02\n",
            "iteration: 30460 loss: 0.0027 lr: 0.02\n",
            "iteration: 30470 loss: 0.0026 lr: 0.02\n",
            "iteration: 30480 loss: 0.0033 lr: 0.02\n",
            "iteration: 30490 loss: 0.0034 lr: 0.02\n",
            "iteration: 30500 loss: 0.0029 lr: 0.02\n",
            "iteration: 30510 loss: 0.0022 lr: 0.02\n",
            "iteration: 30520 loss: 0.0034 lr: 0.02\n",
            "iteration: 30530 loss: 0.0034 lr: 0.02\n",
            "iteration: 30540 loss: 0.0029 lr: 0.02\n",
            "iteration: 30550 loss: 0.0045 lr: 0.02\n",
            "iteration: 30560 loss: 0.0037 lr: 0.02\n",
            "iteration: 30570 loss: 0.0033 lr: 0.02\n",
            "iteration: 30580 loss: 0.0036 lr: 0.02\n",
            "iteration: 30590 loss: 0.0035 lr: 0.02\n",
            "iteration: 30600 loss: 0.0033 lr: 0.02\n",
            "iteration: 30610 loss: 0.0030 lr: 0.02\n",
            "iteration: 30620 loss: 0.0029 lr: 0.02\n",
            "iteration: 30630 loss: 0.0035 lr: 0.02\n",
            "iteration: 30640 loss: 0.0027 lr: 0.02\n",
            "iteration: 30650 loss: 0.0033 lr: 0.02\n",
            "iteration: 30660 loss: 0.0029 lr: 0.02\n",
            "iteration: 30670 loss: 0.0034 lr: 0.02\n",
            "iteration: 30680 loss: 0.0026 lr: 0.02\n",
            "iteration: 30690 loss: 0.0028 lr: 0.02\n",
            "iteration: 30700 loss: 0.0033 lr: 0.02\n",
            "iteration: 30710 loss: 0.0037 lr: 0.02\n",
            "iteration: 30720 loss: 0.0035 lr: 0.02\n",
            "iteration: 30730 loss: 0.0030 lr: 0.02\n",
            "iteration: 30740 loss: 0.0034 lr: 0.02\n",
            "iteration: 30750 loss: 0.0033 lr: 0.02\n",
            "iteration: 30760 loss: 0.0034 lr: 0.02\n",
            "iteration: 30770 loss: 0.0030 lr: 0.02\n",
            "iteration: 30780 loss: 0.0028 lr: 0.02\n",
            "iteration: 30790 loss: 0.0050 lr: 0.02\n",
            "iteration: 30800 loss: 0.0039 lr: 0.02\n",
            "iteration: 30810 loss: 0.0028 lr: 0.02\n",
            "iteration: 30820 loss: 0.0026 lr: 0.02\n",
            "iteration: 30830 loss: 0.0032 lr: 0.02\n",
            "iteration: 30840 loss: 0.0036 lr: 0.02\n",
            "iteration: 30850 loss: 0.0023 lr: 0.02\n",
            "iteration: 30860 loss: 0.0028 lr: 0.02\n",
            "iteration: 30870 loss: 0.0030 lr: 0.02\n",
            "iteration: 30880 loss: 0.0027 lr: 0.02\n",
            "iteration: 30890 loss: 0.0029 lr: 0.02\n",
            "iteration: 30900 loss: 0.0035 lr: 0.02\n",
            "iteration: 30910 loss: 0.0030 lr: 0.02\n",
            "iteration: 30920 loss: 0.0031 lr: 0.02\n",
            "iteration: 30930 loss: 0.0034 lr: 0.02\n",
            "iteration: 30940 loss: 0.0032 lr: 0.02\n",
            "iteration: 30950 loss: 0.0029 lr: 0.02\n",
            "iteration: 30960 loss: 0.0043 lr: 0.02\n",
            "iteration: 30970 loss: 0.0035 lr: 0.02\n",
            "iteration: 30980 loss: 0.0030 lr: 0.02\n",
            "iteration: 30990 loss: 0.0028 lr: 0.02\n",
            "iteration: 31000 loss: 0.0032 lr: 0.02\n",
            "iteration: 31010 loss: 0.0031 lr: 0.02\n",
            "iteration: 31020 loss: 0.0025 lr: 0.02\n",
            "iteration: 31030 loss: 0.0026 lr: 0.02\n",
            "iteration: 31040 loss: 0.0032 lr: 0.02\n",
            "iteration: 31050 loss: 0.0035 lr: 0.02\n",
            "iteration: 31060 loss: 0.0025 lr: 0.02\n",
            "iteration: 31070 loss: 0.0030 lr: 0.02\n",
            "iteration: 31080 loss: 0.0023 lr: 0.02\n",
            "iteration: 31090 loss: 0.0030 lr: 0.02\n",
            "iteration: 31100 loss: 0.0031 lr: 0.02\n",
            "iteration: 31110 loss: 0.0032 lr: 0.02\n",
            "iteration: 31120 loss: 0.0035 lr: 0.02\n",
            "iteration: 31130 loss: 0.0030 lr: 0.02\n",
            "iteration: 31140 loss: 0.0030 lr: 0.02\n",
            "iteration: 31150 loss: 0.0042 lr: 0.02\n",
            "iteration: 31160 loss: 0.0034 lr: 0.02\n",
            "iteration: 31170 loss: 0.0038 lr: 0.02\n",
            "iteration: 31180 loss: 0.0030 lr: 0.02\n",
            "iteration: 31190 loss: 0.0030 lr: 0.02\n",
            "iteration: 31200 loss: 0.0029 lr: 0.02\n",
            "iteration: 31210 loss: 0.0032 lr: 0.02\n",
            "iteration: 31220 loss: 0.0033 lr: 0.02\n",
            "iteration: 31230 loss: 0.0027 lr: 0.02\n",
            "iteration: 31240 loss: 0.0027 lr: 0.02\n",
            "iteration: 31250 loss: 0.0043 lr: 0.02\n",
            "iteration: 31260 loss: 0.0030 lr: 0.02\n",
            "iteration: 31270 loss: 0.0028 lr: 0.02\n",
            "iteration: 31280 loss: 0.0028 lr: 0.02\n",
            "iteration: 31290 loss: 0.0031 lr: 0.02\n",
            "iteration: 31300 loss: 0.0041 lr: 0.02\n",
            "iteration: 31310 loss: 0.0025 lr: 0.02\n",
            "iteration: 31320 loss: 0.0030 lr: 0.02\n",
            "iteration: 31330 loss: 0.0027 lr: 0.02\n",
            "iteration: 31340 loss: 0.0028 lr: 0.02\n",
            "iteration: 31350 loss: 0.0028 lr: 0.02\n",
            "iteration: 31360 loss: 0.0029 lr: 0.02\n",
            "iteration: 31370 loss: 0.0024 lr: 0.02\n",
            "iteration: 31380 loss: 0.0042 lr: 0.02\n",
            "iteration: 31390 loss: 0.0026 lr: 0.02\n",
            "iteration: 31400 loss: 0.0025 lr: 0.02\n",
            "iteration: 31410 loss: 0.0031 lr: 0.02\n",
            "iteration: 31420 loss: 0.0030 lr: 0.02\n",
            "iteration: 31430 loss: 0.0035 lr: 0.02\n",
            "iteration: 31440 loss: 0.0029 lr: 0.02\n",
            "iteration: 31450 loss: 0.0034 lr: 0.02\n",
            "iteration: 31460 loss: 0.0032 lr: 0.02\n",
            "iteration: 31470 loss: 0.0030 lr: 0.02\n",
            "iteration: 31480 loss: 0.0033 lr: 0.02\n",
            "iteration: 31490 loss: 0.0039 lr: 0.02\n",
            "iteration: 31500 loss: 0.0033 lr: 0.02\n",
            "iteration: 31510 loss: 0.0035 lr: 0.02\n",
            "iteration: 31520 loss: 0.0026 lr: 0.02\n",
            "iteration: 31530 loss: 0.0035 lr: 0.02\n",
            "iteration: 31540 loss: 0.0029 lr: 0.02\n",
            "iteration: 31550 loss: 0.0031 lr: 0.02\n",
            "iteration: 31560 loss: 0.0029 lr: 0.02\n",
            "iteration: 31570 loss: 0.0033 lr: 0.02\n",
            "iteration: 31580 loss: 0.0025 lr: 0.02\n",
            "iteration: 31590 loss: 0.0031 lr: 0.02\n",
            "iteration: 31600 loss: 0.0037 lr: 0.02\n",
            "iteration: 31610 loss: 0.0031 lr: 0.02\n",
            "iteration: 31620 loss: 0.0025 lr: 0.02\n",
            "iteration: 31630 loss: 0.0034 lr: 0.02\n",
            "iteration: 31640 loss: 0.0032 lr: 0.02\n",
            "iteration: 31650 loss: 0.0018 lr: 0.02\n",
            "iteration: 31660 loss: 0.0028 lr: 0.02\n",
            "iteration: 31670 loss: 0.0027 lr: 0.02\n",
            "iteration: 31680 loss: 0.0025 lr: 0.02\n",
            "iteration: 31690 loss: 0.0032 lr: 0.02\n",
            "iteration: 31700 loss: 0.0022 lr: 0.02\n",
            "iteration: 31710 loss: 0.0030 lr: 0.02\n",
            "iteration: 31720 loss: 0.0029 lr: 0.02\n",
            "iteration: 31730 loss: 0.0025 lr: 0.02\n",
            "iteration: 31740 loss: 0.0029 lr: 0.02\n",
            "iteration: 31750 loss: 0.0028 lr: 0.02\n",
            "iteration: 31760 loss: 0.0033 lr: 0.02\n",
            "iteration: 31770 loss: 0.0029 lr: 0.02\n",
            "iteration: 31780 loss: 0.0031 lr: 0.02\n",
            "iteration: 31790 loss: 0.0035 lr: 0.02\n",
            "iteration: 31800 loss: 0.0034 lr: 0.02\n",
            "iteration: 31810 loss: 0.0032 lr: 0.02\n",
            "iteration: 31820 loss: 0.0039 lr: 0.02\n",
            "iteration: 31830 loss: 0.0024 lr: 0.02\n",
            "iteration: 31840 loss: 0.0026 lr: 0.02\n",
            "iteration: 31850 loss: 0.0028 lr: 0.02\n",
            "iteration: 31860 loss: 0.0024 lr: 0.02\n",
            "iteration: 31870 loss: 0.0033 lr: 0.02\n",
            "iteration: 31880 loss: 0.0030 lr: 0.02\n",
            "iteration: 31890 loss: 0.0051 lr: 0.02\n",
            "iteration: 31900 loss: 0.0033 lr: 0.02\n",
            "iteration: 31910 loss: 0.0030 lr: 0.02\n",
            "iteration: 31920 loss: 0.0038 lr: 0.02\n",
            "iteration: 31930 loss: 0.0026 lr: 0.02\n",
            "iteration: 31940 loss: 0.0042 lr: 0.02\n",
            "iteration: 31950 loss: 0.0030 lr: 0.02\n",
            "iteration: 31960 loss: 0.0037 lr: 0.02\n",
            "iteration: 31970 loss: 0.0030 lr: 0.02\n",
            "iteration: 31980 loss: 0.0054 lr: 0.02\n",
            "iteration: 31990 loss: 0.0032 lr: 0.02\n",
            "iteration: 32000 loss: 0.0028 lr: 0.02\n",
            "iteration: 32010 loss: 0.0037 lr: 0.02\n",
            "iteration: 32020 loss: 0.0027 lr: 0.02\n",
            "iteration: 32030 loss: 0.0024 lr: 0.02\n",
            "iteration: 32040 loss: 0.0028 lr: 0.02\n",
            "iteration: 32050 loss: 0.0029 lr: 0.02\n",
            "iteration: 32060 loss: 0.0026 lr: 0.02\n",
            "iteration: 32070 loss: 0.0031 lr: 0.02\n",
            "iteration: 32080 loss: 0.0029 lr: 0.02\n",
            "iteration: 32090 loss: 0.0027 lr: 0.02\n",
            "iteration: 32100 loss: 0.0030 lr: 0.02\n",
            "iteration: 32110 loss: 0.0025 lr: 0.02\n",
            "iteration: 32120 loss: 0.0030 lr: 0.02\n",
            "iteration: 32130 loss: 0.0038 lr: 0.02\n",
            "iteration: 32140 loss: 0.0023 lr: 0.02\n",
            "iteration: 32150 loss: 0.0028 lr: 0.02\n",
            "iteration: 32160 loss: 0.0024 lr: 0.02\n",
            "iteration: 32170 loss: 0.0024 lr: 0.02\n",
            "iteration: 32180 loss: 0.0029 lr: 0.02\n",
            "iteration: 32190 loss: 0.0028 lr: 0.02\n",
            "iteration: 32200 loss: 0.0026 lr: 0.02\n",
            "iteration: 32210 loss: 0.0031 lr: 0.02\n",
            "iteration: 32220 loss: 0.0032 lr: 0.02\n",
            "iteration: 32230 loss: 0.0029 lr: 0.02\n",
            "iteration: 32240 loss: 0.0027 lr: 0.02\n",
            "iteration: 32250 loss: 0.0031 lr: 0.02\n",
            "iteration: 32260 loss: 0.0024 lr: 0.02\n",
            "iteration: 32270 loss: 0.0031 lr: 0.02\n",
            "iteration: 32280 loss: 0.0026 lr: 0.02\n",
            "iteration: 32290 loss: 0.0033 lr: 0.02\n",
            "iteration: 32300 loss: 0.0034 lr: 0.02\n",
            "iteration: 32310 loss: 0.0037 lr: 0.02\n",
            "iteration: 32320 loss: 0.0032 lr: 0.02\n",
            "iteration: 32330 loss: 0.0025 lr: 0.02\n",
            "iteration: 32340 loss: 0.0031 lr: 0.02\n",
            "iteration: 32350 loss: 0.0030 lr: 0.02\n",
            "iteration: 32360 loss: 0.0034 lr: 0.02\n",
            "iteration: 32370 loss: 0.0028 lr: 0.02\n",
            "iteration: 32380 loss: 0.0033 lr: 0.02\n",
            "iteration: 32390 loss: 0.0033 lr: 0.02\n",
            "iteration: 32400 loss: 0.0034 lr: 0.02\n",
            "iteration: 32410 loss: 0.0033 lr: 0.02\n",
            "iteration: 32420 loss: 0.0032 lr: 0.02\n",
            "iteration: 32430 loss: 0.0029 lr: 0.02\n",
            "iteration: 32440 loss: 0.0032 lr: 0.02\n",
            "iteration: 32450 loss: 0.0029 lr: 0.02\n",
            "iteration: 32460 loss: 0.0042 lr: 0.02\n",
            "iteration: 32470 loss: 0.0026 lr: 0.02\n",
            "iteration: 32480 loss: 0.0032 lr: 0.02\n",
            "iteration: 32490 loss: 0.0025 lr: 0.02\n",
            "iteration: 32500 loss: 0.0026 lr: 0.02\n",
            "iteration: 32510 loss: 0.0033 lr: 0.02\n",
            "iteration: 32520 loss: 0.0036 lr: 0.02\n",
            "iteration: 32530 loss: 0.0027 lr: 0.02\n",
            "iteration: 32540 loss: 0.0027 lr: 0.02\n",
            "iteration: 32550 loss: 0.0029 lr: 0.02\n",
            "iteration: 32560 loss: 0.0027 lr: 0.02\n",
            "iteration: 32570 loss: 0.0036 lr: 0.02\n",
            "iteration: 32580 loss: 0.0028 lr: 0.02\n",
            "iteration: 32590 loss: 0.0030 lr: 0.02\n",
            "iteration: 32600 loss: 0.0053 lr: 0.02\n",
            "iteration: 32610 loss: 0.0034 lr: 0.02\n",
            "iteration: 32620 loss: 0.0035 lr: 0.02\n",
            "iteration: 32630 loss: 0.0054 lr: 0.02\n",
            "iteration: 32640 loss: 0.0044 lr: 0.02\n",
            "iteration: 32650 loss: 0.0030 lr: 0.02\n",
            "iteration: 32660 loss: 0.0021 lr: 0.02\n",
            "iteration: 32670 loss: 0.0028 lr: 0.02\n",
            "iteration: 32680 loss: 0.0028 lr: 0.02\n",
            "iteration: 32690 loss: 0.0032 lr: 0.02\n",
            "iteration: 32700 loss: 0.0034 lr: 0.02\n",
            "iteration: 32710 loss: 0.0026 lr: 0.02\n",
            "iteration: 32720 loss: 0.0028 lr: 0.02\n",
            "iteration: 32730 loss: 0.0029 lr: 0.02\n",
            "iteration: 32740 loss: 0.0035 lr: 0.02\n",
            "iteration: 32750 loss: 0.0029 lr: 0.02\n",
            "iteration: 32760 loss: 0.0027 lr: 0.02\n",
            "iteration: 32770 loss: 0.0030 lr: 0.02\n",
            "iteration: 32780 loss: 0.0037 lr: 0.02\n",
            "iteration: 32790 loss: 0.0037 lr: 0.02\n",
            "iteration: 32800 loss: 0.0025 lr: 0.02\n",
            "iteration: 32810 loss: 0.0028 lr: 0.02\n",
            "iteration: 32820 loss: 0.0037 lr: 0.02\n",
            "iteration: 32830 loss: 0.0037 lr: 0.02\n",
            "iteration: 32840 loss: 0.0031 lr: 0.02\n",
            "iteration: 32850 loss: 0.0034 lr: 0.02\n",
            "iteration: 32860 loss: 0.0036 lr: 0.02\n",
            "iteration: 32870 loss: 0.0033 lr: 0.02\n",
            "iteration: 32880 loss: 0.0031 lr: 0.02\n",
            "iteration: 32890 loss: 0.0031 lr: 0.02\n",
            "iteration: 32900 loss: 0.0023 lr: 0.02\n",
            "iteration: 32910 loss: 0.0033 lr: 0.02\n",
            "iteration: 32920 loss: 0.0026 lr: 0.02\n",
            "iteration: 32930 loss: 0.0025 lr: 0.02\n",
            "iteration: 32940 loss: 0.0035 lr: 0.02\n",
            "iteration: 32950 loss: 0.0027 lr: 0.02\n",
            "iteration: 32960 loss: 0.0030 lr: 0.02\n",
            "iteration: 32970 loss: 0.0022 lr: 0.02\n",
            "iteration: 32980 loss: 0.0048 lr: 0.02\n",
            "iteration: 32990 loss: 0.0025 lr: 0.02\n",
            "iteration: 33000 loss: 0.0032 lr: 0.02\n",
            "iteration: 33010 loss: 0.0032 lr: 0.02\n",
            "iteration: 33020 loss: 0.0027 lr: 0.02\n",
            "iteration: 33030 loss: 0.0024 lr: 0.02\n",
            "iteration: 33040 loss: 0.0025 lr: 0.02\n",
            "iteration: 33050 loss: 0.0031 lr: 0.02\n",
            "iteration: 33060 loss: 0.0030 lr: 0.02\n",
            "iteration: 33070 loss: 0.0034 lr: 0.02\n",
            "iteration: 33080 loss: 0.0023 lr: 0.02\n",
            "iteration: 33090 loss: 0.0023 lr: 0.02\n",
            "iteration: 33100 loss: 0.0033 lr: 0.02\n",
            "iteration: 33110 loss: 0.0029 lr: 0.02\n",
            "iteration: 33120 loss: 0.0028 lr: 0.02\n",
            "iteration: 33130 loss: 0.0027 lr: 0.02\n",
            "iteration: 33140 loss: 0.0026 lr: 0.02\n",
            "iteration: 33150 loss: 0.0047 lr: 0.02\n",
            "iteration: 33160 loss: 0.0022 lr: 0.02\n",
            "iteration: 33170 loss: 0.0025 lr: 0.02\n",
            "iteration: 33180 loss: 0.0029 lr: 0.02\n",
            "iteration: 33190 loss: 0.0026 lr: 0.02\n",
            "iteration: 33200 loss: 0.0036 lr: 0.02\n",
            "iteration: 33210 loss: 0.0027 lr: 0.02\n",
            "iteration: 33220 loss: 0.0024 lr: 0.02\n",
            "iteration: 33230 loss: 0.0024 lr: 0.02\n",
            "iteration: 33240 loss: 0.0032 lr: 0.02\n",
            "iteration: 33250 loss: 0.0030 lr: 0.02\n",
            "iteration: 33260 loss: 0.0032 lr: 0.02\n",
            "iteration: 33270 loss: 0.0033 lr: 0.02\n",
            "iteration: 33280 loss: 0.0028 lr: 0.02\n",
            "iteration: 33290 loss: 0.0030 lr: 0.02\n",
            "iteration: 33300 loss: 0.0028 lr: 0.02\n",
            "iteration: 33310 loss: 0.0022 lr: 0.02\n",
            "iteration: 33320 loss: 0.0026 lr: 0.02\n",
            "iteration: 33330 loss: 0.0028 lr: 0.02\n",
            "iteration: 33340 loss: 0.0028 lr: 0.02\n",
            "iteration: 33350 loss: 0.0022 lr: 0.02\n",
            "iteration: 33360 loss: 0.0032 lr: 0.02\n",
            "iteration: 33370 loss: 0.0028 lr: 0.02\n",
            "iteration: 33380 loss: 0.0035 lr: 0.02\n",
            "iteration: 33390 loss: 0.0025 lr: 0.02\n",
            "iteration: 33400 loss: 0.0030 lr: 0.02\n",
            "iteration: 33410 loss: 0.0029 lr: 0.02\n",
            "iteration: 33420 loss: 0.0027 lr: 0.02\n",
            "iteration: 33430 loss: 0.0025 lr: 0.02\n",
            "iteration: 33440 loss: 0.0029 lr: 0.02\n",
            "iteration: 33450 loss: 0.0032 lr: 0.02\n",
            "iteration: 33460 loss: 0.0028 lr: 0.02\n",
            "iteration: 33470 loss: 0.0026 lr: 0.02\n",
            "iteration: 33480 loss: 0.0024 lr: 0.02\n",
            "iteration: 33490 loss: 0.0029 lr: 0.02\n",
            "iteration: 33500 loss: 0.0025 lr: 0.02\n",
            "iteration: 33510 loss: 0.0022 lr: 0.02\n",
            "iteration: 33520 loss: 0.0027 lr: 0.02\n",
            "iteration: 33530 loss: 0.0028 lr: 0.02\n",
            "iteration: 33540 loss: 0.0033 lr: 0.02\n",
            "iteration: 33550 loss: 0.0031 lr: 0.02\n",
            "iteration: 33560 loss: 0.0025 lr: 0.02\n",
            "iteration: 33570 loss: 0.0044 lr: 0.02\n",
            "iteration: 33580 loss: 0.0026 lr: 0.02\n",
            "iteration: 33590 loss: 0.0022 lr: 0.02\n",
            "iteration: 33600 loss: 0.0030 lr: 0.02\n",
            "iteration: 33610 loss: 0.0029 lr: 0.02\n",
            "iteration: 33620 loss: 0.0032 lr: 0.02\n",
            "iteration: 33630 loss: 0.0029 lr: 0.02\n",
            "iteration: 33640 loss: 0.0030 lr: 0.02\n",
            "iteration: 33650 loss: 0.0027 lr: 0.02\n",
            "iteration: 33660 loss: 0.0024 lr: 0.02\n",
            "iteration: 33670 loss: 0.0025 lr: 0.02\n",
            "iteration: 33680 loss: 0.0037 lr: 0.02\n",
            "iteration: 33690 loss: 0.0031 lr: 0.02\n",
            "iteration: 33700 loss: 0.0029 lr: 0.02\n",
            "iteration: 33710 loss: 0.0028 lr: 0.02\n",
            "iteration: 33720 loss: 0.0036 lr: 0.02\n",
            "iteration: 33730 loss: 0.0030 lr: 0.02\n",
            "iteration: 33740 loss: 0.0027 lr: 0.02\n",
            "iteration: 33750 loss: 0.0034 lr: 0.02\n",
            "iteration: 33760 loss: 0.0026 lr: 0.02\n",
            "iteration: 33770 loss: 0.0029 lr: 0.02\n",
            "iteration: 33780 loss: 0.0033 lr: 0.02\n",
            "iteration: 33790 loss: 0.0032 lr: 0.02\n",
            "iteration: 33800 loss: 0.0027 lr: 0.02\n",
            "iteration: 33810 loss: 0.0024 lr: 0.02\n",
            "iteration: 33820 loss: 0.0031 lr: 0.02\n",
            "iteration: 33830 loss: 0.0025 lr: 0.02\n",
            "iteration: 33840 loss: 0.0028 lr: 0.02\n",
            "iteration: 33850 loss: 0.0033 lr: 0.02\n",
            "iteration: 33860 loss: 0.0028 lr: 0.02\n",
            "iteration: 33870 loss: 0.0027 lr: 0.02\n",
            "iteration: 33880 loss: 0.0018 lr: 0.02\n",
            "iteration: 33890 loss: 0.0027 lr: 0.02\n",
            "iteration: 33900 loss: 0.0032 lr: 0.02\n",
            "iteration: 33910 loss: 0.0022 lr: 0.02\n",
            "iteration: 33920 loss: 0.0025 lr: 0.02\n",
            "iteration: 33930 loss: 0.0027 lr: 0.02\n",
            "iteration: 33940 loss: 0.0037 lr: 0.02\n",
            "iteration: 33950 loss: 0.0027 lr: 0.02\n",
            "iteration: 33960 loss: 0.0023 lr: 0.02\n",
            "iteration: 33970 loss: 0.0030 lr: 0.02\n",
            "iteration: 33980 loss: 0.0024 lr: 0.02\n",
            "iteration: 33990 loss: 0.0036 lr: 0.02\n",
            "iteration: 34000 loss: 0.0032 lr: 0.02\n",
            "iteration: 34010 loss: 0.0028 lr: 0.02\n",
            "iteration: 34020 loss: 0.0025 lr: 0.02\n",
            "iteration: 34030 loss: 0.0020 lr: 0.02\n",
            "iteration: 34040 loss: 0.0034 lr: 0.02\n",
            "iteration: 34050 loss: 0.0021 lr: 0.02\n",
            "iteration: 34060 loss: 0.0036 lr: 0.02\n",
            "iteration: 34070 loss: 0.0030 lr: 0.02\n",
            "iteration: 34080 loss: 0.0033 lr: 0.02\n",
            "iteration: 34090 loss: 0.0031 lr: 0.02\n",
            "iteration: 34100 loss: 0.0034 lr: 0.02\n",
            "iteration: 34110 loss: 0.0027 lr: 0.02\n",
            "iteration: 34120 loss: 0.0042 lr: 0.02\n",
            "iteration: 34130 loss: 0.0030 lr: 0.02\n",
            "iteration: 34140 loss: 0.0026 lr: 0.02\n",
            "iteration: 34150 loss: 0.0032 lr: 0.02\n",
            "iteration: 34160 loss: 0.0034 lr: 0.02\n",
            "iteration: 34170 loss: 0.0021 lr: 0.02\n",
            "iteration: 34180 loss: 0.0028 lr: 0.02\n",
            "iteration: 34190 loss: 0.0025 lr: 0.02\n",
            "iteration: 34200 loss: 0.0028 lr: 0.02\n",
            "iteration: 34210 loss: 0.0031 lr: 0.02\n",
            "iteration: 34220 loss: 0.0028 lr: 0.02\n",
            "iteration: 34230 loss: 0.0020 lr: 0.02\n",
            "iteration: 34240 loss: 0.0026 lr: 0.02\n",
            "iteration: 34250 loss: 0.0028 lr: 0.02\n",
            "iteration: 34260 loss: 0.0031 lr: 0.02\n",
            "iteration: 34270 loss: 0.0028 lr: 0.02\n",
            "iteration: 34280 loss: 0.0026 lr: 0.02\n",
            "iteration: 34290 loss: 0.0031 lr: 0.02\n",
            "iteration: 34300 loss: 0.0028 lr: 0.02\n",
            "iteration: 34310 loss: 0.0032 lr: 0.02\n",
            "iteration: 34320 loss: 0.0023 lr: 0.02\n",
            "iteration: 34330 loss: 0.0037 lr: 0.02\n",
            "iteration: 34340 loss: 0.0030 lr: 0.02\n",
            "iteration: 34350 loss: 0.0029 lr: 0.02\n",
            "iteration: 34360 loss: 0.0034 lr: 0.02\n",
            "iteration: 34370 loss: 0.0027 lr: 0.02\n",
            "iteration: 34380 loss: 0.0030 lr: 0.02\n",
            "iteration: 34390 loss: 0.0034 lr: 0.02\n",
            "iteration: 34400 loss: 0.0026 lr: 0.02\n",
            "iteration: 34410 loss: 0.0042 lr: 0.02\n",
            "iteration: 34420 loss: 0.0025 lr: 0.02\n",
            "iteration: 34430 loss: 0.0038 lr: 0.02\n",
            "iteration: 34440 loss: 0.0030 lr: 0.02\n",
            "iteration: 34450 loss: 0.0030 lr: 0.02\n",
            "iteration: 34460 loss: 0.0024 lr: 0.02\n",
            "iteration: 34470 loss: 0.0027 lr: 0.02\n",
            "iteration: 34480 loss: 0.0022 lr: 0.02\n",
            "iteration: 34490 loss: 0.0023 lr: 0.02\n",
            "iteration: 34500 loss: 0.0025 lr: 0.02\n",
            "iteration: 34510 loss: 0.0024 lr: 0.02\n",
            "iteration: 34520 loss: 0.0029 lr: 0.02\n",
            "iteration: 34530 loss: 0.0027 lr: 0.02\n",
            "iteration: 34540 loss: 0.0018 lr: 0.02\n",
            "iteration: 34550 loss: 0.0028 lr: 0.02\n",
            "iteration: 34560 loss: 0.0022 lr: 0.02\n",
            "iteration: 34570 loss: 0.0019 lr: 0.02\n",
            "iteration: 34580 loss: 0.0032 lr: 0.02\n",
            "iteration: 34590 loss: 0.0032 lr: 0.02\n",
            "iteration: 34600 loss: 0.0029 lr: 0.02\n",
            "iteration: 34610 loss: 0.0030 lr: 0.02\n",
            "iteration: 34620 loss: 0.0027 lr: 0.02\n",
            "iteration: 34630 loss: 0.0048 lr: 0.02\n",
            "iteration: 34640 loss: 0.0022 lr: 0.02\n",
            "iteration: 34650 loss: 0.0031 lr: 0.02\n",
            "iteration: 34660 loss: 0.0028 lr: 0.02\n",
            "iteration: 34670 loss: 0.0031 lr: 0.02\n",
            "iteration: 34680 loss: 0.0031 lr: 0.02\n",
            "iteration: 34690 loss: 0.0027 lr: 0.02\n",
            "iteration: 34700 loss: 0.0029 lr: 0.02\n",
            "iteration: 34710 loss: 0.0027 lr: 0.02\n",
            "iteration: 34720 loss: 0.0023 lr: 0.02\n",
            "iteration: 34730 loss: 0.0034 lr: 0.02\n",
            "iteration: 34740 loss: 0.0030 lr: 0.02\n",
            "iteration: 34750 loss: 0.0028 lr: 0.02\n",
            "iteration: 34760 loss: 0.0033 lr: 0.02\n",
            "iteration: 34770 loss: 0.0025 lr: 0.02\n",
            "iteration: 34780 loss: 0.0026 lr: 0.02\n",
            "iteration: 34790 loss: 0.0035 lr: 0.02\n",
            "iteration: 34800 loss: 0.0030 lr: 0.02\n",
            "iteration: 34810 loss: 0.0033 lr: 0.02\n",
            "iteration: 34820 loss: 0.0026 lr: 0.02\n",
            "iteration: 34830 loss: 0.0029 lr: 0.02\n",
            "iteration: 34840 loss: 0.0032 lr: 0.02\n",
            "iteration: 34850 loss: 0.0028 lr: 0.02\n",
            "iteration: 34860 loss: 0.0024 lr: 0.02\n",
            "iteration: 34870 loss: 0.0023 lr: 0.02\n",
            "iteration: 34880 loss: 0.0025 lr: 0.02\n",
            "iteration: 34890 loss: 0.0024 lr: 0.02\n",
            "iteration: 34900 loss: 0.0034 lr: 0.02\n",
            "iteration: 34910 loss: 0.0033 lr: 0.02\n",
            "iteration: 34920 loss: 0.0028 lr: 0.02\n",
            "iteration: 34930 loss: 0.0033 lr: 0.02\n",
            "iteration: 34940 loss: 0.0025 lr: 0.02\n",
            "iteration: 34950 loss: 0.0038 lr: 0.02\n",
            "iteration: 34960 loss: 0.0035 lr: 0.02\n",
            "iteration: 34970 loss: 0.0026 lr: 0.02\n",
            "iteration: 34980 loss: 0.0022 lr: 0.02\n",
            "iteration: 34990 loss: 0.0023 lr: 0.02\n",
            "iteration: 35000 loss: 0.0026 lr: 0.02\n",
            "iteration: 35010 loss: 0.0027 lr: 0.02\n",
            "iteration: 35020 loss: 0.0033 lr: 0.02\n",
            "iteration: 35030 loss: 0.0029 lr: 0.02\n",
            "iteration: 35040 loss: 0.0023 lr: 0.02\n",
            "iteration: 35050 loss: 0.0022 lr: 0.02\n",
            "iteration: 35060 loss: 0.0029 lr: 0.02\n",
            "iteration: 35070 loss: 0.0033 lr: 0.02\n",
            "iteration: 35080 loss: 0.0034 lr: 0.02\n",
            "iteration: 35090 loss: 0.0022 lr: 0.02\n",
            "iteration: 35100 loss: 0.0027 lr: 0.02\n",
            "iteration: 35110 loss: 0.0023 lr: 0.02\n",
            "iteration: 35120 loss: 0.0025 lr: 0.02\n",
            "iteration: 35130 loss: 0.0028 lr: 0.02\n",
            "iteration: 35140 loss: 0.0036 lr: 0.02\n",
            "iteration: 35150 loss: 0.0024 lr: 0.02\n",
            "iteration: 35160 loss: 0.0022 lr: 0.02\n",
            "iteration: 35170 loss: 0.0029 lr: 0.02\n",
            "iteration: 35180 loss: 0.0035 lr: 0.02\n",
            "iteration: 35190 loss: 0.0027 lr: 0.02\n",
            "iteration: 35200 loss: 0.0036 lr: 0.02\n",
            "iteration: 35210 loss: 0.0033 lr: 0.02\n",
            "iteration: 35220 loss: 0.0035 lr: 0.02\n",
            "iteration: 35230 loss: 0.0020 lr: 0.02\n",
            "iteration: 35240 loss: 0.0029 lr: 0.02\n",
            "iteration: 35250 loss: 0.0021 lr: 0.02\n",
            "iteration: 35260 loss: 0.0031 lr: 0.02\n",
            "iteration: 35270 loss: 0.0028 lr: 0.02\n",
            "iteration: 35280 loss: 0.0024 lr: 0.02\n",
            "iteration: 35290 loss: 0.0022 lr: 0.02\n",
            "iteration: 35300 loss: 0.0026 lr: 0.02\n",
            "iteration: 35310 loss: 0.0031 lr: 0.02\n",
            "iteration: 35320 loss: 0.0020 lr: 0.02\n",
            "iteration: 35330 loss: 0.0024 lr: 0.02\n",
            "iteration: 35340 loss: 0.0031 lr: 0.02\n",
            "iteration: 35350 loss: 0.0031 lr: 0.02\n",
            "iteration: 35360 loss: 0.0031 lr: 0.02\n",
            "iteration: 35370 loss: 0.0019 lr: 0.02\n",
            "iteration: 35380 loss: 0.0027 lr: 0.02\n",
            "iteration: 35390 loss: 0.0033 lr: 0.02\n",
            "iteration: 35400 loss: 0.0025 lr: 0.02\n",
            "iteration: 35410 loss: 0.0038 lr: 0.02\n",
            "iteration: 35420 loss: 0.0027 lr: 0.02\n",
            "iteration: 35430 loss: 0.0030 lr: 0.02\n",
            "iteration: 35440 loss: 0.0028 lr: 0.02\n",
            "iteration: 35450 loss: 0.0032 lr: 0.02\n",
            "iteration: 35460 loss: 0.0031 lr: 0.02\n",
            "iteration: 35470 loss: 0.0028 lr: 0.02\n",
            "iteration: 35480 loss: 0.0029 lr: 0.02\n",
            "iteration: 35490 loss: 0.0034 lr: 0.02\n",
            "iteration: 35500 loss: 0.0027 lr: 0.02\n",
            "iteration: 35510 loss: 0.0026 lr: 0.02\n",
            "iteration: 35520 loss: 0.0032 lr: 0.02\n",
            "iteration: 35530 loss: 0.0036 lr: 0.02\n",
            "iteration: 35540 loss: 0.0024 lr: 0.02\n",
            "iteration: 35550 loss: 0.0032 lr: 0.02\n",
            "iteration: 35560 loss: 0.0026 lr: 0.02\n",
            "iteration: 35570 loss: 0.0027 lr: 0.02\n",
            "iteration: 35580 loss: 0.0027 lr: 0.02\n",
            "iteration: 35590 loss: 0.0030 lr: 0.02\n",
            "iteration: 35600 loss: 0.0031 lr: 0.02\n",
            "iteration: 35610 loss: 0.0033 lr: 0.02\n",
            "iteration: 35620 loss: 0.0030 lr: 0.02\n",
            "iteration: 35630 loss: 0.0036 lr: 0.02\n",
            "iteration: 35640 loss: 0.0028 lr: 0.02\n",
            "iteration: 35650 loss: 0.0028 lr: 0.02\n",
            "iteration: 35660 loss: 0.0036 lr: 0.02\n",
            "iteration: 35670 loss: 0.0031 lr: 0.02\n",
            "iteration: 35680 loss: 0.0035 lr: 0.02\n",
            "iteration: 35690 loss: 0.0028 lr: 0.02\n",
            "iteration: 35700 loss: 0.0030 lr: 0.02\n",
            "iteration: 35710 loss: 0.0022 lr: 0.02\n",
            "iteration: 35720 loss: 0.0033 lr: 0.02\n",
            "iteration: 35730 loss: 0.0027 lr: 0.02\n",
            "iteration: 35740 loss: 0.0022 lr: 0.02\n",
            "iteration: 35750 loss: 0.0023 lr: 0.02\n",
            "iteration: 35760 loss: 0.0030 lr: 0.02\n",
            "iteration: 35770 loss: 0.0028 lr: 0.02\n",
            "iteration: 35780 loss: 0.0025 lr: 0.02\n",
            "iteration: 35790 loss: 0.0025 lr: 0.02\n",
            "iteration: 35800 loss: 0.0026 lr: 0.02\n",
            "iteration: 35810 loss: 0.0034 lr: 0.02\n",
            "iteration: 35820 loss: 0.0024 lr: 0.02\n",
            "iteration: 35830 loss: 0.0024 lr: 0.02\n",
            "iteration: 35840 loss: 0.0020 lr: 0.02\n",
            "iteration: 35850 loss: 0.0024 lr: 0.02\n",
            "iteration: 35860 loss: 0.0032 lr: 0.02\n",
            "iteration: 35870 loss: 0.0032 lr: 0.02\n",
            "iteration: 35880 loss: 0.0031 lr: 0.02\n",
            "iteration: 35890 loss: 0.0021 lr: 0.02\n",
            "iteration: 35900 loss: 0.0020 lr: 0.02\n",
            "iteration: 35910 loss: 0.0025 lr: 0.02\n",
            "iteration: 35920 loss: 0.0023 lr: 0.02\n",
            "iteration: 35930 loss: 0.0030 lr: 0.02\n",
            "iteration: 35940 loss: 0.0024 lr: 0.02\n",
            "iteration: 35950 loss: 0.0023 lr: 0.02\n",
            "iteration: 35960 loss: 0.0030 lr: 0.02\n",
            "iteration: 35970 loss: 0.0027 lr: 0.02\n",
            "iteration: 35980 loss: 0.0031 lr: 0.02\n",
            "iteration: 35990 loss: 0.0032 lr: 0.02\n",
            "iteration: 36000 loss: 0.0026 lr: 0.02\n",
            "iteration: 36010 loss: 0.0032 lr: 0.02\n",
            "iteration: 36020 loss: 0.0027 lr: 0.02\n",
            "iteration: 36030 loss: 0.0027 lr: 0.02\n",
            "iteration: 36040 loss: 0.0034 lr: 0.02\n",
            "iteration: 36050 loss: 0.0021 lr: 0.02\n",
            "iteration: 36060 loss: 0.0027 lr: 0.02\n",
            "iteration: 36070 loss: 0.0021 lr: 0.02\n",
            "iteration: 36080 loss: 0.0026 lr: 0.02\n",
            "iteration: 36090 loss: 0.0023 lr: 0.02\n",
            "iteration: 36100 loss: 0.0024 lr: 0.02\n",
            "iteration: 36110 loss: 0.0029 lr: 0.02\n",
            "iteration: 36120 loss: 0.0028 lr: 0.02\n",
            "iteration: 36130 loss: 0.0027 lr: 0.02\n",
            "iteration: 36140 loss: 0.0027 lr: 0.02\n",
            "iteration: 36150 loss: 0.0026 lr: 0.02\n",
            "iteration: 36160 loss: 0.0023 lr: 0.02\n",
            "iteration: 36170 loss: 0.0021 lr: 0.02\n",
            "iteration: 36180 loss: 0.0027 lr: 0.02\n",
            "iteration: 36190 loss: 0.0027 lr: 0.02\n",
            "iteration: 36200 loss: 0.0032 lr: 0.02\n",
            "iteration: 36210 loss: 0.0040 lr: 0.02\n",
            "iteration: 36220 loss: 0.0029 lr: 0.02\n",
            "iteration: 36230 loss: 0.0029 lr: 0.02\n",
            "iteration: 36240 loss: 0.0031 lr: 0.02\n",
            "iteration: 36250 loss: 0.0031 lr: 0.02\n",
            "iteration: 36260 loss: 0.0023 lr: 0.02\n",
            "iteration: 36270 loss: 0.0024 lr: 0.02\n",
            "iteration: 36280 loss: 0.0019 lr: 0.02\n",
            "iteration: 36290 loss: 0.0034 lr: 0.02\n",
            "iteration: 36300 loss: 0.0023 lr: 0.02\n",
            "iteration: 36310 loss: 0.0030 lr: 0.02\n",
            "iteration: 36320 loss: 0.0036 lr: 0.02\n",
            "iteration: 36330 loss: 0.0023 lr: 0.02\n",
            "iteration: 36340 loss: 0.0021 lr: 0.02\n",
            "iteration: 36350 loss: 0.0024 lr: 0.02\n",
            "iteration: 36360 loss: 0.0028 lr: 0.02\n",
            "iteration: 36370 loss: 0.0023 lr: 0.02\n",
            "iteration: 36380 loss: 0.0027 lr: 0.02\n",
            "iteration: 36390 loss: 0.0025 lr: 0.02\n",
            "iteration: 36400 loss: 0.0026 lr: 0.02\n",
            "iteration: 36410 loss: 0.0025 lr: 0.02\n",
            "iteration: 36420 loss: 0.0023 lr: 0.02\n",
            "iteration: 36430 loss: 0.0045 lr: 0.02\n",
            "iteration: 36440 loss: 0.0036 lr: 0.02\n",
            "iteration: 36450 loss: 0.0029 lr: 0.02\n",
            "iteration: 36460 loss: 0.0028 lr: 0.02\n",
            "iteration: 36470 loss: 0.0032 lr: 0.02\n",
            "iteration: 36480 loss: 0.0029 lr: 0.02\n",
            "iteration: 36490 loss: 0.0036 lr: 0.02\n",
            "iteration: 36500 loss: 0.0029 lr: 0.02\n",
            "iteration: 36510 loss: 0.0026 lr: 0.02\n",
            "iteration: 36520 loss: 0.0025 lr: 0.02\n",
            "iteration: 36530 loss: 0.0029 lr: 0.02\n",
            "iteration: 36540 loss: 0.0036 lr: 0.02\n",
            "iteration: 36550 loss: 0.0030 lr: 0.02\n",
            "iteration: 36560 loss: 0.0023 lr: 0.02\n",
            "iteration: 36570 loss: 0.0031 lr: 0.02\n",
            "iteration: 36580 loss: 0.0025 lr: 0.02\n",
            "iteration: 36590 loss: 0.0032 lr: 0.02\n",
            "iteration: 36600 loss: 0.0022 lr: 0.02\n",
            "iteration: 36610 loss: 0.0026 lr: 0.02\n",
            "iteration: 36620 loss: 0.0028 lr: 0.02\n",
            "iteration: 36630 loss: 0.0031 lr: 0.02\n",
            "iteration: 36640 loss: 0.0034 lr: 0.02\n",
            "iteration: 36650 loss: 0.0027 lr: 0.02\n",
            "iteration: 36660 loss: 0.0030 lr: 0.02\n",
            "iteration: 36670 loss: 0.0030 lr: 0.02\n",
            "iteration: 36680 loss: 0.0026 lr: 0.02\n",
            "iteration: 36690 loss: 0.0036 lr: 0.02\n",
            "iteration: 36700 loss: 0.0038 lr: 0.02\n",
            "iteration: 36710 loss: 0.0021 lr: 0.02\n",
            "iteration: 36720 loss: 0.0028 lr: 0.02\n",
            "iteration: 36730 loss: 0.0029 lr: 0.02\n",
            "iteration: 36740 loss: 0.0029 lr: 0.02\n",
            "iteration: 36750 loss: 0.0029 lr: 0.02\n",
            "iteration: 36760 loss: 0.0031 lr: 0.02\n",
            "iteration: 36770 loss: 0.0027 lr: 0.02\n",
            "iteration: 36780 loss: 0.0028 lr: 0.02\n",
            "iteration: 36790 loss: 0.0022 lr: 0.02\n",
            "iteration: 36800 loss: 0.0023 lr: 0.02\n",
            "iteration: 36810 loss: 0.0027 lr: 0.02\n",
            "iteration: 36820 loss: 0.0027 lr: 0.02\n",
            "iteration: 36830 loss: 0.0032 lr: 0.02\n",
            "iteration: 36840 loss: 0.0033 lr: 0.02\n",
            "iteration: 36850 loss: 0.0027 lr: 0.02\n",
            "iteration: 36860 loss: 0.0027 lr: 0.02\n",
            "iteration: 36870 loss: 0.0022 lr: 0.02\n",
            "iteration: 36880 loss: 0.0034 lr: 0.02\n",
            "iteration: 36890 loss: 0.0029 lr: 0.02\n",
            "iteration: 36900 loss: 0.0032 lr: 0.02\n",
            "iteration: 36910 loss: 0.0027 lr: 0.02\n",
            "iteration: 36920 loss: 0.0028 lr: 0.02\n",
            "iteration: 36930 loss: 0.0029 lr: 0.02\n",
            "iteration: 36940 loss: 0.0027 lr: 0.02\n",
            "iteration: 36950 loss: 0.0031 lr: 0.02\n",
            "iteration: 36960 loss: 0.0025 lr: 0.02\n",
            "iteration: 36970 loss: 0.0030 lr: 0.02\n",
            "iteration: 36980 loss: 0.0027 lr: 0.02\n",
            "iteration: 36990 loss: 0.0030 lr: 0.02\n",
            "iteration: 37000 loss: 0.0047 lr: 0.02\n",
            "iteration: 37010 loss: 0.0035 lr: 0.02\n",
            "iteration: 37020 loss: 0.0027 lr: 0.02\n",
            "iteration: 37030 loss: 0.0023 lr: 0.02\n",
            "iteration: 37040 loss: 0.0033 lr: 0.02\n",
            "iteration: 37050 loss: 0.0023 lr: 0.02\n",
            "iteration: 37060 loss: 0.0021 lr: 0.02\n",
            "iteration: 37070 loss: 0.0026 lr: 0.02\n",
            "iteration: 37080 loss: 0.0023 lr: 0.02\n",
            "iteration: 37090 loss: 0.0020 lr: 0.02\n",
            "iteration: 37100 loss: 0.0028 lr: 0.02\n",
            "iteration: 37110 loss: 0.0031 lr: 0.02\n",
            "iteration: 37120 loss: 0.0030 lr: 0.02\n",
            "iteration: 37130 loss: 0.0032 lr: 0.02\n",
            "iteration: 37140 loss: 0.0028 lr: 0.02\n",
            "iteration: 37150 loss: 0.0028 lr: 0.02\n",
            "iteration: 37160 loss: 0.0023 lr: 0.02\n",
            "iteration: 37170 loss: 0.0023 lr: 0.02\n",
            "iteration: 37180 loss: 0.0020 lr: 0.02\n",
            "iteration: 37190 loss: 0.0022 lr: 0.02\n",
            "iteration: 37200 loss: 0.0030 lr: 0.02\n",
            "iteration: 37210 loss: 0.0033 lr: 0.02\n",
            "iteration: 37220 loss: 0.0029 lr: 0.02\n",
            "iteration: 37230 loss: 0.0027 lr: 0.02\n",
            "iteration: 37240 loss: 0.0038 lr: 0.02\n",
            "iteration: 37250 loss: 0.0019 lr: 0.02\n",
            "iteration: 37260 loss: 0.0021 lr: 0.02\n",
            "iteration: 37270 loss: 0.0018 lr: 0.02\n",
            "iteration: 37280 loss: 0.0026 lr: 0.02\n",
            "iteration: 37290 loss: 0.0026 lr: 0.02\n",
            "iteration: 37300 loss: 0.0023 lr: 0.02\n",
            "iteration: 37310 loss: 0.0029 lr: 0.02\n",
            "iteration: 37320 loss: 0.0027 lr: 0.02\n",
            "iteration: 37330 loss: 0.0023 lr: 0.02\n",
            "iteration: 37340 loss: 0.0028 lr: 0.02\n",
            "iteration: 37350 loss: 0.0037 lr: 0.02\n",
            "iteration: 37360 loss: 0.0021 lr: 0.02\n",
            "iteration: 37370 loss: 0.0026 lr: 0.02\n",
            "iteration: 37380 loss: 0.0033 lr: 0.02\n",
            "iteration: 37390 loss: 0.0020 lr: 0.02\n",
            "iteration: 37400 loss: 0.0024 lr: 0.02\n",
            "iteration: 37410 loss: 0.0023 lr: 0.02\n",
            "iteration: 37420 loss: 0.0021 lr: 0.02\n",
            "iteration: 37430 loss: 0.0029 lr: 0.02\n",
            "iteration: 37440 loss: 0.0026 lr: 0.02\n",
            "iteration: 37450 loss: 0.0028 lr: 0.02\n",
            "iteration: 37460 loss: 0.0033 lr: 0.02\n",
            "iteration: 37470 loss: 0.0024 lr: 0.02\n",
            "iteration: 37480 loss: 0.0028 lr: 0.02\n",
            "iteration: 37490 loss: 0.0025 lr: 0.02\n",
            "iteration: 37500 loss: 0.0023 lr: 0.02\n",
            "iteration: 37510 loss: 0.0029 lr: 0.02\n",
            "iteration: 37520 loss: 0.0021 lr: 0.02\n",
            "iteration: 37530 loss: 0.0023 lr: 0.02\n",
            "iteration: 37540 loss: 0.0030 lr: 0.02\n",
            "iteration: 37550 loss: 0.0020 lr: 0.02\n",
            "iteration: 37560 loss: 0.0024 lr: 0.02\n",
            "iteration: 37570 loss: 0.0027 lr: 0.02\n",
            "iteration: 37580 loss: 0.0032 lr: 0.02\n",
            "iteration: 37590 loss: 0.0026 lr: 0.02\n",
            "iteration: 37600 loss: 0.0021 lr: 0.02\n",
            "iteration: 37610 loss: 0.0018 lr: 0.02\n",
            "iteration: 37620 loss: 0.0021 lr: 0.02\n",
            "iteration: 37630 loss: 0.0022 lr: 0.02\n",
            "iteration: 37640 loss: 0.0031 lr: 0.02\n",
            "iteration: 37650 loss: 0.0029 lr: 0.02\n",
            "iteration: 37660 loss: 0.0019 lr: 0.02\n",
            "iteration: 37670 loss: 0.0031 lr: 0.02\n",
            "iteration: 37680 loss: 0.0023 lr: 0.02\n",
            "iteration: 37690 loss: 0.0030 lr: 0.02\n",
            "iteration: 37700 loss: 0.0026 lr: 0.02\n",
            "iteration: 37710 loss: 0.0029 lr: 0.02\n",
            "iteration: 37720 loss: 0.0025 lr: 0.02\n",
            "iteration: 37730 loss: 0.0027 lr: 0.02\n",
            "iteration: 37740 loss: 0.0027 lr: 0.02\n",
            "iteration: 37750 loss: 0.0026 lr: 0.02\n",
            "iteration: 37760 loss: 0.0022 lr: 0.02\n",
            "iteration: 37770 loss: 0.0033 lr: 0.02\n",
            "iteration: 37780 loss: 0.0029 lr: 0.02\n",
            "iteration: 37790 loss: 0.0027 lr: 0.02\n",
            "iteration: 37800 loss: 0.0035 lr: 0.02\n",
            "iteration: 37810 loss: 0.0031 lr: 0.02\n",
            "iteration: 37820 loss: 0.0031 lr: 0.02\n",
            "iteration: 37830 loss: 0.0020 lr: 0.02\n",
            "iteration: 37840 loss: 0.0034 lr: 0.02\n",
            "iteration: 37850 loss: 0.0040 lr: 0.02\n",
            "iteration: 37860 loss: 0.0025 lr: 0.02\n",
            "iteration: 37870 loss: 0.0024 lr: 0.02\n",
            "iteration: 37880 loss: 0.0027 lr: 0.02\n",
            "iteration: 37890 loss: 0.0023 lr: 0.02\n",
            "iteration: 37900 loss: 0.0019 lr: 0.02\n",
            "iteration: 37910 loss: 0.0020 lr: 0.02\n",
            "iteration: 37920 loss: 0.0028 lr: 0.02\n",
            "iteration: 37930 loss: 0.0024 lr: 0.02\n",
            "iteration: 37940 loss: 0.0027 lr: 0.02\n",
            "iteration: 37950 loss: 0.0023 lr: 0.02\n",
            "iteration: 37960 loss: 0.0030 lr: 0.02\n",
            "iteration: 37970 loss: 0.0028 lr: 0.02\n",
            "iteration: 37980 loss: 0.0023 lr: 0.02\n",
            "iteration: 37990 loss: 0.0025 lr: 0.02\n",
            "iteration: 38000 loss: 0.0027 lr: 0.02\n",
            "iteration: 38010 loss: 0.0026 lr: 0.02\n",
            "iteration: 38020 loss: 0.0024 lr: 0.02\n",
            "iteration: 38030 loss: 0.0027 lr: 0.02\n",
            "iteration: 38040 loss: 0.0024 lr: 0.02\n",
            "iteration: 38050 loss: 0.0028 lr: 0.02\n",
            "iteration: 38060 loss: 0.0021 lr: 0.02\n",
            "iteration: 38070 loss: 0.0023 lr: 0.02\n",
            "iteration: 38080 loss: 0.0033 lr: 0.02\n",
            "iteration: 38090 loss: 0.0022 lr: 0.02\n",
            "iteration: 38100 loss: 0.0032 lr: 0.02\n",
            "iteration: 38110 loss: 0.0030 lr: 0.02\n",
            "iteration: 38120 loss: 0.0027 lr: 0.02\n",
            "iteration: 38130 loss: 0.0017 lr: 0.02\n",
            "iteration: 38140 loss: 0.0022 lr: 0.02\n",
            "iteration: 38150 loss: 0.0025 lr: 0.02\n",
            "iteration: 38160 loss: 0.0031 lr: 0.02\n",
            "iteration: 38170 loss: 0.0027 lr: 0.02\n",
            "iteration: 38180 loss: 0.0019 lr: 0.02\n",
            "iteration: 38190 loss: 0.0022 lr: 0.02\n",
            "iteration: 38200 loss: 0.0027 lr: 0.02\n",
            "iteration: 38210 loss: 0.0031 lr: 0.02\n",
            "iteration: 38220 loss: 0.0021 lr: 0.02\n",
            "iteration: 38230 loss: 0.0032 lr: 0.02\n",
            "iteration: 38240 loss: 0.0026 lr: 0.02\n",
            "iteration: 38250 loss: 0.0028 lr: 0.02\n",
            "iteration: 38260 loss: 0.0029 lr: 0.02\n",
            "iteration: 38270 loss: 0.0031 lr: 0.02\n",
            "iteration: 38280 loss: 0.0024 lr: 0.02\n",
            "iteration: 38290 loss: 0.0027 lr: 0.02\n",
            "iteration: 38300 loss: 0.0041 lr: 0.02\n",
            "iteration: 38310 loss: 0.0036 lr: 0.02\n",
            "iteration: 38320 loss: 0.0025 lr: 0.02\n",
            "iteration: 38330 loss: 0.0024 lr: 0.02\n",
            "iteration: 38340 loss: 0.0029 lr: 0.02\n",
            "iteration: 38350 loss: 0.0035 lr: 0.02\n",
            "iteration: 38360 loss: 0.0023 lr: 0.02\n",
            "iteration: 38370 loss: 0.0031 lr: 0.02\n",
            "iteration: 38380 loss: 0.0027 lr: 0.02\n",
            "iteration: 38390 loss: 0.0030 lr: 0.02\n",
            "iteration: 38400 loss: 0.0026 lr: 0.02\n",
            "iteration: 38410 loss: 0.0030 lr: 0.02\n",
            "iteration: 38420 loss: 0.0028 lr: 0.02\n",
            "iteration: 38430 loss: 0.0027 lr: 0.02\n",
            "iteration: 38440 loss: 0.0026 lr: 0.02\n",
            "iteration: 38450 loss: 0.0031 lr: 0.02\n",
            "iteration: 38460 loss: 0.0020 lr: 0.02\n",
            "iteration: 38470 loss: 0.0033 lr: 0.02\n",
            "iteration: 38480 loss: 0.0018 lr: 0.02\n",
            "iteration: 38490 loss: 0.0028 lr: 0.02\n",
            "iteration: 38500 loss: 0.0031 lr: 0.02\n",
            "iteration: 38510 loss: 0.0033 lr: 0.02\n",
            "iteration: 38520 loss: 0.0024 lr: 0.02\n",
            "iteration: 38530 loss: 0.0024 lr: 0.02\n",
            "iteration: 38540 loss: 0.0023 lr: 0.02\n",
            "iteration: 38550 loss: 0.0023 lr: 0.02\n",
            "iteration: 38560 loss: 0.0023 lr: 0.02\n",
            "iteration: 38570 loss: 0.0028 lr: 0.02\n",
            "iteration: 38580 loss: 0.0018 lr: 0.02\n",
            "iteration: 38590 loss: 0.0033 lr: 0.02\n",
            "iteration: 38600 loss: 0.0025 lr: 0.02\n",
            "iteration: 38610 loss: 0.0027 lr: 0.02\n",
            "iteration: 38620 loss: 0.0026 lr: 0.02\n",
            "iteration: 38630 loss: 0.0020 lr: 0.02\n",
            "iteration: 38640 loss: 0.0029 lr: 0.02\n",
            "iteration: 38650 loss: 0.0025 lr: 0.02\n",
            "iteration: 38660 loss: 0.0021 lr: 0.02\n",
            "iteration: 38670 loss: 0.0027 lr: 0.02\n",
            "iteration: 38680 loss: 0.0020 lr: 0.02\n",
            "iteration: 38690 loss: 0.0023 lr: 0.02\n",
            "iteration: 38700 loss: 0.0030 lr: 0.02\n",
            "iteration: 38710 loss: 0.0028 lr: 0.02\n",
            "iteration: 38720 loss: 0.0025 lr: 0.02\n",
            "iteration: 38730 loss: 0.0018 lr: 0.02\n",
            "iteration: 38740 loss: 0.0030 lr: 0.02\n",
            "iteration: 38750 loss: 0.0022 lr: 0.02\n",
            "iteration: 38760 loss: 0.0027 lr: 0.02\n",
            "iteration: 38770 loss: 0.0030 lr: 0.02\n",
            "iteration: 38780 loss: 0.0024 lr: 0.02\n",
            "iteration: 38790 loss: 0.0024 lr: 0.02\n",
            "iteration: 38800 loss: 0.0024 lr: 0.02\n",
            "iteration: 38810 loss: 0.0014 lr: 0.02\n",
            "iteration: 38820 loss: 0.0026 lr: 0.02\n",
            "iteration: 38830 loss: 0.0024 lr: 0.02\n",
            "iteration: 38840 loss: 0.0027 lr: 0.02\n",
            "iteration: 38850 loss: 0.0025 lr: 0.02\n",
            "iteration: 38860 loss: 0.0026 lr: 0.02\n",
            "iteration: 38870 loss: 0.0024 lr: 0.02\n",
            "iteration: 38880 loss: 0.0031 lr: 0.02\n",
            "iteration: 38890 loss: 0.0029 lr: 0.02\n",
            "iteration: 38900 loss: 0.0026 lr: 0.02\n",
            "iteration: 38910 loss: 0.0025 lr: 0.02\n",
            "iteration: 38920 loss: 0.0025 lr: 0.02\n",
            "iteration: 38930 loss: 0.0022 lr: 0.02\n",
            "iteration: 38940 loss: 0.0024 lr: 0.02\n",
            "iteration: 38950 loss: 0.0036 lr: 0.02\n",
            "iteration: 38960 loss: 0.0030 lr: 0.02\n",
            "iteration: 38970 loss: 0.0023 lr: 0.02\n",
            "iteration: 38980 loss: 0.0030 lr: 0.02\n",
            "iteration: 38990 loss: 0.0019 lr: 0.02\n",
            "iteration: 39000 loss: 0.0022 lr: 0.02\n",
            "iteration: 39010 loss: 0.0026 lr: 0.02\n",
            "iteration: 39020 loss: 0.0030 lr: 0.02\n",
            "iteration: 39030 loss: 0.0023 lr: 0.02\n",
            "iteration: 39040 loss: 0.0024 lr: 0.02\n",
            "iteration: 39050 loss: 0.0026 lr: 0.02\n",
            "iteration: 39060 loss: 0.0028 lr: 0.02\n",
            "iteration: 39070 loss: 0.0033 lr: 0.02\n",
            "iteration: 39080 loss: 0.0031 lr: 0.02\n",
            "iteration: 39090 loss: 0.0028 lr: 0.02\n",
            "iteration: 39100 loss: 0.0024 lr: 0.02\n",
            "iteration: 39110 loss: 0.0021 lr: 0.02\n",
            "iteration: 39120 loss: 0.0034 lr: 0.02\n",
            "iteration: 39130 loss: 0.0025 lr: 0.02\n",
            "iteration: 39140 loss: 0.0023 lr: 0.02\n",
            "iteration: 39150 loss: 0.0027 lr: 0.02\n",
            "iteration: 39160 loss: 0.0031 lr: 0.02\n",
            "iteration: 39170 loss: 0.0032 lr: 0.02\n",
            "iteration: 39180 loss: 0.0029 lr: 0.02\n",
            "iteration: 39190 loss: 0.0039 lr: 0.02\n",
            "iteration: 39200 loss: 0.0022 lr: 0.02\n",
            "iteration: 39210 loss: 0.0021 lr: 0.02\n",
            "iteration: 39220 loss: 0.0024 lr: 0.02\n",
            "iteration: 39230 loss: 0.0030 lr: 0.02\n",
            "iteration: 39240 loss: 0.0022 lr: 0.02\n",
            "iteration: 39250 loss: 0.0027 lr: 0.02\n",
            "iteration: 39260 loss: 0.0024 lr: 0.02\n",
            "iteration: 39270 loss: 0.0030 lr: 0.02\n",
            "iteration: 39280 loss: 0.0044 lr: 0.02\n",
            "iteration: 39290 loss: 0.0033 lr: 0.02\n",
            "iteration: 39300 loss: 0.0020 lr: 0.02\n",
            "iteration: 39310 loss: 0.0022 lr: 0.02\n",
            "iteration: 39320 loss: 0.0042 lr: 0.02\n",
            "iteration: 39330 loss: 0.0024 lr: 0.02\n",
            "iteration: 39340 loss: 0.0020 lr: 0.02\n",
            "iteration: 39350 loss: 0.0030 lr: 0.02\n",
            "iteration: 39360 loss: 0.0023 lr: 0.02\n",
            "iteration: 39370 loss: 0.0020 lr: 0.02\n",
            "iteration: 39380 loss: 0.0025 lr: 0.02\n",
            "iteration: 39390 loss: 0.0018 lr: 0.02\n",
            "iteration: 39400 loss: 0.0030 lr: 0.02\n",
            "iteration: 39410 loss: 0.0027 lr: 0.02\n",
            "iteration: 39420 loss: 0.0025 lr: 0.02\n",
            "iteration: 39430 loss: 0.0023 lr: 0.02\n",
            "iteration: 39440 loss: 0.0020 lr: 0.02\n",
            "iteration: 39450 loss: 0.0027 lr: 0.02\n",
            "iteration: 39460 loss: 0.0022 lr: 0.02\n",
            "iteration: 39470 loss: 0.0018 lr: 0.02\n",
            "iteration: 39480 loss: 0.0022 lr: 0.02\n",
            "iteration: 39490 loss: 0.0029 lr: 0.02\n",
            "iteration: 39500 loss: 0.0040 lr: 0.02\n",
            "iteration: 39510 loss: 0.0025 lr: 0.02\n",
            "iteration: 39520 loss: 0.0030 lr: 0.02\n",
            "iteration: 39530 loss: 0.0036 lr: 0.02\n",
            "iteration: 39540 loss: 0.0020 lr: 0.02\n",
            "iteration: 39550 loss: 0.0027 lr: 0.02\n",
            "iteration: 39560 loss: 0.0019 lr: 0.02\n",
            "iteration: 39570 loss: 0.0031 lr: 0.02\n",
            "iteration: 39580 loss: 0.0028 lr: 0.02\n",
            "iteration: 39590 loss: 0.0029 lr: 0.02\n",
            "iteration: 39600 loss: 0.0027 lr: 0.02\n",
            "iteration: 39610 loss: 0.0025 lr: 0.02\n",
            "iteration: 39620 loss: 0.0026 lr: 0.02\n",
            "iteration: 39630 loss: 0.0032 lr: 0.02\n",
            "iteration: 39640 loss: 0.0027 lr: 0.02\n",
            "iteration: 39650 loss: 0.0025 lr: 0.02\n",
            "iteration: 39660 loss: 0.0025 lr: 0.02\n",
            "iteration: 39670 loss: 0.0027 lr: 0.02\n",
            "iteration: 39680 loss: 0.0024 lr: 0.02\n",
            "iteration: 39690 loss: 0.0025 lr: 0.02\n",
            "iteration: 39700 loss: 0.0031 lr: 0.02\n",
            "iteration: 39710 loss: 0.0029 lr: 0.02\n",
            "iteration: 39720 loss: 0.0030 lr: 0.02\n",
            "iteration: 39730 loss: 0.0026 lr: 0.02\n",
            "iteration: 39740 loss: 0.0025 lr: 0.02\n",
            "iteration: 39750 loss: 0.0025 lr: 0.02\n",
            "iteration: 39760 loss: 0.0022 lr: 0.02\n",
            "iteration: 39770 loss: 0.0022 lr: 0.02\n",
            "iteration: 39780 loss: 0.0026 lr: 0.02\n",
            "iteration: 39790 loss: 0.0029 lr: 0.02\n",
            "iteration: 39800 loss: 0.0038 lr: 0.02\n",
            "iteration: 39810 loss: 0.0025 lr: 0.02\n",
            "iteration: 39820 loss: 0.0021 lr: 0.02\n",
            "iteration: 39830 loss: 0.0026 lr: 0.02\n",
            "iteration: 39840 loss: 0.0039 lr: 0.02\n",
            "iteration: 39850 loss: 0.0023 lr: 0.02\n",
            "iteration: 39860 loss: 0.0025 lr: 0.02\n",
            "iteration: 39870 loss: 0.0021 lr: 0.02\n",
            "iteration: 39880 loss: 0.0033 lr: 0.02\n",
            "iteration: 39890 loss: 0.0032 lr: 0.02\n",
            "iteration: 39900 loss: 0.0023 lr: 0.02\n",
            "iteration: 39910 loss: 0.0034 lr: 0.02\n",
            "iteration: 39920 loss: 0.0021 lr: 0.02\n",
            "iteration: 39930 loss: 0.0031 lr: 0.02\n",
            "iteration: 39940 loss: 0.0030 lr: 0.02\n",
            "iteration: 39950 loss: 0.0024 lr: 0.02\n",
            "iteration: 39960 loss: 0.0032 lr: 0.02\n",
            "iteration: 39970 loss: 0.0029 lr: 0.02\n",
            "iteration: 39980 loss: 0.0029 lr: 0.02\n",
            "iteration: 39990 loss: 0.0021 lr: 0.02\n",
            "iteration: 40000 loss: 0.0024 lr: 0.02\n",
            "iteration: 40010 loss: 0.0026 lr: 0.02\n",
            "iteration: 40020 loss: 0.0024 lr: 0.02\n",
            "iteration: 40030 loss: 0.0029 lr: 0.02\n",
            "iteration: 40040 loss: 0.0022 lr: 0.02\n",
            "iteration: 40050 loss: 0.0026 lr: 0.02\n",
            "iteration: 40060 loss: 0.0029 lr: 0.02\n",
            "iteration: 40070 loss: 0.0025 lr: 0.02\n",
            "iteration: 40080 loss: 0.0022 lr: 0.02\n",
            "iteration: 40090 loss: 0.0021 lr: 0.02\n",
            "iteration: 40100 loss: 0.0023 lr: 0.02\n",
            "iteration: 40110 loss: 0.0020 lr: 0.02\n",
            "iteration: 40120 loss: 0.0032 lr: 0.02\n",
            "iteration: 40130 loss: 0.0019 lr: 0.02\n",
            "iteration: 40140 loss: 0.0020 lr: 0.02\n",
            "iteration: 40150 loss: 0.0019 lr: 0.02\n",
            "iteration: 40160 loss: 0.0024 lr: 0.02\n",
            "iteration: 40170 loss: 0.0031 lr: 0.02\n",
            "iteration: 40180 loss: 0.0025 lr: 0.02\n",
            "iteration: 40190 loss: 0.0022 lr: 0.02\n",
            "iteration: 40200 loss: 0.0026 lr: 0.02\n",
            "iteration: 40210 loss: 0.0028 lr: 0.02\n",
            "iteration: 40220 loss: 0.0026 lr: 0.02\n",
            "iteration: 40230 loss: 0.0023 lr: 0.02\n",
            "iteration: 40240 loss: 0.0024 lr: 0.02\n",
            "iteration: 40250 loss: 0.0024 lr: 0.02\n",
            "iteration: 40260 loss: 0.0028 lr: 0.02\n",
            "iteration: 40270 loss: 0.0019 lr: 0.02\n",
            "iteration: 40280 loss: 0.0032 lr: 0.02\n",
            "iteration: 40290 loss: 0.0021 lr: 0.02\n",
            "iteration: 40300 loss: 0.0017 lr: 0.02\n",
            "iteration: 40310 loss: 0.0024 lr: 0.02\n",
            "iteration: 40320 loss: 0.0032 lr: 0.02\n",
            "iteration: 40330 loss: 0.0020 lr: 0.02\n",
            "iteration: 40340 loss: 0.0027 lr: 0.02\n",
            "iteration: 40350 loss: 0.0017 lr: 0.02\n",
            "iteration: 40360 loss: 0.0023 lr: 0.02\n",
            "iteration: 40370 loss: 0.0020 lr: 0.02\n",
            "iteration: 40380 loss: 0.0026 lr: 0.02\n",
            "iteration: 40390 loss: 0.0039 lr: 0.02\n",
            "iteration: 40400 loss: 0.0025 lr: 0.02\n",
            "iteration: 40410 loss: 0.0030 lr: 0.02\n",
            "iteration: 40420 loss: 0.0025 lr: 0.02\n",
            "iteration: 40430 loss: 0.0021 lr: 0.02\n",
            "iteration: 40440 loss: 0.0021 lr: 0.02\n",
            "iteration: 40450 loss: 0.0030 lr: 0.02\n",
            "iteration: 40460 loss: 0.0017 lr: 0.02\n",
            "iteration: 40470 loss: 0.0032 lr: 0.02\n",
            "iteration: 40480 loss: 0.0019 lr: 0.02\n",
            "iteration: 40490 loss: 0.0023 lr: 0.02\n",
            "iteration: 40500 loss: 0.0020 lr: 0.02\n",
            "iteration: 40510 loss: 0.0029 lr: 0.02\n",
            "iteration: 40520 loss: 0.0026 lr: 0.02\n",
            "iteration: 40530 loss: 0.0021 lr: 0.02\n",
            "iteration: 40540 loss: 0.0025 lr: 0.02\n",
            "iteration: 40550 loss: 0.0028 lr: 0.02\n",
            "iteration: 40560 loss: 0.0021 lr: 0.02\n",
            "iteration: 40570 loss: 0.0026 lr: 0.02\n",
            "iteration: 40580 loss: 0.0018 lr: 0.02\n",
            "iteration: 40590 loss: 0.0016 lr: 0.02\n",
            "iteration: 40600 loss: 0.0027 lr: 0.02\n",
            "iteration: 40610 loss: 0.0022 lr: 0.02\n",
            "iteration: 40620 loss: 0.0021 lr: 0.02\n",
            "iteration: 40630 loss: 0.0023 lr: 0.02\n",
            "iteration: 40640 loss: 0.0029 lr: 0.02\n",
            "iteration: 40650 loss: 0.0027 lr: 0.02\n",
            "iteration: 40660 loss: 0.0026 lr: 0.02\n",
            "iteration: 40670 loss: 0.0025 lr: 0.02\n",
            "iteration: 40680 loss: 0.0025 lr: 0.02\n",
            "iteration: 40690 loss: 0.0031 lr: 0.02\n",
            "iteration: 40700 loss: 0.0021 lr: 0.02\n",
            "iteration: 40710 loss: 0.0024 lr: 0.02\n",
            "iteration: 40720 loss: 0.0024 lr: 0.02\n",
            "iteration: 40730 loss: 0.0023 lr: 0.02\n",
            "iteration: 40740 loss: 0.0020 lr: 0.02\n",
            "iteration: 40750 loss: 0.0026 lr: 0.02\n",
            "iteration: 40760 loss: 0.0020 lr: 0.02\n",
            "iteration: 40770 loss: 0.0020 lr: 0.02\n",
            "iteration: 40780 loss: 0.0023 lr: 0.02\n",
            "iteration: 40790 loss: 0.0026 lr: 0.02\n",
            "iteration: 40800 loss: 0.0023 lr: 0.02\n",
            "iteration: 40810 loss: 0.0024 lr: 0.02\n",
            "iteration: 40820 loss: 0.0021 lr: 0.02\n",
            "iteration: 40830 loss: 0.0025 lr: 0.02\n",
            "iteration: 40840 loss: 0.0023 lr: 0.02\n",
            "iteration: 40850 loss: 0.0019 lr: 0.02\n",
            "iteration: 40860 loss: 0.0023 lr: 0.02\n",
            "iteration: 40870 loss: 0.0020 lr: 0.02\n",
            "iteration: 40880 loss: 0.0032 lr: 0.02\n",
            "iteration: 40890 loss: 0.0025 lr: 0.02\n",
            "iteration: 40900 loss: 0.0026 lr: 0.02\n",
            "iteration: 40910 loss: 0.0035 lr: 0.02\n",
            "iteration: 40920 loss: 0.0043 lr: 0.02\n",
            "iteration: 40930 loss: 0.0022 lr: 0.02\n",
            "iteration: 40940 loss: 0.0028 lr: 0.02\n",
            "iteration: 40950 loss: 0.0019 lr: 0.02\n",
            "iteration: 40960 loss: 0.0022 lr: 0.02\n",
            "iteration: 40970 loss: 0.0032 lr: 0.02\n",
            "iteration: 40980 loss: 0.0025 lr: 0.02\n",
            "iteration: 40990 loss: 0.0022 lr: 0.02\n",
            "iteration: 41000 loss: 0.0025 lr: 0.02\n",
            "iteration: 41010 loss: 0.0024 lr: 0.02\n",
            "iteration: 41020 loss: 0.0021 lr: 0.02\n",
            "iteration: 41030 loss: 0.0019 lr: 0.02\n",
            "iteration: 41040 loss: 0.0028 lr: 0.02\n",
            "iteration: 41050 loss: 0.0024 lr: 0.02\n",
            "iteration: 41060 loss: 0.0023 lr: 0.02\n",
            "iteration: 41070 loss: 0.0028 lr: 0.02\n",
            "iteration: 41080 loss: 0.0022 lr: 0.02\n",
            "iteration: 41090 loss: 0.0021 lr: 0.02\n",
            "iteration: 41100 loss: 0.0025 lr: 0.02\n",
            "iteration: 41110 loss: 0.0028 lr: 0.02\n",
            "iteration: 41120 loss: 0.0023 lr: 0.02\n",
            "iteration: 41130 loss: 0.0023 lr: 0.02\n",
            "iteration: 41140 loss: 0.0023 lr: 0.02\n",
            "iteration: 41150 loss: 0.0024 lr: 0.02\n",
            "iteration: 41160 loss: 0.0021 lr: 0.02\n",
            "iteration: 41170 loss: 0.0029 lr: 0.02\n",
            "iteration: 41180 loss: 0.0022 lr: 0.02\n",
            "iteration: 41190 loss: 0.0023 lr: 0.02\n",
            "iteration: 41200 loss: 0.0021 lr: 0.02\n",
            "iteration: 41210 loss: 0.0022 lr: 0.02\n",
            "iteration: 41220 loss: 0.0021 lr: 0.02\n",
            "iteration: 41230 loss: 0.0020 lr: 0.02\n",
            "iteration: 41240 loss: 0.0023 lr: 0.02\n",
            "iteration: 41250 loss: 0.0023 lr: 0.02\n",
            "iteration: 41260 loss: 0.0027 lr: 0.02\n",
            "iteration: 41270 loss: 0.0021 lr: 0.02\n",
            "iteration: 41280 loss: 0.0022 lr: 0.02\n",
            "iteration: 41290 loss: 0.0017 lr: 0.02\n",
            "iteration: 41300 loss: 0.0020 lr: 0.02\n",
            "iteration: 41310 loss: 0.0022 lr: 0.02\n",
            "iteration: 41320 loss: 0.0022 lr: 0.02\n",
            "iteration: 41330 loss: 0.0021 lr: 0.02\n",
            "iteration: 41340 loss: 0.0021 lr: 0.02\n",
            "iteration: 41350 loss: 0.0021 lr: 0.02\n",
            "iteration: 41360 loss: 0.0024 lr: 0.02\n",
            "iteration: 41370 loss: 0.0029 lr: 0.02\n",
            "iteration: 41380 loss: 0.0022 lr: 0.02\n",
            "iteration: 41390 loss: 0.0024 lr: 0.02\n",
            "iteration: 41400 loss: 0.0024 lr: 0.02\n",
            "iteration: 41410 loss: 0.0023 lr: 0.02\n",
            "iteration: 41420 loss: 0.0022 lr: 0.02\n",
            "iteration: 41430 loss: 0.0023 lr: 0.02\n",
            "iteration: 41440 loss: 0.0027 lr: 0.02\n",
            "iteration: 41450 loss: 0.0025 lr: 0.02\n",
            "iteration: 41460 loss: 0.0020 lr: 0.02\n",
            "iteration: 41470 loss: 0.0021 lr: 0.02\n",
            "iteration: 41480 loss: 0.0021 lr: 0.02\n",
            "iteration: 41490 loss: 0.0024 lr: 0.02\n",
            "iteration: 41500 loss: 0.0032 lr: 0.02\n",
            "iteration: 41510 loss: 0.0030 lr: 0.02\n",
            "iteration: 41520 loss: 0.0024 lr: 0.02\n",
            "iteration: 41530 loss: 0.0020 lr: 0.02\n",
            "iteration: 41540 loss: 0.0030 lr: 0.02\n",
            "iteration: 41550 loss: 0.0021 lr: 0.02\n",
            "iteration: 41560 loss: 0.0026 lr: 0.02\n",
            "iteration: 41570 loss: 0.0027 lr: 0.02\n",
            "iteration: 41580 loss: 0.0021 lr: 0.02\n",
            "iteration: 41590 loss: 0.0024 lr: 0.02\n",
            "iteration: 41600 loss: 0.0021 lr: 0.02\n",
            "iteration: 41610 loss: 0.0021 lr: 0.02\n",
            "iteration: 41620 loss: 0.0021 lr: 0.02\n",
            "iteration: 41630 loss: 0.0021 lr: 0.02\n",
            "iteration: 41640 loss: 0.0026 lr: 0.02\n",
            "iteration: 41650 loss: 0.0021 lr: 0.02\n",
            "iteration: 41660 loss: 0.0021 lr: 0.02\n",
            "iteration: 41670 loss: 0.0028 lr: 0.02\n",
            "iteration: 41680 loss: 0.0023 lr: 0.02\n",
            "iteration: 41690 loss: 0.0020 lr: 0.02\n",
            "iteration: 41700 loss: 0.0026 lr: 0.02\n",
            "iteration: 41710 loss: 0.0023 lr: 0.02\n",
            "iteration: 41720 loss: 0.0022 lr: 0.02\n",
            "iteration: 41730 loss: 0.0023 lr: 0.02\n",
            "iteration: 41740 loss: 0.0043 lr: 0.02\n",
            "iteration: 41750 loss: 0.0022 lr: 0.02\n",
            "iteration: 41760 loss: 0.0029 lr: 0.02\n",
            "iteration: 41770 loss: 0.0031 lr: 0.02\n",
            "iteration: 41780 loss: 0.0027 lr: 0.02\n",
            "iteration: 41790 loss: 0.0024 lr: 0.02\n",
            "iteration: 41800 loss: 0.0022 lr: 0.02\n",
            "iteration: 41810 loss: 0.0025 lr: 0.02\n",
            "iteration: 41820 loss: 0.0037 lr: 0.02\n",
            "iteration: 41830 loss: 0.0026 lr: 0.02\n",
            "iteration: 41840 loss: 0.0031 lr: 0.02\n",
            "iteration: 41850 loss: 0.0026 lr: 0.02\n",
            "iteration: 41860 loss: 0.0021 lr: 0.02\n",
            "iteration: 41870 loss: 0.0031 lr: 0.02\n",
            "iteration: 41880 loss: 0.0029 lr: 0.02\n",
            "iteration: 41890 loss: 0.0019 lr: 0.02\n",
            "iteration: 41900 loss: 0.0021 lr: 0.02\n",
            "iteration: 41910 loss: 0.0032 lr: 0.02\n",
            "iteration: 41920 loss: 0.0027 lr: 0.02\n",
            "iteration: 41930 loss: 0.0023 lr: 0.02\n",
            "iteration: 41940 loss: 0.0025 lr: 0.02\n",
            "iteration: 41950 loss: 0.0029 lr: 0.02\n",
            "iteration: 41960 loss: 0.0023 lr: 0.02\n",
            "iteration: 41970 loss: 0.0028 lr: 0.02\n",
            "iteration: 41980 loss: 0.0021 lr: 0.02\n",
            "iteration: 41990 loss: 0.0023 lr: 0.02\n",
            "iteration: 42000 loss: 0.0026 lr: 0.02\n",
            "iteration: 42010 loss: 0.0027 lr: 0.02\n",
            "iteration: 42020 loss: 0.0023 lr: 0.02\n",
            "iteration: 42030 loss: 0.0023 lr: 0.02\n",
            "iteration: 42040 loss: 0.0026 lr: 0.02\n",
            "iteration: 42050 loss: 0.0022 lr: 0.02\n",
            "iteration: 42060 loss: 0.0025 lr: 0.02\n",
            "iteration: 42070 loss: 0.0025 lr: 0.02\n",
            "iteration: 42080 loss: 0.0025 lr: 0.02\n",
            "iteration: 42090 loss: 0.0024 lr: 0.02\n",
            "iteration: 42100 loss: 0.0020 lr: 0.02\n",
            "iteration: 42110 loss: 0.0023 lr: 0.02\n",
            "iteration: 42120 loss: 0.0030 lr: 0.02\n",
            "iteration: 42130 loss: 0.0025 lr: 0.02\n",
            "iteration: 42140 loss: 0.0025 lr: 0.02\n",
            "iteration: 42150 loss: 0.0023 lr: 0.02\n",
            "iteration: 42160 loss: 0.0024 lr: 0.02\n",
            "iteration: 42170 loss: 0.0020 lr: 0.02\n",
            "iteration: 42180 loss: 0.0027 lr: 0.02\n",
            "iteration: 42190 loss: 0.0026 lr: 0.02\n",
            "iteration: 42200 loss: 0.0020 lr: 0.02\n",
            "iteration: 42210 loss: 0.0030 lr: 0.02\n",
            "iteration: 42220 loss: 0.0033 lr: 0.02\n",
            "iteration: 42230 loss: 0.0024 lr: 0.02\n",
            "iteration: 42240 loss: 0.0019 lr: 0.02\n",
            "iteration: 42250 loss: 0.0028 lr: 0.02\n",
            "iteration: 42260 loss: 0.0025 lr: 0.02\n",
            "iteration: 42270 loss: 0.0020 lr: 0.02\n",
            "iteration: 42280 loss: 0.0020 lr: 0.02\n",
            "iteration: 42290 loss: 0.0018 lr: 0.02\n",
            "iteration: 42300 loss: 0.0026 lr: 0.02\n",
            "iteration: 42310 loss: 0.0023 lr: 0.02\n",
            "iteration: 42320 loss: 0.0028 lr: 0.02\n",
            "iteration: 42330 loss: 0.0022 lr: 0.02\n",
            "iteration: 42340 loss: 0.0019 lr: 0.02\n",
            "iteration: 42350 loss: 0.0022 lr: 0.02\n",
            "iteration: 42360 loss: 0.0020 lr: 0.02\n",
            "iteration: 42370 loss: 0.0027 lr: 0.02\n",
            "iteration: 42380 loss: 0.0024 lr: 0.02\n",
            "iteration: 42390 loss: 0.0022 lr: 0.02\n",
            "iteration: 42400 loss: 0.0018 lr: 0.02\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-112ebf4a54ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdeeplabcut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_config_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplayiters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msaveiters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/deeplabcut/pose_estimation_tensorflow/training.py\u001b[0m in \u001b[0;36mtrain_network\u001b[0;34m(config, shuffle, trainingsetindex, max_snapshots_to_keep, displayiters, saveiters, maxiters, allow_growth, gputouse, autotune, keepdeconvweights, modelprefix)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/deeplabcut/pose_estimation_tensorflow/training.py\u001b[0m in \u001b[0;36mtrain_network\u001b[0;34m(config, shuffle, trainingsetindex, max_snapshots_to_keep, displayiters, saveiters, maxiters, allow_growth, gputouse, autotune, keepdeconvweights, modelprefix)\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0mmax_to_keep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_snapshots_to_keep\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m                 \u001b[0mkeepdeconvweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeepdeconvweights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m                 \u001b[0mallow_growth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_growth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m             )  # pass on path and file name for pose_cfg.yaml!\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/deeplabcut/pose_estimation_tensorflow/train.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(config_yaml, displayiters, saveiters, maxiters, max_to_keep, keepdeconvweights, allow_growth)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         [_, loss_val, summary] = sess.run(\n\u001b[0;32m--> 277\u001b[0;31m             \u001b[0;34m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerged_summaries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m         )\n\u001b[1;32m    279\u001b[0m         \u001b[0mcum_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1363\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CPk5DWHUKxle",
        "outputId": "b3fc8497-2f1f-4e3e-ce32-8c28a70834b9"
      },
      "source": [
        "%matplotlib inline\n",
        "deeplabcut.evaluate_network(path_config_file,plotting=True)\n",
        "\n",
        "# Here you want to see a low pixel error! Of course, it can only be as good as the labeler, so be sure your labels are good!"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Config:\n",
            "{'all_joints': [[0],\n",
            "                [1],\n",
            "                [2],\n",
            "                [3],\n",
            "                [4],\n",
            "                [5],\n",
            "                [6],\n",
            "                [7],\n",
            "                [8],\n",
            "                [9],\n",
            "                [10],\n",
            "                [11],\n",
            "                [12],\n",
            "                [13],\n",
            "                [14]],\n",
            " 'all_joints_names': ['nose',\n",
            "                      'front_left_t',\n",
            "                      'front_left_m',\n",
            "                      'front_left_b',\n",
            "                      'front_right_t',\n",
            "                      'front_right_m',\n",
            "                      'front_right_b',\n",
            "                      'hind_left_t',\n",
            "                      'hind_left_m',\n",
            "                      'hind_left_b',\n",
            "                      'hind_right_t',\n",
            "                      'hind _right_m',\n",
            "                      'hind_right_b',\n",
            "                      'tail_start',\n",
            "                      'tail_end'],\n",
            " 'batch_size': 1,\n",
            " 'crop_pad': 0,\n",
            " 'dataset': 'training-datasets/iteration-0/UnaugmentedDataSet_walkdatasetJune26/walkdataset_moon95shuffle1.mat',\n",
            " 'dataset_type': 'imgaug',\n",
            " 'deterministic': False,\n",
            " 'fg_fraction': 0.25,\n",
            " 'global_scale': 0.8,\n",
            " 'init_weights': '/usr/local/lib/python3.7/dist-packages/deeplabcut/pose_estimation_tensorflow/models/pretrained/resnet_v1_50.ckpt',\n",
            " 'intermediate_supervision': False,\n",
            " 'intermediate_supervision_layer': 12,\n",
            " 'location_refinement': True,\n",
            " 'locref_huber_loss': True,\n",
            " 'locref_loss_weight': 1.0,\n",
            " 'locref_stdev': 7.2801,\n",
            " 'log_dir': 'log',\n",
            " 'mean_pixel': [123.68, 116.779, 103.939],\n",
            " 'mirror': False,\n",
            " 'net_type': 'resnet_50',\n",
            " 'num_joints': 15,\n",
            " 'optimizer': 'sgd',\n",
            " 'pairwise_huber_loss': True,\n",
            " 'pairwise_predict': False,\n",
            " 'partaffinityfield_predict': False,\n",
            " 'regularize': False,\n",
            " 'scoremap_dir': 'test',\n",
            " 'shuffle': True,\n",
            " 'snapshot_prefix': '/content/cloned-DLC-repo/examples/walkdataset/dlc-models/iteration-0/walkdatasetJune26-trainset95shuffle1/test/snapshot',\n",
            " 'stride': 8.0,\n",
            " 'weigh_negatives': False,\n",
            " 'weigh_only_present_joints': False,\n",
            " 'weigh_part_predictions': False,\n",
            " 'weight_decay': 0.0001}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Running  DLC_resnet50_walkdatasetJune26shuffle1_42000  with # of trainingiterations: 42000\n",
            "Initializing ResNet\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Analyzing data...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "77it [00:13,  5.65it/s]\n",
            "  0%|          | 0/77 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Done and results stored for snapshot:  snapshot-42000\n",
            "Results for 42000  training iterations: 95 1 train error: 17.18 pixels. Test error: 54.68  pixels.\n",
            "With pcutoff of 0.4  train error: 9.11 pixels. Test error: 69.81 pixels\n",
            "Thereby, the errors are given by the average distances between the labels by DLC and the scorer.\n",
            "Plotting...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 77/77 [00:54<00:00,  1.41it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The network is evaluated and the results are stored in the subdirectory 'evaluation_results'.\n",
            "If it generalizes well, choose the best model for prediction and update the config file with the appropriate index for the 'snapshotindex'.\n",
            "Use the function 'analyze_video' to make predictions on new videos.\n",
            "Otherwise consider retraining the network (see DeepLabCut workflow Fig 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Oz9LhqelLLEE",
        "outputId": "4d20f172-5204-4bfa-c066-cf4e6058deee"
      },
      "source": [
        "videofile_path = ['/content/cloned-DLC-repo/examples/walkdataset/videos/dogwalk4.mp4'] #Enter the list of videos to analyze.\n",
        "deeplabcut.analyze_videos(path_config_file,videofile_path, videotype='.mp4')"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Config:\n",
            "{'all_joints': [[0],\n",
            "                [1],\n",
            "                [2],\n",
            "                [3],\n",
            "                [4],\n",
            "                [5],\n",
            "                [6],\n",
            "                [7],\n",
            "                [8],\n",
            "                [9],\n",
            "                [10],\n",
            "                [11],\n",
            "                [12],\n",
            "                [13],\n",
            "                [14]],\n",
            " 'all_joints_names': ['nose',\n",
            "                      'front_left_t',\n",
            "                      'front_left_m',\n",
            "                      'front_left_b',\n",
            "                      'front_right_t',\n",
            "                      'front_right_m',\n",
            "                      'front_right_b',\n",
            "                      'hind_left_t',\n",
            "                      'hind_left_m',\n",
            "                      'hind_left_b',\n",
            "                      'hind_right_t',\n",
            "                      'hind _right_m',\n",
            "                      'hind_right_b',\n",
            "                      'tail_start',\n",
            "                      'tail_end'],\n",
            " 'batch_size': 1,\n",
            " 'crop_pad': 0,\n",
            " 'dataset': 'training-datasets/iteration-0/UnaugmentedDataSet_walkdatasetJune26/walkdataset_moon95shuffle1.mat',\n",
            " 'dataset_type': 'imgaug',\n",
            " 'deterministic': False,\n",
            " 'fg_fraction': 0.25,\n",
            " 'global_scale': 0.8,\n",
            " 'init_weights': '/usr/local/lib/python3.7/dist-packages/deeplabcut/pose_estimation_tensorflow/models/pretrained/resnet_v1_50.ckpt',\n",
            " 'intermediate_supervision': False,\n",
            " 'intermediate_supervision_layer': 12,\n",
            " 'location_refinement': True,\n",
            " 'locref_huber_loss': True,\n",
            " 'locref_loss_weight': 1.0,\n",
            " 'locref_stdev': 7.2801,\n",
            " 'log_dir': 'log',\n",
            " 'mean_pixel': [123.68, 116.779, 103.939],\n",
            " 'mirror': False,\n",
            " 'net_type': 'resnet_50',\n",
            " 'num_joints': 15,\n",
            " 'optimizer': 'sgd',\n",
            " 'pairwise_huber_loss': True,\n",
            " 'pairwise_predict': False,\n",
            " 'partaffinityfield_predict': False,\n",
            " 'regularize': False,\n",
            " 'scoremap_dir': 'test',\n",
            " 'shuffle': True,\n",
            " 'snapshot_prefix': '/content/cloned-DLC-repo/examples/walkdataset/dlc-models/iteration-0/walkdatasetJune26-trainset95shuffle1/test/snapshot',\n",
            " 'stride': 8.0,\n",
            " 'weigh_negatives': False,\n",
            " 'weigh_only_present_joints': False,\n",
            " 'weigh_part_predictions': False,\n",
            " 'weight_decay': 0.0001}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Using snapshot-42000 for model /content/cloned-DLC-repo/examples/walkdataset/dlc-models/iteration-0/walkdatasetJune26-trainset95shuffle1\n",
            "Initializing ResNet\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/278 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Starting to analyze %  /content/cloned-DLC-repo/examples/walkdataset/videos/dogwalk4.mp4\n",
            "/content/cloned-DLC-repo/examples/walkdataset/videos  already exists!\n",
            "Loading  /content/cloned-DLC-repo/examples/walkdataset/videos/dogwalk4.mp4\n",
            "Duration of video [s]:  13.85 , recorded with  20.07 fps!\n",
            "Overall # of frames:  278  found with (before cropping) frame dimensions:  1448 826\n",
            "Starting to extract posture\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "280it [00:29,  9.49it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Saving results in /content/cloned-DLC-repo/examples/walkdataset/videos...\n",
            "The videos are analyzed. Now your research can truly start! \n",
            " You can create labeled videos with 'create_labeled_video'\n",
            "If the tracking is not satisfactory for some videos, consider expanding the training set. You can use the function 'extract_outlier_frames' to extract a few representative outlier frames.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'DLC_resnet50_walkdatasetJune26shuffle1_42000'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49RXuQbHLcCz",
        "outputId": "2277b2b2-0033-4ac7-f930-d3b117080257"
      },
      "source": [
        "deeplabcut.create_labeled_video(path_config_file,videofile_path)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/cloned-DLC-repo/examples/walkdataset/videos  already exists!\n",
            "Starting to process video: /content/cloned-DLC-repo/examples/walkdataset/videos/dogwalk4.mp4\n",
            "Loading /content/cloned-DLC-repo/examples/walkdataset/videos/dogwalk4.mp4 and data.\n",
            "Duration of video [s]: 13.85, recorded with 20.07 fps!\n",
            "Overall # of frames: 278 with cropped frame dimensions: 1448 826\n",
            "Generating frames and creating video.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 278/278 [00:04<00:00, 67.95it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RNn_sg39PdH5",
        "outputId": "c7c7efdb-64e4-4eb4-97d8-a96df80cbadd"
      },
      "source": [
        "deeplabcut.plot_trajectories(path_config_file,videofile_path)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading  /content/cloned-DLC-repo/examples/walkdataset/videos/dogwalk4.mp4 and data.\n",
            "Plots created! Please check the directory \"plot-poses\" within the video directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nw9wZhInLg79",
        "outputId": "4410522b-60a9-473d-e962-c8e255daf090"
      },
      "source": [
        "#create a path variable that links to the config file:\n",
        "path_config_file = '/content/cloned-DLC-repo/examples/openfield-Pranav-2018-10-30/config.yaml'\n",
        "\n",
        "# Loading example data set:\n",
        "deeplabcut.load_demo_data(path_config_file)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded, now creating training data...\n",
            "Downloading a ImageNet-pretrained model from http://download.tensorflow.org/models/resnet_v1_50_2016_08_28.tar.gz....\n",
            "The training dataset is successfully created. Use the function 'train_network' to start training. Happy training!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 560
        },
        "id": "ZigmzK3rLkXD",
        "outputId": "18c1e2ea-c2e0-4caf-c185-7b19384a4650"
      },
      "source": [
        "#let's also change the display and save_iters just in case Colab takes away the GPU... \n",
        "#if that happens, you can reload from a saved point. Typically, you want to train to 200,000 + iterations.\n",
        "#more info and there are more things you can set: https://github.com/AlexEMG/DeepLabCut/blob/master/docs/functionDetails.md#g-train-the-network\n",
        "\n",
        "deeplabcut.train_network(path_config_file, shuffle=1, displayiters=10,saveiters=100)\n",
        "\n",
        "#this will run until you stop it (CTRL+C), or hit \"STOP\" icon, or when it hits the end (default, 1.03M iterations). \n",
        "#Whichever you chose, you will see what looks like an error message, but it's not an error - don't worry...."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iteration: 1310 loss: 0.0156 lr: 0.005\n",
            "iteration: 1320 loss: 0.0108 lr: 0.005\n",
            "iteration: 1330 loss: 0.0098 lr: 0.005\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-a580e94b3953>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#more info and there are more things you can set: https://github.com/AlexEMG/DeepLabCut/blob/master/docs/functionDetails.md#g-train-the-network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdeeplabcut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_config_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplayiters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msaveiters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#this will run until you stop it (CTRL+C), or hit \"STOP\" icon, or when it hits the end (default, 1.03M iterations).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/deeplabcut/pose_estimation_tensorflow/training.py\u001b[0m in \u001b[0;36mtrain_network\u001b[0;34m(config, shuffle, trainingsetindex, max_snapshots_to_keep, displayiters, saveiters, maxiters, allow_growth, gputouse, autotune, keepdeconvweights, modelprefix)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/deeplabcut/pose_estimation_tensorflow/training.py\u001b[0m in \u001b[0;36mtrain_network\u001b[0;34m(config, shuffle, trainingsetindex, max_snapshots_to_keep, displayiters, saveiters, maxiters, allow_growth, gputouse, autotune, keepdeconvweights, modelprefix)\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0mmax_to_keep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_snapshots_to_keep\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m                 \u001b[0mkeepdeconvweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeepdeconvweights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m                 \u001b[0mallow_growth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_growth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m             )  # pass on path and file name for pose_cfg.yaml!\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/deeplabcut/pose_estimation_tensorflow/train.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(config_yaml, displayiters, saveiters, maxiters, max_to_keep, keepdeconvweights, allow_growth)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         [_, loss_val, summary] = sess.run(\n\u001b[0;32m--> 277\u001b[0;31m             \u001b[0;34m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerged_summaries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m         )\n\u001b[1;32m    279\u001b[0m         \u001b[0mcum_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1363\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GP8yWiPTNDFS",
        "outputId": "2b3bed26-d02e-4c04-8a78-5d453910ce0e"
      },
      "source": [
        "!git clone -l -s https://github.com/Yejin-Moon/deeplabcut.git\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'deeplabcut'...\n",
            "warning: --local is ignored\n",
            "remote: Enumerating objects: 72, done.\u001b[K\n",
            "remote: Counting objects: 100% (72/72), done.\u001b[K\n",
            "remote: Compressing objects: 100% (65/65), done.\u001b[K\n",
            "remote: Total 72 (delta 3), reused 3 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (72/72), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KSiLDwmzNUf0",
        "outputId": "219d8f3d-357f-4c82-9636-5fe017f75d70"
      },
      "source": [
        "cd deeplabcut/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/deeplabcut\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y32DakbNNbm3",
        "outputId": "b98005d0-ebd3-413d-a91c-7a4b1491fd8a"
      },
      "source": [
        "!pip install deeplabcut"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: deeplabcut in /usr/local/lib/python3.7/dist-packages (2.1.10.4)\n",
            "Requirement already satisfied: numpy~=1.17.3 in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (1.17.5)\n",
            "Requirement already satisfied: intel-openmp in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (2021.2.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (2.5.1)\n",
            "Requirement already satisfied: opencv-python-headless~=3.4.9.33 in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (3.4.9.33)\n",
            "Requirement already satisfied: tables in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (3.4.4)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (2.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (7.1.2)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (5.5.0)\n",
            "Requirement already satisfied: ruamel.yaml>=0.15.0 in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (0.17.9)\n",
            "Requirement already satisfied: scipy>=1.4 in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (1.4.1)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (0.29.23)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (1.15.0)\n",
            "Requirement already satisfied: filterpy in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (1.4.5)\n",
            "Requirement already satisfied: moviepy<=1.0.1 in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (0.2.3.5)\n",
            "Requirement already satisfied: matplotlib==3.1.3 in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (3.1.3)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (0.36.2)\n",
            "Requirement already satisfied: tensorpack==0.9.8 in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (0.9.8)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (4.41.1)\n",
            "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (1.1.5)\n",
            "Requirement already satisfied: patsy in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (0.5.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (2020.12.5)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (3.0.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (57.0.0)\n",
            "Requirement already satisfied: bayesian-optimization in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (1.2.0)\n",
            "Requirement already satisfied: imgaug in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (0.2.9)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (3.1.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (0.2.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (3.13)\n",
            "Requirement already satisfied: scikit-image>=0.17 in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (0.18.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (0.22.2.post1)\n",
            "Requirement already satisfied: statsmodels>=0.11 in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (0.12.2)\n",
            "Requirement already satisfied: numba==0.51.1 in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (0.51.1)\n",
            "Requirement already satisfied: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx->deeplabcut) (4.4.2)\n",
            "Requirement already satisfied: numexpr>=2.5.2 in /usr/local/lib/python3.7/dist-packages (from tables->deeplabcut) (2.7.3)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->deeplabcut) (5.0.5)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.7/dist-packages (from ipython->deeplabcut) (4.8.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->deeplabcut) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->deeplabcut) (1.0.18)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->deeplabcut) (0.8.1)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->deeplabcut) (2.6.1)\n",
            "Requirement already satisfied: ruamel.yaml.clib>=0.1.2; platform_python_implementation == \"CPython\" and python_version < \"3.10\" in /usr/local/lib/python3.7/dist-packages (from ruamel.yaml>=0.15.0->deeplabcut) (0.2.2)\n",
            "Requirement already satisfied: imageio<3.0,>=2.1.2 in /usr/local/lib/python3.7/dist-packages (from moviepy<=1.0.1->deeplabcut) (2.4.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.3->deeplabcut) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.3->deeplabcut) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.3->deeplabcut) (1.3.1)\n",
            "Requirement already satisfied: msgpack-numpy>=0.4.4.2 in /usr/local/lib/python3.7/dist-packages (from tensorpack==0.9.8->deeplabcut) (0.4.7.1)\n",
            "Requirement already satisfied: msgpack>=0.5.2 in /usr/local/lib/python3.7/dist-packages (from tensorpack==0.9.8->deeplabcut) (1.0.2)\n",
            "Requirement already satisfied: pyzmq>=16 in /usr/local/lib/python3.7/dist-packages (from tensorpack==0.9.8->deeplabcut) (22.0.3)\n",
            "Requirement already satisfied: psutil>=5 in /usr/local/lib/python3.7/dist-packages (from tensorpack==0.9.8->deeplabcut) (5.4.8)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.7/dist-packages (from tensorpack==0.9.8->deeplabcut) (1.1.0)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.7/dist-packages (from tensorpack==0.9.8->deeplabcut) (0.8.9)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.1->deeplabcut) (2018.9)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from imgaug->deeplabcut) (7.1.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from imgaug->deeplabcut) (4.1.2.30)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.7/dist-packages (from imgaug->deeplabcut) (1.7.1)\n",
            "Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py->deeplabcut) (1.5.2)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.17->deeplabcut) (1.1.1)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.17->deeplabcut) (2021.4.8)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->deeplabcut) (1.0.1)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba==0.51.1->deeplabcut) (0.34.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect; sys_platform != \"win32\"->ipython->deeplabcut) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->deeplabcut) (0.2.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kuezTYUvNeM_"
      },
      "source": [
        "# Use TensorFlow 1.x:\n",
        "%tensorflow_version 1.x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KlOFZ52zNhhD"
      },
      "source": [
        "#GUIs don't work on the cloud, so we will supress wxPython: \n",
        "import os\n",
        "os.environ[\"DLClight\"]=\"True\"\n",
        "\n",
        "import deeplabcut"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        },
        "id": "Z4nJvALCNkMC",
        "outputId": "5ea5a54c-de18-410f-ae2d-f1479a62e433"
      },
      "source": [
        "#create a path variable that links to the config file:\n",
        "path_config_file = '/content/deeplabcut/config.yaml'\n",
        "\n",
        "# Loading example data set:\n",
        "deeplabcut.load_demo_data(path_config_file)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This is not an offical demo dataset.\n",
            "Loaded, now creating training data...\n",
            "/content/deeplabcut/training-datasets/iteration-0/UnaugmentedDataSet_dataset2June12  already exists!\n",
            "/content/deeplabcut/labeled-data/dog/CollectedData_moon.h5  not found (perhaps not annotated). If training on cropped data, make sure to call `cropimagesandlabels` prior to creating the dataset.\n",
            "Annotation data was not found by splitting video paths (from config['video_sets']). An alternative route is taken...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-25ef35d538b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Loading example data set:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdeeplabcut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_demo_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_config_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/deeplabcut/create_project/demo_data.py\u001b[0m in \u001b[0;36mload_demo_data\u001b[0;34m(config, createtrainingset)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcreatetrainingset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loaded, now creating training data...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mdeeplabcut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_training_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_shuffles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/deeplabcut/generate_training_dataset/trainingsetmanipulation.py\u001b[0m in \u001b[0;36mcreate_training_dataset\u001b[0;34m(config, num_shuffles, Shuffles, windows2linux, userfeedback, trainIndices, testIndices, net_type, augmenter_type)\u001b[0m\n\u001b[1;32m    880\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m         Data = merge_annotateddatasets(\n\u001b[0;32m--> 882\u001b[0;31m             \u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproject_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainingsetfolder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindows2linux\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    883\u001b[0m         )\n\u001b[1;32m    884\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mData\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/deeplabcut/generate_training_dataset/trainingsetmanipulation.py\u001b[0m in \u001b[0;36mmerge_annotateddatasets\u001b[0;34m(cfg, trainingsetfolder_full, windows2linux)\u001b[0m\n\u001b[1;32m    590\u001b[0m             \u001b[0;34m\"Annotation data was not found by splitting video paths (from config['video_sets']). An alternative route is taken...\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m         )\n\u001b[0;32m--> 592\u001b[0;31m         \u001b[0mAnnotationData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversioncode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge_windowsannotationdataONlinuxsystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    593\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAnnotationData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No data was found!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/deeplabcut/utils/conversioncode.py\u001b[0m in \u001b[0;36mmerge_windowsannotationdataONlinuxsystem\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0muse_cropped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"croppedtraining\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0mannotationfolders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0melem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mauxiliaryfunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrab_files_in_folder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelative\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m         if os.path.isdir(elem) and (\n\u001b[1;32m    205\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0muse_cropped\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_cropped\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/deeplabcut/utils/auxiliaryfunctions.py\u001b[0m in \u001b[0;36mgrab_files_in_folder\u001b[0;34m(folder, ext, relative)\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgrab_files_in_folder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelative\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m     \u001b[0;34m\"\"\"Return the paths of files with extension *ext* present in *folder*.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrelative\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/deeplabcut/labeled-data'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTIfppwLPOLX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSSR0zioIJpj"
      },
      "source": [
        ""
      ]
    }
  ]
}